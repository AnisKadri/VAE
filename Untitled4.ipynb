{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e433565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Channels_Coupling': {'amplitude': [], 'channels': []},\n",
      " 'Noise': {'channel': [], 'index': [], 'slope': []},\n",
      " 'Pulse': {'amplitude': [], 'channel': [], 'index': []},\n",
      " 'Seasonality': {'amplitude': [],\n",
      "                 'channel': [],\n",
      "                 'frequency_per_week': [],\n",
      "                 'phaseshift': []},\n",
      " 'Std_variation': {'amplitude': [], 'channel': [], 'interval': []},\n",
      " 'Trend': {'channel': [0],\n",
      "           'index': ['2023-03-05T16:00:00'],\n",
      "           'slope': [-0.0005698505231139161]}}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAHqCAYAAAAZLi26AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAD+K0lEQVR4nOzdd3wU1doH8N9md1NJAiEEQu+hhd5BejMoTS+IyhWxoAKiKL4ionItKPYK2EBUEAWlC4QSauidhAChBAghEEJ62TLvH2F3Z3ZnZmd2Z2ue7+fjvWR25sw5m9nNPHPOeY6KYRgGhBBCCCGEEEIIUVyApytACCGEEEIIIYT4Kwq6CSGEEEIIIYQQF6GgmxBCCCGEEEIIcREKugkhhBBCCCGEEBehoJsQQgghhBBCCHERCroJIYQQQgghhBAXoaCbEEIIIYQQQghxEQq6CSGEEEIIIYQQF6GgmxBCCCGEEEIIcREKugkhhBAZTp48iaeeegpNmjRBSEgIQkJC0KxZM0yePBmHDx/2dPUUtW/fPrzzzju4e/eu4mVPnDgRDRs2tLtfv379oFKp0LhxYzAMY/P6rl27oFKpoFKpsGTJEgAw/2zvv6SkJADA1atX8cILL6B58+YICQlBVFQU4uPj8cwzz+Dq1asKtpoQQkhlpPF0BQghhBBfsWjRIkydOhVxcXGYPn06WrduDZVKhdTUVCxfvhxdunTBhQsX0KRJE09XVRH79u3D3LlzMXHiRFStWtVj9QgPD8elS5ewfft2DBw4kPPazz//jIiICOTn55u3JScnc/Z59913sWPHDmzfvp2zvVWrVrh27Ro6duyIqlWr4pVXXkFcXBzy8vKQkpKCP//8ExcvXkS9evVc1zhCCCF+j4JuQgghRIK9e/fihRdewPDhw7Fy5UoEBgaaXxswYACmTJmCv/76CyEhIR6spbji4mKEhoZ6uhqy1a9fH+Hh4fj55585QXdBQQH++usvPPbYY/jhhx/M27t37845vkaNGggICLDZDgCffvopbt++jYMHD6JRo0bm7aNGjcIbb7wBo9HoghYRQgipTGh4OSGEECLBBx98ALVajUWLFnECbrb//Oc/qF27Nmfb4cOHMWLECERFRSE4OBgdOnTAn3/+ydlnyZIlUKlU2LFjB55//nlER0ejevXqGDNmDDIzM23Os2LFCvTo0QNhYWGoUqUKhg4dimPHjnH2mThxIqpUqYJTp05hyJAhCA8PNwesiYmJGDlyJOrWrYvg4GA0bdoUkydPxu3bt83Hv/POO5g5cyYAoFGjRjZDsqXWw9S+uLg4BAUFoWXLlli6dKnIO81v0qRJ+PvvvzlD3f/44w8AwCOPPCK7PJOcnBwEBAQgJiaG9/WAALpVIoQQ4hz6S0IIIYTYYTAYsGPHDnTu3BmxsbGSj9uxYwd69eqFu3fvYuHChVizZg3at2+PcePGmecfsz399NPQarVYtmwZ5s+fj6SkJDz++OOcfT744AOMHz8erVq1wp9//olff/0VBQUFuO+++5CSksLZt7y8HCNGjMCAAQOwZs0azJ07FwCQnp6OHj16YMGCBdiyZQveeustHDhwAL1794ZOpzPXZdq0aQCAv//+G8nJyUhOTkbHjh1l1WPJkiV48skn0bJlS6xatQpvvvkm3n33XZuh3vY88sgjUKvVWL58uXnbTz/9hIcffhgRERGyymLr0aMHjEYjxowZg82bN3OGqRNCCCGKYAghhBAiKisriwHAPPLIIzav6fV6RqfTmf8zGo3m11q0aMF06NCB0el0nGMeeOABJjY2ljEYDAzDMMzixYsZAMwLL7zA2W/+/PkMAObGjRsMwzBMRkYGo9FomGnTpnH2KygoYGrVqsWMHTvWvO2JJ55gADA///yzaNuMRiOj0+mYK1euMACYNWvWmF/7+OOPGQDMpUuXOMdIrYfBYGBq167NdOzYkfO+XL58mdFqtUyDBg1E68YwDNO3b1+mdevW5jZ17tyZYRiGOXPmDAOASUpKYg4dOsQAYBYvXsxbxhNPPMGEhYUJtn/y5MlMQEAAA4BRqVRMy5YtmZdfftmm3YQQQogjqKebEEIIcUKnTp2g1WrN/3366acAgAsXLuDs2bN47LHHAAB6vd78X0JCAm7cuIG0tDROWSNGjOD83LZtWwDAlStXAACbN2+GXq/Hf//7X055wcHB6Nu3L2fot8lDDz1ksy07OxvPPfcc6tWrB41GA61WiwYNGgAAUlNT7bZZaj3S0tKQmZmJRx99FCqVynx8gwYN0LNnT7vnsTZp0iQcPnwYp06dwk8//YQmTZqgT58+ssthU6lUWLhwIS5evIjvvvsOTz75JHQ6HT7//HO0bt0aO3fudKp8QgghhBKpEUIIIXZER0cjJCTEHPyyLVu2DMXFxbhx4wYnaL558yYA4NVXX8Wrr77KWy57DjUAVK9enfNzUFAQAKCkpIRTZpcuXXjLs55/HBoaajP02mg0YsiQIcjMzMScOXMQHx+PsLAwGI1GdO/e3XwuMVLrkZOTAwCoVauWzT61atXC5cuX7Z6LrU+fPmjWrBkWLVqEP//8Ey+99BInmHdGgwYN8Pzzz5t//vPPPzF+/HjMnDkTBw8eVOQchBBCKicKugkhhBA71Go1BgwYgC1btuDGjRuced2tWrUCAJsAMjo6GgAwa9YsjBkzhrfcuLg4WfUwlbly5Upzz7QYvoD09OnTOHHiBJYsWYInnnjCvP3ChQuK18P0ECErK8vmNb5tUjz55JN48803oVKpOPVX2tixYzFv3jycPn3aZecghBBSOVDQTQghhEgwa9Ys/Pvvv3juueewcuVKaLVa0f3j4uLQrFkznDhxAh988IEidRg6dCg0Gg3S09N5h41LYQrETb3oJosWLbLZ17qnXW494uLiEBsbi+XLl2PGjBnmc1+5cgX79u2zyfQuxRNPPIEDBw6gZcuWqFOnjuzjrVk/RDEpLCzE1atXHaojIYQQwkZBNyGEECJBr1698O2332LatGno2LEjnn32WbRu3RoBAQG4ceMGVq1aBQCc4dyLFi3C/fffj6FDh2LixImoU6cO7ty5g9TUVBw9ehR//fWXrDo0bNgQ//vf/zB79mxcvHgRw4YNQ7Vq1XDz5k0cPHgQYWFh5gzlQlq0aIEmTZrg9ddfB8MwiIqKwrp165CYmGizb3x8PADgyy+/xBNPPAGtVou4uDjJ9QgICMC7776Lp59+GqNHj8YzzzyDu3fv4p133uEdci5F7dq1sXr1aoeO5fP+++9j7969GDduHNq3b4+QkBBcunQJ33zzDXJycvDxxx8rdi5CCCGVEwXdhBBCiETPPfccevTogS+//BKff/45MjMzoVKpULduXfTs2RPbtm3DgAEDzPv3798fBw8exPvvv4+XXnoJubm5qF69Olq1aoWxY8c6VIdZs2ahVatW+PLLL7F8+XKUlZWhVq1a6NKlC5577jm7x2u1Wqxbtw7Tp0/H5MmTodFoMGjQIGzduhX169fn7NuvXz/MmjULv/zyC3744QcYjUbs2LHDvF1KPZ566ikAwEcffYQxY8agYcOGeOONN7Bz507exG/uNmHCBAAVa35//PHHyMvLQ1RUFDp16oSNGzfi/vvv93ANCSGE+DoVwzCMpytBCCGEEEIIIYT4I1oyjBBCCCGEEEIIcREKugkhhBBCCCGEEBehoJsQQgghhBBCCHERCroJIYQQQgghhBAXoaCbEEIIIYQQQghxEQq6CSGEEEIIIYQQF6F1ugEYjUZkZmYiPDwcKpXK09UhhBBCCCGEEOLlGIZBQUEBateujYAA4f5sCroBZGZmol69ep6uBiGEEEIIIYQQH3P16lXUrVtX8HUKugGEh4cDqHizIiIiPFwbfjqdDlu2bMGQIUOg1Wo9XR3F+XP7/LltgH+3z5/bBvh3+6htvsuf2+fPbQP8u33UNt/lz+3z57YBvtG+/Px81KtXzxxPCqGgGzAPKY+IiPDqoDs0NBQRERFee9E5w5/b589tA/y7ff7cNsC/20dt813+3D5/bhvg3+2jtvkuf26fP7cN8K322ZuiTInUCCGEEEIIIYQQF6GgmxBCCCGEEEIIcREKugkhhBBCCCGEEBehoJsQQgghhBBCCHERCroJIYQQQgghhBAX8WjQPW/ePHTp0gXh4eGIiYnBqFGjkJaWZrNfamoqRowYgcjISISHh6N79+7IyMgwv56VlYUJEyagVq1aCAsLQ8eOHbFy5Up3NoUQQgghhBBCCLHh0aB7586dmDJlCvbv34/ExETo9XoMGTIERUVF5n3S09PRu3dvtGjRAklJSThx4gTmzJmD4OBg8z4TJkxAWloa1q5di1OnTmHMmDEYN24cjh075olmEUIIIYQQQgghADy8TvemTZs4Py9evBgxMTE4cuQI+vTpAwCYPXs2EhISMH/+fPN+jRs35hyXnJyMBQsWoGvXrgCAN998E59//jmOHj2KDh06uLgVhBBCCCGEEEIIP6+a052XlwcAiIqKAgAYjUZs2LABzZs3x9ChQxETE4Nu3bph9erVnON69+6NFStW4M6dOzAajfjjjz9QVlaGfv36ubkFhBBCCCGEEEKIhUd7utkYhsGMGTPQu3dvtGnTBgCQnZ2NwsJCfPjhh3jvvffw0UcfYdOmTRgzZgx27NiBvn37AgBWrFiBcePGoXr16tBoNAgNDcU///yDJk2a8J6rrKwMZWVl5p/z8/MBADqdDjqdzsUtdYypXt5aP2f5c/v8uW2Af7fPn9sG+Hf7qG2+y5/b589tA/y7fdQ23+XP7fPntgG+0T6pdVMxDMO4uC6STJkyBRs2bMCePXtQt25dAEBmZibq1KmD8ePHY9myZeZ9R4wYgbCwMCxfvhwAMG3aNBw8eBAffPABoqOjsXr1anz++efYvXs34uPjbc71zjvvYO7cuTbbly1bhtDQUBe1kBBCCCGEEEKIvyguLsajjz6KvLw8RERECO7nFUH3tGnTsHr1auzatQuNGjUyby8vL0dYWBjefvttvPnmm+bt//d//4c9e/Zg7969SE9PR9OmTXH69Gm0bt3avM+gQYPQtGlTLFy40OZ8fD3d9erVw+3bt0XfLE/S6XRITEzE4MGDodVqPV0dxflz+/y5bYB/t8+f2wb4d/uobb7Ln9vnz20D/Lt91Dbf5c/t8+e2Ab7Rvvz8fERHR9sNuj06vJxhGEybNg3//PMPkpKSOAE3AAQGBqJLly42y4idO3cODRo0AFDxdAEAAgK409PVajWMRiPveYOCghAUFGSzXavVeu0v1MTVdTxwMQd703Pw4oCm0KjdP+XfF34HjvLntgH+3T5/bhvg3+2jtvkuf26fP7cN8O/2Udt8lz+3z5/bBnh3+6TWy6NB95QpU7Bs2TKsWbMG4eHhyMrKAgBERkYiJCQEADBz5kyMGzcOffr0Qf/+/bFp0yasW7cOSUlJAIAWLVqgadOmmDx5Mj755BNUr14dq1evRmJiItavX++ppvmscd/vBwDEhAfh8e4NPFwbQgghhBBCCPFtHs1evmDBAuTl5aFfv36IjY01/7dixQrzPqNHj8bChQsxf/58xMfH48cff8SqVavQu3dvABVPFzZu3IgaNWrgwQcfRNu2bbF06VL88ssvSEhI8FTTfN7l20X2dyKEEEIIIYQQIsrjw8ulmDRpEiZNmiT4erNmzbBq1SqlqkUAqFSergEh7pNbVI6IEC3UAd5z4ecWleP9jakY27keujaK8nR1CCGEEEKIg7xqnW5CCJHqWm4x9Ab+vA1ypGTmo8O7iXj8xwMK1Eo5725Iwcoj1zB2UbKnq0IIIYQQQpxAQTfhpaKubuLF9py/jd4f7cDjPzkfKC8/mAEASL6Y43RZSrqSU+zpKhBCCCGEEAVQ0E0I8VrF5Xpcv1tis/3X/ZcBAPsv3nFzjdzHC1ZzJIQQQgghCqCgm/iMG3klyCvWeboaxI26f7ANvT7cbpPYT8l4lAEFt4QQQgghxHU8mkiNeC9vG1yeU1iGHvO2AwAufzjcw7Uh7pJfqgcA7L5wGw2jw8zblQiT1xy/jg0nb6BKkGe+BovL9VBBhZBAtUfOTwghhBBC3IOCbuITUm7ku/wcaVkFyLhTjMGtarr8XEQeJR4CrTpyDdvTsvHpf9ohWKvG9D+OK1CqY/QGI1q9tRkAcOH9+6FRu37Q0d3icgRr1QjWUpBPCCGEEOJONLyc8PO2rm43GPrFLjyz9DBOXL3r6aoQOxwZXv7KXyew4eQNLDuQoXyFZMplTZPIK3H9lInconK0/18ius/b5vJzEUIIIYQQLgq6CbGSllXg6SooJq9Yh3fXp+D09TxPV0Vhjg8wv1tcrmA9HCNlHrmSM82PZuQCAO5STgRCCCGEELejoJvwUlXGru57/Cmx1nsbUvDTnkt44Os9nq6KU6xXsPOnxN6OLs9XUKrDvvTbMBrtvxkSdiGEEEIIIS5CQTepdDJyinH1TuVYA/msH/XaK8Yb1qCXEATbq+XYRfvx6A8HsDT5sv3T+dNTCkIIIYQQH0NBN+HlDXGJkorK9NAZjCjVGdDn4x24b/4OlOuNvPv6U3wS4Ge/RxM/+hUJBtf22ph6L7ngP8eu2z0H9XQTQgghhHgOBd3Er5XqDFh+MAOt396MPvN3IJ+VtKqk3ODBmrlHgJ9E3dbTHYz+9GREwLGMu5L2kxZQ+//7RQghhBDirSjoJrzYIU5xuR4GF3SVnb6eh3fWnnFpYqt316dg1t+nAAA38kolHeNP4Yna34YsyGQwMigu13M3ekHArmQNpOQgoJ5uQgghhBDPoXW6iaicwjJ0em8r2tWrijVTeilatim5V36JDh+Naa1o2SZiQ2+FghUviMkU4y893dak/o4e+HoPUm/k4+icwa6tkAxGI4PzNwvNPzt7uUl5L/zpmiaEEEII8TXU0014mTpIt6VmA4BL166WkuzLenjxhexCPP7jARy8dEf0uMoWbGTllWJp8mUUlVX07vpLT7dN9nKJx5nmPe8+f0vZCjnhrbWn8fhPB8w/O5vkTMrhlWE4PiGEEEKIt6Kgm/gE617pZ5cexp4LtzF2UbLocTbBhn/EoILGfLcXb605g/+tSwEAqN3c0331TjEOXroDg5HB9rM3kVvkmqkDvpyN+7f9GZyfne7pVmgfQgghhBDiGhR0E4d4Ouhx5fzspcmXcel2kQNHupbewJ9tnS3z3vuyI61ihIK7h5ffN38Hxi5KxmsrT2LSksMY9d1eRcp1thVSL1e9wYhtqTc5eQauFgI/7rks6f1nK9cbJV1Hzn6UpHwWPf15JYQQQgipzCjoJrxMw7n55j1/s/08us/bhut3S5Q5l52IyjRU2l3OZhWg/ydJbj2nPa/+dQJt3tmM7Hx5DxvkxNzbz97Ev6duyK8cj1VHrwEAruQosx66s6PkpSQbA4Dvd1/EU78cxpjv9pm3fXJKg482n8Oygxk4k5mHTaezJJX1yPfJ6P9JErafvalI3aTIyivFrYIy23MInOKttSn4+oxa9gMFQgghhBAiHQXdRBTfzfonW87hZn4ZPttyzuXn/zzxHFq/vRlbU8QDF0F+0sG38sg1lOqMWHYww/7OsPzepM7p1huMmLTkMJ7//Sju8AwJLyrTY+ZfJ8w96J7mqo7b9ScqHjpc5OmhTr1RgOFf7cFzvx3ByWt37ZZ19N6SX8sPXhXdL4PnwcS5m7Z5DoxGBjvOZuN2ITeoNk2hKC7Xo/u8bejy/lYYrdKVCwX2yw9dw4V8FQ5ezhWto1x6gxH70m+7/YEZIYQQQog3oqCb8HJF/i2xIa4l5Qak5amgs+px+3LbeQDAL8lXHDunv0TdMpnea6nDyw2s3w17LXOT75Iu4K8j1/Dk4kPKVNBBO85mY92JTNmJwaTuLrXUC9mF9neS6JW/TthsG8eTq+CvI1fx5JJDGPzZTs52U9uyWFMuDFYNNtrpyFZ6ScCFO9Px6A8H8NQvnr1eCCGEEEK8AS0ZRkSJ3YrLGbq87EAGvth6Dkuf6oqNJ2/gViG3N/WlP09ie5oaZYnnMefBNo5VlodYLOGL01yts7gLkTu8nF0u39uSeVfasHZXUUGFv49ew4w/bQNUt9WB9V7KuXZM+85ZfRqRIVqb12/zDAfPLbZ98JGYks37mhKJ1JT+KPx+oGJExv6L4qsLAMCRK3ewNPkKZie0RExEsMI1IYQQQgjxPAq6CS8psZqczNhv/HMKAPB/K0/ixLU8m9e3p1Us6fTbgauKBt3Wvevs4NIXl1GSOgLB1G5HspfzjUjwhqTvC5LSHT5W6q9arJ2OXy4MLt8uwq/7+UdrSC9WaF152+3W17a9a13pRGtyes4fWlDRq19YqsdPE7soWg9CCCGEEG9Aw8t9TLneiO1nb6LQgbmSU5cdxdRlRxWri0rGfGET62GvjmAYxuHh7+zh5pdzivHHwQybIe3+wNLTbXmjyvXC7bQ7DN8Lom5npjxIvepc9Rim3IXXGGP1/wDPAwI3P19y5IHW5RzvWzGAEEIIIUQJFHT7mE8Sz2PSksN4dulhzvZ1JzIx6tu9uJbLny06p7AM60/ewPqTN3gTZdm4F+GcvZEvuIvUTlR2lnOxwE8qhnFiji5rw0ML9uH1v0/hl32XJZV19U4xHvtxP3aeu2Xz2unreVh/MlNapSQwGBkczch1+P3SGyoaepI1quCzRNvEdwzDYOZfJ/DN9guOVdRJs/85hcd+3G+/Z1TBoD+3WIckgYRwqSLXu8MPehhlqi94zfNst97Xbk+31c86gxGbTt9Azr2kbQzD2CRns8Z+Xek54mLySnT4Zd9l3qzthBBCCCHegIJuH7PicMVSTPvSczjbpy0/huNX7+KdtWd4j2PfA0sdSnr1TrFoAjNHhi7zcUXSNhPrpvK1/OAl+/NOgYqEV3sv5OCJnw/avPbA13swddkxHL4srSx7vtx6DmO+24dXrZJsLU2+gp7zttld/7mwTI/x3+9Hxh3LQxjTMl5sRzPu4q8j1/A1K+i26SRlGMlzye3JLSrHmuPXUaozAKiY+7v3Qg4OXMqxc6Q8DMPgJmt5NfY1/+v+K5joZEI4OSGl3fnUTsanpoCaXY51kG3vFNYx8oKkdDz321GMvrd02rO/HkH/T5NQpjfwHv/GP6fQ88PtyLuXhM+NMTde/esE3l57BhMX234uCSGEEEK8AQXdfqagVJklelQQ7/UDuEOXpTp3Uzzrs9SkUCU6/pv/lMx8fLYlTdZSRVtTb5qDQDF8Ca+sfb/rIgA4NPyfbeHOinLWnuD2nt8uLENmXin6f5KEG3ni66QnX+QGsnzBHV+72ftNWnIIo77dKxhsydXh3URM/+M43l2fwtleVKZM+SZz1pxGtw+2mX/25Oz9tKwCRc4vo6Ob58EJzz4i0f7Ge+u1mx7aJKbcxJWcYsHEaMsOZCArvxR/Ha5YHs1er7iSEu8tJ3gmU/z7ihBCCCHEUyjo9jNCgbAjS2fZO0Jq0K3UgwATsWAh4avd+Gr7BXyyJU3gWNttRgaYuy7F9gVrEpq7JeUmdqRlo83bm/HtDseHbAdI+GQ+/cth+zs54LnfjmD1sevQG4zYfjYbJ67lYf3JG06XawrkAEt2a5Picj0OXMzB0uTLiiRy+22/1XrmCsSAjg71v363xKXD96/kFGPzmSywG2nq6TblU+AbXs7eZJNwUOCzLfX34ItJCgkhhBBCXIWCbh9jb2i4UkO+z2Tm2R32KnYqU09XdkEpHvh6jyJ1MpFyO3/6um2G9Ipj+Y9efjCDdzsbX3PfW5+CmVZDwCctqRi6/PFm/sBfCo2EqFuJnj2+Nl3ILsRLK447Xba1F34XTuJXqjNg3Pf78daaM9h7gX+ouVJD3B218ohleL7cbN/WIxYcIXbOyb8e4e5rBH4/cAVNZ/+LpLRszrGmfwslXtt74bbgKBepg1uUSJhICCGEEOIvKOj2M47Oj06/VYisPMsc2K2p2XYTjAUIRN3/t/Iken64HfmlOmxN4U9Y5Qybedq8SyYBd4ttE8Y5EwtY9+wzDIMf91zCX0euWW13/ByWczlfhrPkNuPKHf4kflJcvWMZKp8hUI4r5/7LxX5vjEZG0vQEMWez8rF47yVOpn/ZdWL3XIPB7H9OAwCm/H6U93cp1Bv92I8HBM8h9cGH0OjyYxm5mPnXCUp6RgghhJBKhdbp9jH2AqHd52/jnbVn8M6I1pLLvF1YhoGf7rTZbj0n2JpQELTi3rzONceuI7pKkOR6AI5leWYY27owDIPHf7INHpyJh23P4URhdrBHLNgLxErKDfgsMQ1DWteSfyKRN1xO+8oMwKDPHR/R8A3PUHz2PHKpy9MJcWR6hVTjf9iPA5fu4NicwagWFij7eAYMhn2xG0DF771PsxoO1WMdawoAO+hVqVRWQ8krrmUjp/db2jmk/hqE5nSbErPll+qwaEJn3n3SbxWiflQotGp6JkwIIYQQ/0B3NT7sqkCP4BJWD3W53ojLVpmurW+H07PFk5sJkTKnu0qw8s91rAMo/l484PR12yGyziR4sunpdrgk+9Ss4eVi86kHfpqEjzadxQ+7L+E/C5Nln0es51JOoPr+cbXscwsxvc3LDtgf8i+VKx+QHLiX/X7bWedHdby15gz6fZLE+9q1XPHEeV9tO2/+N3v0h/Vv2Lyut1XPuJLsDS+/eIs/+/6a49cx8NOdeMpF+QoIIYQQQjyBgm4fw76XfVtgeTC2x37cj36fJGF7qiUgkLKMlhT2hkBLLVfuXF0pw8uF5r/KDb4KSnVYfjDDZm3z09fzZM/rFcIwDJ5Zehhvrzlt3qZhvbm5PMPkTdJvFXEesihJavN0BiPyykWCd4ZxKPs5ewiy1CvkWm4xdDwjAxSPuRUsUMr7/OPuizgv4+GY9bMl9vMi85xuq95vIZtOZ1nKkXh+KW3SG4yYYZU74Oc9lwAAu87dkngmQgghhBDvR0G3DyuUkBX80OVcANxs0dbBoqOZhtUqFQrL9Jiy7Cg2KJDdGgDK9EbO+spSSFkyybJdXltfW3kSs/4+hUlLDnGGOD/w9R7F4q4zmflITLnJWROdPbxc6anMP+y6iG2pFcssleiEr6HsfGnzbtefzBJ9feLiQ4h/e4vk+pnae4EVZIoNqijVGXAttxjrTmSi90c7MPufUzb7OHKJT1pyCGUyMpYr9RCGz3sbUmXtz6mLwHvHGV4uUtbcdayHewpdjOezC7Fk32X8few6ZzulXyOEEEKIP6Kg24fJCSDZ+zIAcgrL8PKK49h/McfhobcqlQo/77mEDSdvYMoy/szUjpQ9bfkx0ddte7pt9xF6kPDl1vO824X8e6+X7/jVuzY9+0Jtszfq3mhkkM16sGBd15v5pbh+1zKUWChhnSMOX76D9zemmofvzvrbNkA16fPxDkll3i4SD853nruFchkJwkzv35Z76y/bM/DTnej90Q7zdfPn4Wt2jpBm+9lsrDkunnX8623yrid3YV9RKlgPJbfdR+oDAyUzyMt9kEAIIYQQ4qso6K4k2PfURobBu+tT8M+x63jk+/2yMgnfyGMFgyoVCkp1oud0JJ4/eG+OrFR8Dx+EYgjrnjU5bBKpCbTOXlgy6+9T6PrBNvMQWvZccaORQbcPtskqTzoGN616r61/dkRJuXOZu63xBXZiDzLYDyiEODpnuVSkpzunsAyfJp5zqFxXE1uuzvTZkDrChf3WW/8etqXeRIoCS9eZ0EpjhBBCCPFHlL3cR1y5U4yPT6o5w13l3KBaz99kL8skZ03mZ5da1gNWqYCwIPdfQkXl3CHRprblFVseADiRL02Q7ZJhwvuJBTSm7O4fbTqLPs1rcMrlS0ClV7AxSnSa3y4sM2elz84vxdc7LjpUjiuHYytFLfB+MWDwyZY0q22OccW7wF5T3Drzu+kBBMN6niD2q7DuNTdJycxXPOGZK7PME0IIIYR4ikd7uufNm4cuXbogPDwcMTExGDVqFNLSbHtoUlNTMWLECERGRiI8PBzdu3dHRgY3s3FycjIGDBiAsLAwVK1aFf369UNJif0eMF8xe/UZXCvi3jwXlunx2ZY0pGUV2D2eHQQaGUZS5nE+p67ncX6uwgq6Z/x5HEuTL5t/ZhhGUmBVztObKJZlvM98/mHP1+5aHiS4IqA7eS3P/k6QltUdsCQKYyUqh4Gn3T/vvSSpPCmcXXoLAD769ywKy/QY891edLXqlVeEC9bj1smYm83GziLPti01G8sPXnWmSm4j9CtnB7hiD4mEXjqfbf97hxBCCCGEeDjo3rlzJ6ZMmYL9+/cjMTERer0eQ4YMQVGRZTmZ9PR09O7dGy1atEBSUhJOnDiBOXPmIDg42LxPcnIyhg0bhiFDhuDgwYM4dOgQpk6digCBG2ZflFdim/DqbFYBvtp+AUO/2CWrLIaRHhiybTljmzCLnWX776PX8dYaS9IlqWFv2k3bm/cSnfCQ5WKr4cymoIAdHCjZOyybxLfWFGBzhpfzRDhX7yj38Gi/nbXXpbhbosNPuy/haMZdp8oRnBPPu02FsxIeLgl5Z12KQ8cJLRV9lW/5Lm/q6maxWTLMPLyctY/I9wE7OFfioY0YHxj8QAghhBAim0eHl2/atInz8+LFixETE4MjR46gT58+AIDZs2cjISEB8+fPN+/XuHFjznEvv/wyXnzxRbz++uvmbc2aNXNhzT3AybtRTqZiBg71Jj776xHOz58lnsPMoXGixzhaazkZ1fmGpPL1GMvFt/QU57yCw8ullW96MLCDtb6zqx8WsJcXc3Q0AMMwKCwTnssvVWE5f+Z0vsBOyTW75bh0u5h3uzPrvVvTGx3rhXcW+zMmdslyEqG7NuamoJsQQgghfsmruoLz8iqG70ZFRQEAjEYjNmzYgObNm2Po0KGIiYlBt27dsHr1avMx2dnZOHDgAGJiYtCzZ0/UrFkTffv2xZ49ezzRBJdx9l6Um72YUWwE74+7hefzMgwcrvjDC5LxucQkVXw36koEMuMWJYufV6BxUkcRGI0M7hSVY96/ZznbXMX6fXL0wQTDKNPj+UUif+ZvvpIPXpaXXE8pP+y5zLudb+69ox8qVw/KsJnTzTMyRIzQnG6pZv9zCg8t2Ae9hAz27HP9fuAKUm8ol6SNEEIIIcRTvCaRGsMwmDFjBnr37o02bdoAqAioCwsL8eGHH+K9997DRx99hE2bNmHMmDHYsWMH+vbti4sXK4K+d955B5988gnat2+PpUuXYuDAgTh9+jRvj3dZWRnKyixZm/PzK27sdDoddDrne/BcQW6AZN2O86w1j8t1OqgUGtNaWCa8zrPBaIBOb38tcT5pNwt4h53zKSsvR2AAAz3rXHqD8+2zN3y6vJz/WmEHJuxryvp3Uqo3oOO7idxtAmUqwfohQUlZuUPlGIxGGAzOZywXmqtuMBi89nNowhdA6vXuq/eSvXKS1zG4dMvyWdLpdNCojChn1VVv0AvWnT0iQq+37KfX214DfL+73++NUtiZJr4EHMMADOth2ex/TgMAzr87RPQ4lcryAIF9bqHPnasYjAzUCi7vJ8bdbXM3f26fP7cN8O/2Udt8lz+3z5/bBvhG+6TWzWuC7qlTp+LkyZOcHmrjvRuwkSNH4uWXXwYAtG/fHvv27cPChQvRt29f8z6TJ0/Gk08+CQDo0KEDtm3bhp9//hnz5s2zOde8efMwd+5cm+1btmxBaGio4m1TQmGhGnL6mTZu3HjvX7a/4qSkncjJCYAiAx2MRsF6paSkIDsIANTOn0dEwmfbMau9AVcLAVN7i4tLBOullM1btoDv/S1izTm3/B6AxERTgF1xjI7nwUBi4lbeMpVQXlYO9nuycdNmh86VnZ2NgELAVQNlTp48gaAbx+FFX082CgqLYH19HTl+EmE3T+BOGXC7VAVXXvfvrJO+xnV5eTmWJFuG52/avBlBauBuGWB6j0+ePAVN5sl7e3Df99KyMpjampycjMMHgNhQ4HiObRtTU1OxMS/FpgwAOHDwkM3+bEVFhShRAdbvK/szxIuxfDea9q0YjVHxsuVz5zpXCoGvT6sxvL4R/Wu7b4y8O9rmSf7cPn9uG+Df7aO2+S5/bp8/tw3w7vYVF/NPRbTmFXe106ZNw9q1a7Fr1y7UrVvXvD06OhoajQatWrXi7N+yZUtzcB4bGwsAvPtYZzg3mTVrFmbMmGH+OT8/H/Xq1cOQIUMQERGhSJuU9uX5PUCJtF8qAAwbdj+WHboK4KzNa/f16Ysd+ak4n+/8kF2NRg2djn/YaMuWrVC/Wgh+SDvu9HnEZJWo0Kv/4Ipl0E4dAABog4IAnWM9uVINHjwE/3dwu+g+CQkJ0Ol0SExMxODBg6HVajE9eYvg/n37DwCOyEuMJ1VgUCCgtzyNGzhoMHCQPxO8mBo1YtCkZhVsz7ysYO0s2rVrh/vbxWJ6svd+wQaHhABlpZxtqy6r8eFTQ9BsjvDv1xOCAoNQyPoshDTuhJa1whGoCcDbRyuutTbx8UjoUPHda319BgYGAvee4p5X1cHGUzcxuGUMhrWviV8vnOLs27JlSyT0ash7jXfu3Bnfnz0mWM+wsCrQBKhwo7iQsz0hIUG0fS/t32Iel56QkIAjV3Lx3O/H8X9DmiI0+5T5c+dKD36zDzqmEKuvqPHx0+I980qw/k7xN/7cPn9uG+Df7aO2+S5/bp8/tw3wjfaZRkzb49Ggm2EYTJs2Df/88w+SkpLQqFEjzuuBgYHo0qWLzTJi586dQ4MGDQAADRs2RO3atXn3uf/++3nPGxQUhKCgIJvtWq3Wa3+hcnttN6ZkY+5624AbqAiU1UJpmWUSm9urVquhUru2l9skv8wItdpyObsjIZNGa//jw76epFxfAS58v1RW11DypbsOFqRCgELXDx+NRg21xls/hxWEpid74/eH9Wd02h8nAAB7Xx9g3hYQoIZWq0Up76oBrNERpyuGiCemZuOBdrVt9lSr1YLvgUYjfm0bGAYXs4tstst5T7VaLZ5fdhx3S3SYtSYVX/Zwz/e6irVShjuvAXe0Lf1WISKCtagRbvs309W8+2+yc/y5bYB/t4/a5rv8uX3+3DbAu9sntV4eDbqnTJmCZcuWYc2aNQgPD0dWVsWSVJGRkQgJCQEAzJw5E+PGjUOfPn3Qv39/bNq0CevWrUNSUhKAihvKmTNn4u2330a7du3Qvn17/PLLLzh79ixWrlzpqaa5gLwoMvWG8HzotcczkZzu/NJRAKAWW2qIYdyWjdj6NLyJrpQ+pwtO4cpE1tbVnbZcuNfRXjnWAbySVFB5LKO3VB5dkk4moY8oO2kfA6BUZ0Drtzfb7uemD/GVHOkjedhU4F7bztS2uFyPF34/isGtauKxbg2cKMk/3MgrwcBPdwIALn843MO1IYQQQnyXR4PuBQsWAAD69evH2b548WJMnDgRADB69GgsXLgQ8+bNw4svvoi4uDisWrUKvXv3Nu//0ksvobS0FC+//DLu3LmDdu3aITExEU2aNHFXU1xO7n2vWEj01fYLTtVF8ong+LJUcjEMwwkOlFgyzP5J7e9yt7gcYVrpAerS5MuO18cOpX4XDMO4dOkolcq1Dx+U4K5AVAlSf1WpN/J5Pzdymno5x7an2pcsTb6CpLRbSEq7JSvodnX6tMu3i3CnuBwd61dz8Zm4zlyn7PGEEEKIEjw+vFyKSZMmYdKkSaL7vP7665x1uv2N3Fv8a7klLqmHNXvLY7krNGEYbiDkyqW3zOeU0Lr2/0vErlf7SC7zxz38Gb2VoORb4uog49T1PBefwTlSlr/ydhdvswJkhsH0P47z7ifnYc1v+zPw3qh4J2vmHGeuzYJS78yO2u+TJADArpn9Ub+6dyb7JIQQQogwr1qnmwiT27O24dQNF9WESyzmtg6EXcnIcJcJc8fw8tXHrkvaL+ncLRfXRBrleroVKUbUWDtrpHtafqljS+F5gtBn9ImfD5r/zQAViQh5CP2695y/La8eLn9U4zx7DxGFuHLkB9uFW9KWUVSK74znIIQQQrybV2QvJ/Z562hWuz3dbqq3kWE4gXapQEZ1Jb2zLkXSfiqoYDACr606hV7NYlxcK2FK/S4YuHZ4eXG582uAB6iU7dn3ZVKCXbFrQ+i1v45cc7BGriWW3NEeR4NuqbILSlGmM6JeFPVWE0IIIZUJBd0+wlvjhztFwstyMWBw/qZ7emZSb+SjWmigW84ll0oFHL6twj/pN/DPcfeMQOCj1KiDvRdyHE56JcWsv0/Z38mOQE2AWx68+AIpcaSUqRKeUq43IlATIPizXAzDIONOMepHhdoE6K7u6e76/jYAwPG3BqOqA99XtwrK8MqfJzC+Sx3ZxxJCCCHEc2h4uY9wV0IypSmatE3EjD9P4Mklh9xyLrlUAIq8YDSykj2/7soZ4CgKuOUR7+lWaFqCg4F98zf/xbyNqQCAz7akofmb/+JoRq75devA2V78O+/fs+j7cRK+S0q3eS3ATcPEhYby2/N/q05h1dFreHjRAYVrRAghhBBXoqDbR/hizP3jbtclBfMl7prvaU8J7xrMxN9JufzEvl684atn0a6LACwP8d7fkOpwWd/fK+vjzWk2rwU4GHVLG8JveSd9YX47IYQQQpRDQbeP8IYbX7myC8o8XQUvQTfYxHf50vJogHMPuYSGl9/ML0W5vmL0RFZeqc3qCFLOyT7EFQ/iNp3Owi4vSdpICCGEEC6a0+0jfO3Gl7DR7454N0cSqXmDw5fv8K4t7ijrju7icj3eXnMGfx25hha1wvHasDhMWnIYQ1vXxKIJnWWV7crv8OyCUjz32xEAwOUPh7vsPIQQQghxDPV0+wovvvEl4t5YnQID/f6Ih0iLSYV3UurSLdcr9yFgGAZlegMeXshdWu5sVj5yRJI72qO2irrnb0ozZ2k/m1WARTsrhqZvPnOTs5+Ujmv2wwGle7pzi1yzvriv5hIhhBBCvA0F3T6Cbn182/oMtaerQCopKQnMRGMrhb58TD2xSpm27JjNtmFf7HaqZ946KVtyeo7jhVlh18uVS5NRoEwIIYR4Hwq6fQQNLyeEOELKV4d4zO2d3z1bUm7a30kmh7OX8wTRWXmlmLTkkHmetYFxXU83+3dEfyoIIYQQ70NBt4+gGylCiCOkfHUoOae7oNQ1Q53ZXLUkHHt4Od9c8QOX7kgu641/TmH72Wz89+eDmPdvKufBqdLZy+nvAyGEEOLdKOj2EXOGt/B0FQghPkjKcGOx3my9zERleSWuD7pTbuS7pFx2KCxndBFfCJ2VV2r+96KdFzkZz125jCDF34QQQoj3oaDbRwRp6FdFiLdbfjDD01Ww4e5e0IJSvXtP6CJGhpEcHPPtZ72Ns2SY49XiJfY7vlNUjqIy//idEEIIIb6KlgzzEbTSMyHeb9bfpzxdBRvODi+Xq1RnUK4wd2NFyko/rFByaTMxFSMbKtqRV6JDx3cTAdBSYoQQQognUfepr6ComxDiACnDy99el6rY+c5nFypWltLK9dLngssZXq4z2C+XXZ4rw2922eduFihWFiGEEEIcR0G3j7BeyoYQQrzRaytPeroKZiV6y0OHDSdvoPmb/0o+VmrH9PGrd3H6uu0cc9vh5Y5lGNdLCOiF5uSLZWNnGMa3RyUQQgghPoSGl/sICrkJIUSe1w9psL/8NJrHRmD+pjS7+zuSSO2jf89K2o89vFxq2bvP38KTiw/Z3Y9dHPvfYg9rX191CisOX8WWl/ugec1wSfUhhBBCiGOop9tHUEc3IcQRucWuzybuzVafuCEp4Aa4w6kZhVclEwqMxTz36xHZ2ePZxP5srDh8FQCwcGe6w+UTQgghRBoKun0ExdyEEKKsE1fvCr4mZ043H+u1uB3p6XZkWhF7qHkA63ijmxK5EUIIIcQWBd0+guZ0E0KIskZ+uxfF5ZbltBxdp1uK7WezFS1PCLvanKDbBWvHGY2M27KyE0IIIb6Mgm4fQSE3IYQor5C1hjU7fLyRV6poQPm/9SmW8yhQ7Ip0y59vofLYz2oNCgfdDMPg/i93Y/DnOynwJoQQQuygRGq+gqJuQghRHHsYOHt5tQe+3mP32IycYiRfzJF9TsnDy0Ve25cdYHc5OHbQrXRHd36JHmn3liTLLihFbGSIsicghBBC/Aj1dPsI6/mBhBBSmaw5ft1lZWfd69WWG5iO+FY4MBebESR2mrxiHfZfzJG0vrppF+Elw1w4vJz+JBFCCCGSUU+3j6Ap3YSQymz6H8ddUu7+izmYtvwY7msWjQEtYmQde9fBzPBiAfCwL3fhRl4p5o5oDZ1RPIW6qRShzOjsoNuRIeAumAZOCCGEVErU0+0jKOaWTh2gQhytO0sIkWBp8mUAwO7zt6HU1OSUzHycvJYn+LpYMHsjrxQA8PbaMyjViQfdfME7N3s5e1/+MpQYRUXBOSGEECKOgm4fQT3d0qkDVFAH0BtGCJFHypBuKR5asM/emRQ5z28HrkJvMAqWxp3Tzb+X0NB0e7z9b9KnW9Lwyp8nFPudEkIIIc6goNtH0Jxu6dTefjdICPFKSsVnJTqD6OtK9ah/8G8aeny4HeV6S484uw0qJ4eXe7szmXkYuygZR67csXnt6+0XsOroNZzJzPdAzfgZjQx+2nsZlws8XRNCCCHuRkG3j6A4Ujpv6eV+bVicp6tACJFh4+kbbjkPwwA380ux9kQmdAbxIeT23Coow6nrwkPZTR75fj9Sb8gNQL07UH/8xwM4eOkOHlqQLLhPmV78AYg7rT5+HR9uOofPT1M6HUIIqWwo6CZ+J0DFfUjx7sjWHqlHdFiQR85LCJHu0OVc87+PZdx1yznzS3To9sE2vLj8GD7ZnIbXVp7g7a2Vij2Emh0mF7HWID+fXYgnfj7o8Dlsz8n6t2KlypMrIZGdN40uP59d6OkqEEII8RAKun0E9XRLZ93TPaFHQ6+oByHE/3297bzdfRbuTDf/e9Gui/jz8DXR3lp7uNnLLT+8vuoUZ7/sgjKZJfOvYV6xQWZRhBBCSCVGQbePsDenu0vDam6qifdTB6jQpWGUp6sBjZqCbkIqm08Tz9nd51ah3OBXnFAytBQJw8nF/7awetBtYm7fiLp9o5aEEEL8HU0s8hFiPd2Na4ShTtUQHEKu8E6ViEqlwmvD4lAzIhhDW9f0WD20anqmRQixdSWn2GVleyLI9PcM4UczcrHpdBZeGtQMoYH8t02lOgN2n7+NHk2qo0oQ3VoRQgjhoqjAR4j1RXw4pi09zWdRAQgN1OD5fk3QuEYVj9XD3cPLh7Ty3AMGQiobVwSajpYpJzH5aTtJ1/QGI1Iy823qYn0K7pB26ed3NyXqNua7ffh+10V8uVV46sD/1qfgmaWH8cLvRwX3obFPhBBSeVHQ7SNUIl3dASrvvulxN2+Z/x6oce/H68lejdx6vsqkc4Oqnq4C8TLt5m5RvMxHfzjg0HFyAuBnlx4WfK1Mb8Crf51Awle7sYA175z3nHIq6EGOPMiYseI4Hlqwz2aZtQsiidCWHcgAAOw6d0v2+QghhPg/Crp9hFgcWbtqiNvq4QuEhv+5W7BG7dbzecvDBn+0/Omunq4C8TL5pXr7O8mUfDHHoeOMMgLLYqs1xE1zsz/89yzi3tyE1cczAQALdqQLJmjj+9lbnL6eh4cX7HOqjL+PXceRK7k4lmE7ZevczQKM/GYPktKynToHIYSQyoWCbl8hElDVrhriM70O7vDlI+09XQUAQLDWvR8vf4+5p/ZvivFd66FTA2WSBmoouzzxR3b+GDAMf8C80E7PtszTeMxjPx7A4SuWYNmZevId+9yvR3DiWh4mLj7kRMmEEEIqG48G3fPmzUOXLl0QHh6OmJgYjBo1CmlpaTb7paamYsSIEYiMjER4eDi6d++OjIwMm/0YhsH9998PlUqF1atXu6EF3qOkXPleF1908YMEtK1b1e5+gW5Icubu4eViUxD8QVytcMwb0xbZBaWKlDdtQDNFyiHEm9jLKp5XosOEn6St183+SvGVOd15JfbX7pYq36oslQq4U1yuWPmEEEIqD48G3Tt37sSUKVOwf/9+JCYmQq/XY8iQISgqKjLvk56ejt69e6NFixZISkrCiRMnMGfOHAQHB9uU98UXX/h94CFka2rlGuq2/JnuvNsDBHovuzXiLiEW5IaAWBPg+DmiwgIFX2sZGyG5nLGd6zpcB29juq+/W+z8TXV0lUAajk/8htyh3nsu3Db/W2zJMLHAmh3cf7IlDVfvOJ+RPa9Yh02ns1CmNzpdlokzDwSe+uUw1hy/rlh5xDfoDUbsu3AbRWXUmUEIUY5Hg+5NmzZh4sSJaN26Ndq1a4fFixcjIyMDR44cMe8ze/ZsJCQkYP78+ejQoQMaN26M4cOHIyYmhlPWiRMn8Nlnn+Hnn392dzPcon5UKLQq9/+1Xzyxi9vPac83j3ZAjybVZR1jHWAF8Qz9frq3sonInAnqHMl8znc+Oe9T/ahQ2ef0BOvkRo5Y9XxPBWqirJjwIE9Xgfgooyt6nXm+T3QGI77edr5irjPrPGtPZOIhJ+dRA8CEnw/gud+O4IttF5wuy8TZ9cTfWnNGoZpQ3g1f8V1SOh798QCepCkEhBAFeUfGqXvy8iqWMomKquiVNBqN2LBhA1577TUMHToUx44dQ6NGjTBr1iyMGjXKfFxxcTHGjx+Pb775BrVq1bJ7nrKyMpSVlZl/zs/PBwDodDrodMoNTVNSRKAKczsZ0LR9N4z98QjnNal17lAvEseuii8XY61X46qy9ne1wS1jMKRFtGCbBd8LqztRvjW0H+5YGz/uueR0HU30OsefkqtFbs6EerUMetvzta4lfck0b02MZGI0GKDT6RS5ca0dEQijUXpvmju+F2YMaor3/01DgQsSdBH/pjdYkqPp9DrodNI/JEbGiNdXnbDdbmQ45ZbrdFh28Co+TTyHTxPP4aleDTj7ZxeUOf05OXmt4u/TmuOZiI+X97kT2tegNzhZL8v3otHIcH6W+3fIaDDa3cfXmdrly+3742DF9MWDl+9w2uEPbRPiz20D/Lt9/tw2wDfaJ7VuXhN0MwyDGTNmoHfv3mjTpg0AIDs7G4WFhfjwww/x3nvv4aOPPsKmTZswZswY7NixA3379gUAvPzyy+jZsydGjhwp6Vzz5s3D3LlzbbZv2bIFoaHe29sXpgWOHDwA61/bxo0bbbbxGVsrB8euyvuV//vvv5LKdpc41Q38+2/mvZ9s61XxXti6nRMA9sAOXVkJrLtydu3ayVumo3bt3uVweeVlpTDVb14XPd4/rkbhvRvpgvx88HVD7d+fbD7fq/F6qFRA6kHpbSouLuYt154O1Y04luP6QTPHjh2D6ioDo14NZ9PGbdy4EeeuqQBIyzCfmJgIV38OUk+fRKdqKiTdoPyWRJ5z587D9P2WmLgVVbSmV+xfs6fSr+NCvu3nqajcgKNHj8L0Gfn3303YcdnyPfrT3is2xwh9/0pXUV/TQ/GKz539/bnn5rZ5/4EDuHNW7gNFSxnl5TqYvm+ys29CV64y/8xtL19duC5kWN4/+23zbb7cvuISy98Yvt+lL7fNHn9uG+Df7fPntgHe3b6K+2f7vCaamjp1Kk6ePIk9e/aYt5l6okaOHImXX34ZANC+fXvs27cPCxcuRN++fbF27Vps374dx44dk3yuWbNmYcaMGeaf8/PzUa9ePQwZMgQREdLny7qTTqdDYmIiHh8xCBtzjuLU9XzzawkJCZieLL5m7KxhzfFwr4aYdUje2rJSynbG8Pha2HAqS/L+3bt3M8/P5qtXQkIC73Hfpu8D8i1rrFYNr4LbpUWcffr06YsPju+VXBd7+tzXB0fKLyDRgfn2VcJCcaesBAAwZPBgfJKyF4W6igQ+4RERQHGBzTE9evTEF6crEiQ9PHwQqt+bF7677AxWHr1us7+11vWjseu8/CWLfp8yEK3+t0PWMd0aVcOBS7bL8Yjp0KEDEuJr4e3jO1DiZLKkhIQEXE66iI1XbYex1qgSiFuF3GRJgwcPBpLltVGujh06QHM9D0k3bIMZa+z3LyxQjaJyg50jiD9r1qwpNl27CACo3bobmtQIQ2xksKTv7siq1YD8u7yvdezYEYvPVfSCv3cqGHkl4qMwhL5/pTLVVxsYBKAY+dFtkFdqwOQ+/FN/2O0zndu6zd26dUWPxvKmI7HL0Gi1gKGi3TExNXG1NBe49z6w28tXF2tnE88j8XrFaKrBgwdDq9Xy7ufLTPcqvty+D1N24W55RcJO9u/SH9omxJ/bBvh3+/y5bYBvtM80Ytoerwi6p02bhrVr12LXrl2oW9eS+Ck6OhoajQatWrXi7N+yZUtzcL59+3akp6ejatWqnH0eeugh3HfffUhKSrI5X1BQEIKCbOdParVar/2FmgQHBWLt1N5oNMvy9FVKnSf3cyxTs6vfj9nDW8kKujUajWidhF67kc/NeM23xq5Go+zHITBQIymx37uj2mDO6tPcurCGv2sDtZwh1QECZQaoLb22wYGWa7lGhG3SQT4f/6c9un2wTdK+bI5cI3MeaI0Hvt5jf0cWjUYNrVbr0Hx3a1qtFmqBDPYhgRoA5Tb7u5pGo0GAxOR7ASrLfmM61sWv++0H6sR/qQIsn/0nf6mYfnTh/fslHasWueY0rO8UewE3YPs5WXsiEzXDg9BNZtBrMmfdWQBAQtvaaFxDfKqM0GdUrbb9m1GqM+C1lScxuFVNPNiutnn7kSu5tkk2WZ3kAQEqzkwloXMKbVex3mtP3W8cuZKLm/mlSIiPdel5fOF+Sgj7LwxfG3y5bfb4c9sA/26fP7cN8O72Sa2XR8cxMgyDqVOn4u+//8b27dvRqBH3aXZgYCC6dOlis4zYuXPn0KBBxXyy119/HSdPnsTx48fN/wHA559/jsWLF7ulHe7GF8hVC/XOC1FpRgfnHVvPk80uKBPYUzlNalTBmI517O43vks9m22OrCHNzuodEsg/bHr+w22R9Go/3tdqSgzO/Z2ziZfcgRIyETa+fAxSH8SIXUvOfBLO3yzAi8uPYdz3+2Ufa/0ZLHQiizTfn4zFey9j7YlMTFtuGSGXW1SOhxbss3kY6P3fBvI8tGAfXvj9KM7dtB0tRQghxHU8GnRPmTIFv/32G5YtW4bw8HBkZWUhKysLJSUl5n1mzpyJFStW4IcffsCFCxfwzTffYN26dXjhhRcAALVq1UKbNm04/wFA/fr1bYJ44n3krmXtaK6vr8d3cOxAGTa82Nv874EtYqBSqTC0dS2seYF/eTMTjToA3RtzlzRj92ZLja+iwiwPXoI0/EH32M710DA6TGKJwOaX+kjeVwnfPdbR7j6KJWcWiDZclU8u8eU+aFxD+L2XE0g7G3QrMVqAeA++azYlU9pwN6GRM45YsvcSRn27F3nFOly7W2L/ABHsNokta2ay+hj/FBq+4DKn0Pah6y2ebf7sWq7zS7wRQgiRzqNB94IFC5CXl4d+/fohNjbW/N+KFSvM+4wePRoLFy7E/PnzER8fjx9//BGrVq1C7969RUqufBRYRcntXhrUDNFV5C2T5GhP94Ptart8SazWtSPN/zbdx6pUKrSSsK62dRCkYaUvrwgOLT+P71oP7epVtSmjY/1qmP9QW6ye0ouz3Zlb6rha4U4cLd+gljXt7uPqLOuOFi+2tjoANKsZjl+f6obJfRrzPmxSQfhBgO2+zgVKYzvbjq4gvovve1HqZSxxRoMk76xLwfGrd7FwVzrnCj157S56zNtms+a1EIbh1v/Bb/bg9HXxlTdeWnGcd/v/1qdIPif/duW+bzw9QuWfY9c8WwFCCKnEPD68nO+/iRMncvabNGkSzp8/j5KSEhw/ftxulnKGYThLilUG3r7cE5+XBjWXfYzYw4Wv7PRm28zVs6JS2d/HVax7m6znWbJffrx7A6x8rofN0HWVSoWxXeqhPU9A7oy3H2xlfyeZhC5XKR2w3vqAaccr/ezuU6dqCGYltETtSNuh/I72dDsyHN4Vv1PiOTlWif8A6Q+PnH2Aw6fEKrHfiG/24kZeKab/cZyz3SjyYbau//O/H+HfkeXy7SK7+wDyRst46deNQ15eYbs0HCGEEPegtWn8hA/G3Gan5w7F1pekjVwQe7jQ1E6inU4Nqpn/HVeTvwd32yt9UUuRuc3ybmStg+5BLWIES1KpVNCqA9w2B/vJXu6bpiE21NU0KsJbHzCpPPRtKhY0fTA63vxvdp6AYK20pdKIb1hx+KrNNsm9yi4KK+2N2lh+MANt527B4ct3bOvE2Aa7JeVGm/2sLdl3WUYNpeF+3aj8KggnhBDiPhR0+wklbgQm9mzo0HHLnxGes/xot/o224a1roUlT3Yx/1wlSIPoKpahuWLJx5yJt16/v4X53w93qsu7T91qoYKvySF3GKF1D2+f5jWcroMvEnrfZgxubl4qjn0J/N+wFvwHOGBiz4b4d/p9Dgf1zs+NVaFKkLQM+lKHoY9sb8nMnBAfi+HxsXhzeEuHakd8i17ikBBPPcOa9fcpFJbpOcnMTBgwLh3RIqfNvpBYkRBCiPejoNtPsAOFI28OcqgMrVp+0BAaqEaPJsLLwbw3so3NtoUTOqFfXAzP3hVeFhl2Ljan297NURgroAngGcdsCng0DrwP1uSWIJbYSii+knoOvuNjwuXNpVea0O+KL5isGRGEFwc2s7zGOvTZPo3xQr8moueSOmWge+MotIyNcPgW29mrRqtWYVLvRujdNBrvj7b93DhyLvZ1FagJwLePdcTT9zV2opb8Nr54n+JlEvfYl57j0fMX8WQm5+vp9pqB3l5SDWe5YloBIYQQYRR0+wl2r0B1mcnJTOQmNQPs3/zzBbfOcKb3g10T6yW53hvVxjxcWyuwfjNQkfxN0rlEmt21YRQe6sjtTbfXS8r3qqeT8kjhSB1nDG4uOtqAfQlIycLdRGDagXXdTNfWm8Mdm+/saEbwJ3s1RNeGUejbvAaqBGnw29Pd8Fi3BqLHSJ3Tzd5PLfLLmNLPuUCcegP9kzt6wQ08X+qMm84NAJ9uSbt3Tv4TOlIPb50CQwghxHMo6PYTzt70Ln+mO4a1qeXAeYW9O7K1IuWwifZ02ylEow7AkFY10aNxdTSyWjbr8e6WIIcdPJ19d5j5390bR+HBdrUhhVAvQqA6AH8+18M8VNqkW2Ph0QLO9kjwvS/uCtgdGXL94sBm+OQ/7cw/W7ffVTe0pmtreNtY/PREZ1S/l438iWYGscPMAlQqvDuyNWcag8nYzsIPEd5+sDX+fK4HNCIPe6yxA+jh8bWhDlDZLDtnqpP53yLFj+kg7boWQjGGf3J0tQg5+PILMAxgfwa3Mr7efsF8Tj42myV8pX2eeM6pOsmx+/wtfLvjgs33YnG542ubE0IIUR4F3X7CmR7gyBAtejSpjgbVw7D39QEID5Y2r9Se/zi4LJFYnOZswPX9fztj+bPdRXslhRJOhchIPiU31nyih/SeTfM2Lx8eGBUWiF8ndeV9Tc6vUahHWuh1exbcWwvc+v1jlzuwZU0cmTMYZ+cORsdoaZVVqYAJPRriub7c4e5p7w3D/IfbCRzlmMhQLcZ3rY9Hu9VHt0ZRODN3KG9uBU7QLfJG1Y4M5s2oTio3vvnWUol9LNnf43zTeRgwNt8RHnuwY31eCfX46l4gb+3bHenO18fKhJ8O4uPNadiSctO8bf6ms2j11mbsOX9b8fMRQghxDAXd/kKhG5I6VUMQGigjuFTmtJx5t9VChdc7VurGS6ze1kPPHSpfZhFivZwqFTDpXgbxQS1jONtdURclxIQH4cibg9CzabTiZVuP6hjdQXriuzkPtML98bH85fJcXHKGjPPtOaBFDII0ymcKbxwdhnlj4vHB6HgEBKgQrFXzzodnV1/0QZM6ALte6+9wfYSG8JPKyd7D0d8OZJj/zffwsKjM4DUTFsoNlj53Z75LC0p1CtRG2PXcEvO/v0uqCO7flbhGOSGEENejoNtPKDmn0hPrIGvVAZjVTo/1U3pwEp5ZE6ubvN5T4bsnrUjiLafXvlVZ/b9Ez9zXGGum9MK393ppAaC7yJB0e1zdS65VB4i+x3IuMetSrH8HTWOq4Oicwbjw/v3oKZLUDwAMRuFBq64YSuuKMp+5r5HkZGgqiT3dgPiDH3tCAtWY/3Bbm+09ZF6jUpPeEe8ndLldul2EOatPm38WehaUZ7X0eE5RucsDVyn4PtFShnKLfPUQL+MtD3wIIf6F7nB8VF+rJaWk3ts3qVExlznQC29ua4UCcbX41882cSZ7ORv7hrBfHPe9HNm+DupWC8E4B4fHV5zA8UP5BASo0K5eVU6vaa+m0WheU9kext2v9Uf9qFCb7WM6VCzjZn3dCXEkE74Q6+Cd7xKICguERh1g93PAXkbJZti6kzfFfA8ZXPEA642Elg6tsy13fn3TGHnXVlig7cOyelEhsspQ+u1qHE63z97mrTWnOT9n5pXi+13pKNVxcyfMP2l7PU3+9YgidVDyIXVWXilavbVZsfI8ae+F29iWetP+joQQQmTzvsiLOETqLcTPE7tgZPvaWDOll3mb9b24UOCiUgFt6kRIrpMrhjU7k0iNjR2AvPUAN1t1lSANdr/WHx/x9NyxnXhrCOJq8j8kcNeI7p5NHBu+LfS7qRcVyttb/P7oeHz7aEd882gHSeWLZYB3ltgNcy0785INBuFjne2V5ntLXZH0Teoa3dbk/kpMUxqk4quW7IcOCr5dbWpHYHobaUnwiPKERtPoDLZPtz7YeBbf7eCfB83myuXNpI9i4lpz/LpjB3oZg5HBYz8ewFO/HMadonL7BxBCCJGFgm4/ITVgaFA9DF8+0gEtY4WDZ6FAISo0ENMHCq+hbc1001VFZLi4XDVE1peuW016rxo7QOALEIUDG8t7Ex6sEVzTW+h4R+67nH14ocRQ8pBANYa3jUV4sFbS/vaGKjsTjIodOnt4S9FjOT3dMsp1lDdl9VZ6+T5r/A8dhPd/qrdtUM/3QGXGYOnfOfbKIu7hyDt/8nqe4vUQYn1drjiUwb+jlcSUmyjVWx7k5MsZ7u5tl6PVB5a9dFteieeH8RNCiL+hoNtPKHlzL1QU33YpvW4bXuztVH32vj4A30/ohJlD43jniNapGoINL/aWtT45O/6QE9Sy1zJXqURGBdgpR07444r51+KZhZ0vnz28fOuMvnhtWBzi60Q6XzDEHzBFVwlCM5Fh0XxrAkspVwr+nl7P3Wlbvw9yh5dXDZX2gMWEr3ixwLd5zSp4cSB33Xu+X4+90QvE+yxNvoKcojJZxwRKHIqRlJbtSJVE/d+qU5L31bFGy0xeKn24Oz0EIoSQyo2CbmJDqBeSb7uUHssG1cPw8qCK3qqHOkrPNG1Sp2oIhrSuhSn9m/IG+Q2qh6J1bbkBnaUcqRmqVSoVqoYGYuVzPbBuam+oVCrB2yhPZAz3Jr1YWcubxlTBC/2acpZck/KwJvZesDWQlbEdgPlacmTOvc4onInYFfOvPRl0WwfZapkXZZUgjbzs7Tzly21+GN/KCQ6+hd40yqAyWnaAv/f45DX+Hm2pl+fExYdw8Vaho9VSVPJF4eHuaVkFnJ/peiSEkMqNgu5KYNnT3Xi3zx3RGgDwxbj2nO2NBZb/kXLPcF8zS7DFfrI/bUBTbHzxPnz0ULyEUuRx5GaGHUvI7QHs3DAK8XXFg3x7JcpJZOeOAH581/rmfzvbI/POg60w3aoH07pcKQ9rVk/phQ9Gx2PW/dwh41MHNMXml/rggzH815LY++XSOd1uSqQmlfVwcuuf3x3VBgAwqoHwvOeO9atKPh/f227vPbU+ZnjbWNzfppbkcxLvVc4zdxsAisv5r7fNZ6Qn8LqcU8S73d73Sk5hGfaLBMqOWHHoKu/2B7/Zw/lZiQdwRplfKI7+7ajkz4wJIcQlKOj2UVI7oNrVqyq4VvITPRvi/Pv3o18ctyfxq/EdbJKtAfzBrXWgwZ6/zQ5mAwJUaFU7wqlliZQkZyklZ8vnbq/4/2FtaqFzg2p4rm8Txc8tdE62b1hLjwHAPIEAVq6IYA0m9mrEm11b7j1nzYhgPNqtPkKsej9VKhXiaoXL6oU10btweDkvDwbd1h81657uCd0b4PAb/dG/tjLJCfm8OiRO1v6agAAseLwTNArMP6eeRc9y5fJvDAOsP5lps10sJr16pxid3tuKR77fj8QU2wDf0YeNF2/zPwAo11c8dNiachPf70p3+gHcO2vPoOsHW3G7UN6wfSEpmfn4/cAV84MKzkNRRc5ACCGETbkMV8St5jzQCqcz9+PZe+v1atUqzlwzE3tP/vmSiNWpGoIvH7HNUi2ld7JqqBZfPtIeQRq1UxmsI0O0yCvRyV5ySCr2Lb0z9/dC74m9IoM0aqx8vqekc7ii16Fj/WqY2LMhluy7bPOaq4IVRuDf7tS9cZTga3J7kcT0alodey/k4PEeDRQrs3F0GOrISBZoHWTzXeeRIeLztmWtp87zdKdeVChev78FPvz3rMAx1udjeLcT37P/4h2Xlc0wwNRlx2y2X79bwrv/7weuYPY/p+3u5wpPLz0MAKhbzXYpRjlM39VL9l7Gq0OlPcwS+y7/eHMagIoH5SPb13GqboQQQuyjoNtHNa5RBQffGGi+0f396e545a/j+N/INpz9lOy9Y2A/ADQaocgf8JXP9cCCpHRM4xmmrAT2u+JIj6mQJjXCkH6rCCPa11asTDGuWJLKVd56oBVGfruXd+i5q+17fQBSb+RjQAvLqA7rBHU1wh1P2PXv9Ps4Py+e2BUZd4plr3UtZuuMvrKCUXvDy6WQc32xS28VG4Hn+lWM4pA7l7yiLBVMn1JHeyB955NB5BL6u7b9LH+SNXbA7SmZCgX6YqN1HJFyIx8j29fhBOj0zIsQQpRHQbcPY/csdW0Uhd2vDbDZR2BanUMaVA+1eyOrVJDfrGY4PrOaa64kdjDh6LrHFeVwf14/7T5k3ClGXC2h9bsdCECc7PYTOrpjg2r8Pd1OnU1Yu3pVce69+xGoCcDRjFzz9mGta2HTmSzFzhNiNbQ9IkSD2lVDULuqcC/xs30aOzyXuEqQxmYJvkBNgNMBd1zNcKTdtCRjkhs0W0+bkDuNQqsOkNnTbfn3RtZDCKHT8n0WzNtYL/nQcyXiJq64JFx9nSlVvsGo4B91gJ5OEUKIm3jHBFviMkoMmV09pRcS4mvhu0c72d3XV/5+K9XTbd0LFxKoFgy4vc2DbWPx+bh22P5KX7ed05RAjn0D2rlhNUXP8enYdmgcHYZhrWuhQ/2qmP9QO9792MHgGwktHV7HOljrmq/RjdPvwysOrlEN2PYwy53t0a2R8FB8PkoOCVekKAWjqHcebKVYWcR5cn+1Ll6iXhK5D6TTbxXim+3nUVSm52znm0bmDF/5m+1O9KCPEOIK1NPt55ToeW5fryq+e6wi4D53s0B0XyXnxboS+23xhhsyMaJrajtTrkqF0R3kL+GmNKVvcJrGhGP7q/2ULZTltWFxmL8pzfyzq27Q1AEqpxIPWgfBcnu65T6EcGgUh8Ax3jSnO0gTUJFscl2Kp6tCzOR96NQBKhgVDlblMsj8ohj46U4AwM38MvNKAwBgkPE3VsrnyJxIzTf+dBNCiM+inm4/FV0lCADQt3kNt5wvrmZF7+7IDu5PyOLYnE/LMc5kL/fHGxVn2uTIUHhnlyhzN22A+742H2gbCwCIryN3HXrbERyOXOeyrgWhYeQOz+n2DoGaAJescEAcJ/fZrqd+f+xpNH8KLC0mpwxAfE63Izk+TMWxv4cdebsKSnX4dscFXMkpln8wIYRUAtTT7afWT+uNHWnZGO3iINj0t3nN1F64lluMpjG+MbSa29PtvhsyR07ln/f7vhVos1n/Plz5+6kXFYoTbw1BlWD5X9XWQbcj0ygUiLlFw2fB+d7sOd0y6sCm1BUWoFL56WfQd8kdwSXl2i/RCa9X76gx3+0z//syTzB6/mYBliZfQUJ8LI5dzcUTPRoiLIj7Wbe+9oTmdF+6XYQu72/D8/3kLUNpeiudfYD83vpUrDh8FV9tO4/5XZwrixBC/BH1dPupWpHBGN+1Pu96yc4Q+rscrFX7TMANcNshNRbh283ToaNrlpb2dKu8U82IitEj/Vtw17VvXEN+wrRZCS0BAE/2amh338hQrUMBs/Uol/b1qsouQ1b2cjtr0/O+JnO7J2gCHA+6H+tWH4FOTBEg/OR+70nJoH/mep6DtXHc8K/34Nf9VzD+h/2YvykN8zfZLq1nPepDLzBM/tf9V3C7sAzvrpc3DUKp7/sDl3IAAGV6hRO9EUKIn6CeblIpsW/apAY0fLcm7liyy9ns5d6I/bYp/WBIKrnv686Z/XGnqNwmC/qXj7SXfe6hrWvh+FuD7a6T7YyJPRsCAHbN7I9rucVo50DQfbdYJ3lf2QG0WDDO+t04+h4p9dEMCFA5PBpmQIsYzB3RGk1n/6tMZQgA+Q87peQn8MT3bLlVgHr4Sq7NPgEqYPf5W+aflV4yzNzT7Ww5TteEKCm/VIdgjdqcvJQQ4nn0aSSy1AgP4vw8ZUBTD9XEOUotGSaX0mdySeZsO3dPrWtHiO8g09jO9dC9cRRev7+FouUqLVirtgm4x3aui9hI4aXIxFQNDXTptWdKwla/eih6No12qAw5HexKNsW0nnr9qFAMbe3YUm5KsdfT/fIg4QzzKhWcSoZH+Ml92Cnlwao3PNvka9aJa3mY8NNB889yEqkBwPW7JRj//X5sTblp59zOhc3+mN/EV+UV69D2nS3o/dF2T1eFEMJCPd1Elvb1qmLW/S1Qp1oImsWEo3lN59YidoYmQAW9kUGPxvIDCkfuD7xxePkL/Zoi+WKOeBZyhe8mV0/phWZO9twFaSy928FaNf54toez1ZLNC+6xvd4Xj3TAjD+P4+KtIrv7CmciV7H+Le3m/P3RbdChflUkxMdCHaBC4+gwXLxtvw5sSg2bDVCpEBUWKPh6tTDhnnh/HKXiDX5NviJrfykjFbzhNyXlipV7Xc/6+xSSL+Yg+WIOf3mm7OWySnW+XsR1jmTcAQBkF5R5uCaEEDYKuolsk/vKS9TiKjte7Yc9F25jTEf5yeIUeyrv4fuMamGBWD/tPkXLtDccTasOQFigGkXltomHpMYYbepEYHSHOqhdNdiRKhI3aV+vKra/0g8NX99gd9+ujaLQpEYYmljNcWdfE0sndeX02kVbjZwx7RserMWTvRqZt7vjY9a6dgTOZObbbFcHqBCkUeO7xzrihd+P2rwu9l3iDYGcP+Ibhi3G1wYbnBaZXy73b9ctO4GXYn8K/SjmpgcIhBBX8LE/RYRY1IsKxfiu9Tm9plIp9Uf1uXuZYhPiPTsEVkkzBjdHs5gqePvBVi47h0qlwufj2mPmUM8NKadOSGUFagKQ+HJfLJrQSXCf+5pxk7s93Ik7QsOTN+6Pd2/Au900NNmRqRzU0+0dNG5c5k8J05YfE3xN7mfE3rBxI8863d60ZJ83yC4oRQnPQ2ZCCJGDerpJpdSxfjVUDwtEvahQycdoebpLxnauhy4No1BfYjm+cBMeExGMxBl9RffxhXa4A92ccvElrBJ7h7TqADzevT5+258hWq7SCQu1ahV0AlmgrZmCbkd+1w4knScu4Csxt+k6LxNZvkzOcmlGhrF7nZuniEsstlxv5B0N5U893Ww380vR7YNtCAtU4/icgZ6ujiT0d4kQ7+Qjf4oIUVawVo39bwzE38/3tLvvuyNbo3F0GN58oCXv642iwxxa0sldvLdmnqXE++Jtzx68rT4A7FbKVTeIYkGAdUI8AIJLe/EtN7V+Wm9JdaCbX+8gZckwd394xB4kiWVblxPbnrtZiEt2ciHICZbPZOah+Zv/4oONqTJq4T2OZuTi4KU7so4x7c83nYoQQuSgoJtUWlp1gKSlZCb0aIjtr/ZD3WrSe8Urm6F1K5a+eXdkGw/XhLjb0kldRV+3t5SYJ55X8SXWCtaq8eN/O9sMj+d7oNamTqSk83jlQ5BKSMr3vLvxBbsXbxdh7YlMXMstkXWckzW597/2C/5kcxoA4PtdF5WuhMuV640Y890+jF2UjIJS6UshEkKIUijoJoQ4LaGeEUdn98eD7Wp7uiqSje5YF9XDAh1KxEcq9GxSHX2a1xDdx17gyZ6qoOQaxGIlxUZyE/jVrRaCAS1iMKhVTZslypwZxWLvyMY1whwum0gnJfu+u/Fdn+V6I14Umc8tfKQT9WC4/w8AX2w7x7uv2LQipaeAKK24XG/+N83PJoR4AgXdhLhBFU3FDUm3RlGC+7SoFQ6gIpBRkrt628KDhZdO8kaRIVocnD0In41t73AZ1JNpn5wh1kI9UC1juWvDd6hfFUDFOumO+Pg/7cz/fr5fE+ya2R8hgfwJGe31kooFG/ZyH0hZyor4J0cfMCkd25qDbta2v49e5/0s+vLVWqY3mv/Nl5/Fncr0BlzJceGDIF/+RRHixyiRGiFu8HK8AbmRzTHpPuHl1pZO6opVR69jXJd6dpftIsrw5rn4viAyRNkHLfmlet7t74+OR2GZHrvP3wYAfPdYR5y4ehf94mLw5+FrvMeIBSd1WHO6VRAPrJ25ROzF1N52+SW+3AeDP9/l6Wp4hpt7at9am+LQcXISqUkhNKzcaOTdLFKOdytnBd2eftg15rt9OJOZj2XPdEPPJtEerQshxH3ozp4QN4gOBqYPbIqosEDBfWIigvF8vyaICgtElSANjs0ZjDNzhzp97ggf64EmvqF+VCjefrC13f3sDy+3/Htc53q8+0SFBXLOVSVIg2FtYhGslb9coJT6vTqkufnfphv0IIElw/iSspnLtnNuT9/8W2tWMxxNKumQ96+2X3Dr+f4+lunQcUoHt5bh5fZL3nY2W/C1Yi8fss3u6fb0OtxnMvMBAKuOXPdoPQgh7kU93YR4qWoiAbocj3arj/0XczCgRYwi5RH/Nax1LWw6k4X7mtnvfflzcg/UspobzcdeWPlwp7pYvPcyQgPVGN42VlI9pS1ZJ+3Gmm/4e7t6Vc3/NvVGd29UHcNa10KzmlUAAL9M6orT1/MwuFVNwbLtDU2npfe8x62CMk9XQRKGqZifHKLAAyfAsmSYM2Ho7cIy5JV4R3KyV/86gfM3C7Dy+Z6cYeTsnm5HBwv4yjVC3yqEeCeP9nTPmzcPXbp0QXh4OGJiYjBq1CikpaXZ7JeamooRI0YgMjIS4eHh6N69OzIyKtZ1vXPnDqZNm4a4uDiEhoaifv36ePHFF5GXl+fu5hDilYK1anz/3854pGt9T1fF7yjR06qkYI1z9flkbDt8Pq4dvnm0o919nY0XTQFn69qRODh7IE6+PUT6sc6d2i72TbmpngEBKiyc0AmvDIkDAPRtXgNT+jcVDZzt93Q7W1NS2ew8dwut3tqM/1t1Eov3Xna6PFOvr3UgKqc3eGvKTafroZSVR67hxLU8HLjIXRqs3CBzvPw97Heh5/ydTtSMEFLZeTTo3rlzJ6ZMmYL9+/cjMTERer0eQ4YMQVGRJcFEeno6evfujRYtWiApKQknTpzAnDlzEBxc0cOSmZmJzMxMfPLJJzh16hSWLFmCTZs24amnnvJUswjxe3WrCQ+prQz+N7I12taNxLQBzTxdFY6lT3VFnaoh+N5q2SupqgRpMLpDXUlztaXGi1KC85jwYGjsJjey3P46GvA3rB6KXk25iQr5Al/GzutS2Z/T7d1Rd9eGwokfiWf9efga/rfesXnhHAKxtZzeYG+8jq0fGugN7OHl3sEdw9y9Pas8IZWJR4eXb9q0ifPz4sWLERMTgyNHjqBPnz4AgNmzZyMhIQHz588379e4cWPzv9u0aYNVq1aZf27SpAnef/99PP7449Dr9dBoaAQ9IUpb+HgnvLchBc/3E04M58/+26Mh/tujoaerYaNLwyjsfX2Ae04m8T5bKHt5o2jH172XkhGd715z2yv9bINonoCBfaPqXEAhfiwlTCSeZkrM5kwAaP0RuVQAGBVc/s8R1t8RcrLFc0a6KFUhD2EYWmWDEG/hVRGpaUh4VFTF03Wj0YgNGzbgtddew9ChQ3Hs2DE0atQIs2bNwqhRo0TLiYiIEAy4y8rKUFZmmZuTn1+R1EKn00Gn8455SdZM9fLW+jnLn9vnj22rExmIBY+2B+Cf7TPx57YBzrVPr9NLOs5gtCRY0ul0+POZrriUU4S2tcNlnVens2Q21+t10KnEh4vyBRFGgx7WRxkNBpt66PWsLOoM4/Dv32gQfo9a1KwClcRAZ9OLvTBl+XGku3i9aZ1OhyY1wsznMTJGzBvdGrP+OePS8xLPMRiN9+59uCsH6PQ66HTC0Rr7ujYauUnUvjitQdUGGXisewNlKyuD3uqzV1Zu+Xe5TrxtbAaDbYI4pf4eMPfee6Wx61xWXi5hFBH9rfNl/tw2wDfaJ7VuXhN0MwyDGTNmoHfv3mjTpg0AIDs7G4WFhfjwww/x3nvv4aOPPsKmTZswZswY7NixA3379rUpJycnB++++y4mT54seK558+Zh7ty5Ntu3bNmC0FDHe1/cITEx0dNVcCl/bp8/tw3w7/b5c9sAKe2z/VOxbds2REjI9XcqWwWgYq75xo0bAQDBADbeOCGrjlnFlnps3rwZlmTi/H/GioqKbbaZzs8+7sKFC9hYdo6zX0qupc45ObetjuPDX4fkffuQGW77+rud9AjV3MV3KWpI6UtLO7QTHauokH7LtTkENm7ciL6hwJZ79c25k4uTJ3Ngei+I/7l+PRMbN15DXjnAvk63Jm5FGGuWyX7W5xjgfpZO3eK+BgBLdp5FtTvKPqzRG4ETd1RoFsGIfPdUtOHgwYPIS7M81Dp711LHrVu3IlxkBk1pqeVzeezYMVi3zfm/BxV1vHb9OjZuvOpkWbZSWd9f//67CXKWJae/db7Ln9sGeHf7iott7zf4eE3QPXXqVJw8eRJ79uwxbzPeWyhy5MiRePnllwEA7du3x759+7Bw4UKboDs/Px/Dhw9Hq1at8Pbbbwuea9asWZgxYwbnuHr16mHIkCGIiIhQslmK0el0SExMxODBg6HV+t8SUP7cPn9uG+Df7fPntgHS2zc9eYvNtsGDBqJ6lSC75yg9dh3L0ituvhMSEhyu64XsQsw7sQ8AMGzYMATdG5p9MSQdX25Pt9m/4gFqAWcb+/ymNjVv1gwJA7jTJELP3cKis8cAADE1aiAhQXyOPN/7AwC9evVC27qRNq8/MqqiHituHkJ6Qa5o2aZ65x68ir8updrd1xEx4UH49OF4dG9cMcps9uGK+larVg09OtfH8vSTLjkv8bzY2rWRkNC2YnTDkb3m7QMHDTIvcZmVX4rpH3PXb2d/lnQnbuC3C6c4r1erGomEhO6K1vWzreex9Pwl1I4Mxs5X+/DuY/qsdevaFT2bWPI3hJ67BaRWfKYHDRT/7nr/9E7k6ypGQ3bo0AG/nOde/87+PTDVsW6dOkhIiHe4HCFVzt/GwrNHAQBDhw2TNI2F/tb5Ln9uG+Ab7TONmLbHK4LuadOmYe3atdi1axfq1q1r3h4dHQ2NRoNWrVpx9m/ZsiUnOAeAgoICDBs2DFWqVME///wj+osJCgpCUJDtF65Wq/XaX6iJL9TRGf7cPn9uG+Df7fPntgGOtU/qMWq15c+MM+8he7pQUKDWvBxQn7iavEE3Xw8y3/nVarXNdrVazfp3gMP1DuR5jyKCNeZt9pYUM9FqtZw6Ke33p7uhWc1wm+0MgIS2dfDiCgq6/ZVKpUJBOYNhX+3lbNdqtcgq0OHY1buI47k22Ne1lmfVhIAAxz83QrafvQ0AyMwrtVu2RqPh7qNifaY10r/v+D53Sv09ULngPQK435VqjQZaGats0N863+XPbQO8u31S6+XRoJthGEybNg3//PMPkpKS0KhRI87rgYGB6NKli80yYufOnUODBpa5Qvn5+Rg6dCiCgoKwdu1ac2ZzQgghriF1jWmlcviwZz9LKbNZTBgA+0+f+ZrRMtYy4smZRGp8h/7xbA/L616SpkmoiQwDSfNBie9iGGD/xRye7Qzum78DADC5b2Ob19n4PiOuSN7lTLI3g5GbzaGwTI+ScgNqhNsfreMylFickErFo39Np0yZgt9++w3Lli1DeHg4srKykJWVhZKSEvM+M2fOxIoVK/DDDz/gwoUL+Oabb7Bu3Tq88MILACp6uE3LjP3000/Iz883l8OXAIMQQojzlFwyTPa5OYXy37m+N7IV73absni2xUZalsRTci3tB9vVRqvaloDee7IK81eEYgL/x4Dh/e0fumxZ5/rgpTs8e1jwB90qlOoM+PPQVdzML5VUly1nsvDpljTBZa7Ym/UGI1YduYard/jnUi7cmY49529b9mdlL2fAoN3cLejy/lbkFJbxHe43aMUwQryHR3u6FyxYAADo168fZ/vixYsxceJEAMDo0aOxcOFCzJs3Dy+++CLi4uKwatUq9O7dGwBw5MgRHDhwAADQtGlTTjmXLl1Cw4YNXdoGQgipjDwZMNo7dbVQrc2czfBg/j93zWvZDp2VdzaRI60OtQ7g5byH7etWdbgeDqM7dr9XsaSU7YX4f6tO8ezNdfp6Ht5acxptea5NFYBPt6Thh92XUCsiGPvfGGh+bcfZbDSKDkPD6DDOMc/+egQAEF8nEkNa1xI997KDGXhrTUWuiMsfDrd5fff529h9/rb5NQN7yTDG8vOp63noFxdjt62+hPtIkj7DhHgLjw8vl2LSpEmYNGkS72v9+vWTXA4hhBD3Uio4jwi2zJlil8n++v/ykfZ4d30KFk0QT3wGAOum9sap63kY0qqm6H7O9HRb/2myXitYzvDy+LqRss8/vG0sNpy8YXc/od+Rh5daJm7AMPzXOPu+SugWa8JPB5BbrMPRjLs2r6lUwLbUbAAVidhM9l/MwZNLDgHgD5YBCPaMs6uRnG4ZEj9uUTKWP9OdN0dCSmY+mtesAr1BoPecd6t7uOPc9BkmxHs4FHTr9XokJSUhPT0djz76KMLDw5GZmYmIiAhUqVJF6ToSQgjxMlIDRqXmLdeKDMYHo+MRFqQWnE8+ol1tjGhXGyqVyrxu5icPx+O9jWfx/YTOnH3j60ZKCmSdmdNtsLrj1Ru480qDZSQ4ckTVEOeSzlAvmf9jwPBe45wcCgIfgdxi4bVpVeb/4Tpx9a6c6nHrJPAg4MClOzh8JRddG0XZHJPw1W481LEuujaqZjlWzjkdqagXoU4pQryH7DndV65cQXx8PEaOHIkpU6bg1q1bAID58+fj1VdfVbyChBBCfJeSw9Af7VYfI9vXESxfpVLZBOQj28Xi2JzBvDfkUgQ4kfnEwFgH3dyf336wFRpHhyFawtJrjpg+qJmk/YTuy+l+3f8ZGf7PqL3f/eHL4vO891/iXwpPqUvK+oHQ2EXJgvuuOnrNZpSJdYWWH8xA34934PLtIoVqaJFTWIYjV8TfLyVxRgK57ayEEHtk305Mnz4dnTt3Rm5uLkJCLMlmRo8ejW3btilaOUIIIV7KS5KASQkMpWZaV/pY614mndWNf72oUGx/tR8e7Vbf4XOIiQkPxoAWjs9XpaGp/i8x5Sae+uWwzXZ7PaQPLxQOcsVI+bxKuez4yhGrM3vUCXs3U/A+6+9TuJJTjDdXn+Ycp8TXXI8Pt+OhBcnYe+E2Z7s7eqHfW5+ClExpawgTQlxLdtC9Z88evPnmmwgMDORsb9CgAa5fv65YxQghhPg+Z4JWb+DM8HLroLV+VAjvfkq9Q0Ea2z/pYzvXdbg8GppaeSnxm1f6k88I/Nu8TaTSgnO6rTbvuXAb2QXKZjQv11dMK9lxNlvRcqX48/A1JHy12+3nJYTYkh10G41G3qW4rl27hvBwe1lgCSGEVCauDrldHRaqnWiAqXftz8k9ML5rPcwc0kKhWvGLiQjCpF6NONuGtq6FKkGO5UylmLvyMkpIpGYP3wM3oTwBOqt8B5duF2H4V7vx7ylWIkA79RB7+fcDVwTrkFciPDfd0Y/A1pSbmPDTAU5SOBo5QkjlJjvoHjx4ML744gvzzyqVCoWFhXj77beRkJCgZN0IIYT4uMGtaqJxdBjGdKhjf2cvMnNoHGqEB+GVIXEOl2G8d5fdtVEU5o1pi8hQ5xKbsXWsbrS/Eyr+RsfZXRZNKLOzY1HCyud6yD6mRrhr5rUTx7jjgcsnm9MAVMx5bvvOFs65+3+ShDOZ+Xj+96OS6yc2MiP9VhFrP245s/4+KbPm/H7cfRGbTmehTG/A00sPY/f523hrjWW4uvXniWJwQioX2Y+/P//8c/Tv3x+tWrVCaWkpHn30UZw/fx7R0dFYvny5K+pICCHEy0gddR2sVWPbK319bpj5lP5N8UK/Jk7VW2rPltxTTOxRH+XZl3A0x/6+gOPDxE2HfflIe0z/47jk4zo3lJ+0LixQjVuyjyKuUqaX9lBHzIXsQvO/P0s8h5jwIE7A+82OC3hxYDP8ffQ6SnSWEZRC16u9kNWRq5wBsOOs8JUn9aN55Eou3tuQCgAIDbSsSpBTWG45lxdG2WV6A8r0RoS4diEFQggc6OmuXbs2jh8/jldffRWTJ09Ghw4d8OGHH+LYsWOIiXE8YQshhBD/5GsBt4mz9a4qsWc7vo78NbiFasabiVp26dzjrDPGu4Izc+eJax13Ypkvk6+2nbdJUgZUBH3WHL1ec4vK7e9kVT7DMIqssJCVZxlGXlxu2yaAO2RfaXeLy7Hm+HWUlBtkBffdP9iGtu9sQb7IEHsTo5HBlZwiyvVAiIMcmugVEhKCSZMmYdKkSUrXhxBCiA+gEMli3ph4rD2eieSL3K7nNhKD6QEtYvDFuPZoVTsCQz7fJekYoUBBHWD7Qts6kTiWcVdSuWyuDBJs0AVVKZXq7Peobzx1A4Nb1RRcp9uk6wfKr6CTmHLTZtvBy3ewJeUWZg5rYc6XIDQVg/05tf48KfnxemLxIZy4ehfju9bDsDaxko8zrbV+8rr9DOdz153BL8lXMDuhJZ7p09jhuhJSWckOupcuXSr6+n//+1+HK0MIIYTI4Q2dLuO71sf4rvXR8PUN5m09m1SXfLxKpcIoBea8q6Di7TGeOawFfkm+wnOEHU68t8/c1wg/7L7keAGkUuDr6bb2wu9H8eqQ5nazl0vFCd7t7Lv2RKbNtsd+qlhirWZkMF7o1/RemVLOy799+9mb+OPgVYzuUAf3x0sPmNlO3BuNsOZ4Joa2ruVQGSZX7xQjr0Rn89DQ9B0yf/NZCroJcYDsoHv69Omcn3U6HYqLixEYGIjQ0FAKugkhpBLQqmXPTqpUJvZs6LKyVSqVYMewhqenu0qQBo2iw3DpdhHPEcKcCWxev78lakYEm+e5mvRpXgO7ztHs7crKOks5X083X3C6xarH2ZkhztaJ1OQMsvg2xfK9l1dsGZItVBv2J5Uvx8PVO8WYtKQiiN+SchOXPxwuoza2GMb5BG33zd8BANj7+gDUqWq7zKE3POgkxBfJvmvKzc3l/FdYWIi0tDT07t2bEqkRQoif+3xcO3w1vgOCtZR5R0iVIA2GONnbZA9PbF2xXeAFR4IUZwIbdYAKUWGBNtsfbMvfk+eu0eUPdXR83XLivC+2nuf8XKqz7enekWa7nrV1fgXl4j55JZ3Ls9w2B2ks/5byWbHehwGQebdE1vklUejNYSfCI4Q4T5GuimbNmuHDDz+06QUnhBDiX0Z3qIsR7Wp7uhpmrWtHAACiq9gGeO7WpWE1ABXLjbmSWICqFpjsXb2K/CW5nF1XOEhj+2CGHTx1b2zJcs43F92eMR3lDclfOqkr3h/dxvzzqPbecx1XVnx5A3afv22zTa2y7aFW4pximcvt4SZkE9hJZE63qzi61J8v0xucz7ZPiKspNj5QrVYjM9N27gshhBDiKmFBGpyZOxT7Xh/o6apg6aRu+OeFnpjQvYHLzyWUSE2j5n/hs7Ht0LVRFBZP7GLzmnUs0KlBxcODsZ2d6xUe0rqm6OvsKQqBGgduR2TEFp/+px36NK/BGaHRIjaCp06U0c2dpMahKpVKsWCS/TBpxeGrDq9SwHkIIFQ31mapD7HyS3X4bf8V5BSWiZybwb+nbthMGWHAuHz4t7eF9FdyitDq7c2Yu+6Mp6tCiCjZc7rXrl3L+ZlhGNy4cQPffPMNevXqpVjFCCGEECnCghxaiENxIYFqdKhfzS3nqhHMf+sr1GPcoHoY/pzcQ1LZv0zqiuMZdzk90Y7QqgMw/+G2eG3lSfM2du345p+7ykOdbB8gdG9sm+yuYfUwnKdhtV7nyJVczoMmpRKpOYMdaAsVaRTJuC5Uj/9beRL/ns7CH4cysH7afbz7bE3NxvO/HwUAzjxwhhF/kJFXokNkiLSlDIV425Jh3+64gHK9EYv3XsbbD7b2dHUIEST7TmXUqFGcn1UqFWrUqIEBAwbg008/VapehBBCCBFQNwz49OF4vLLyFGe70PByOaoEadC7WbTT5QB2hsIHeCYZ38HZA3E9twTt61W1ec27wgn/J+f9Zsd6ziTjc3bahAkjoRfbek1wa3xrev97OgsAcFpkGa+jGbnC9RLY/sOui3h/YyreH90Gj3WzPxpH6LNLnxFCHCP7L57RaOT8ZzAYkJWVhWXLliE21rGlDgghhBAijSmuHtEuFs1iqnC2CyVSc5d/p3N75sSG7rqzp5stJjzYbSMSiDiGYTwwB5l7PkevQnsBtfV26zndl3OK8OSSQzbHSHludjOvVLBOQnV5f2PFSgKz/zlt/wQucvLaXfT7eAc2n8nyWB0I8RRa84UQQgjxUda3144EskqFPF+P74CWVvOkrWvD7tx2JHkamxL1/nBMPLdMLxs6S5Tnip5uoSLZ263PK9STLeUS/PvYdeF62T/cKc58RJ7+5TAu5xRj8q9HvKI+hLiTpOHlM2bMkFzgZ5995nBlCCGEeK9AWpvb5bo2isLBS3ccPt7ZQFZp1r127Izm7KRvStw4L53UFReyC/G/9SmSj6kayp3fSvfv7sWAu5a1OyiVRTz9ViEYhqkYzSE0vJwzBN01V9fLK46zTqhcEKrATBUbfMPpCaksJAXdx44dk1SYoxkgCSGEeK8/nu2OuetS8N6oNvZ3Jk5Z9HgnrD91A/P/PYuCMr3s47s0lJ/8zJV/udm3BS/0a8IJCDQKz+nu07wGalcNlnWMEgHK070b4cc9l5wvqBLafe42Pt96zq3nNFqtLuXI5wwAElNu4vOt5zFjcHPBIfKSlhWzolLJuy7/sen19t5HRzSShFRmkoLuHTt2uLoehBBCvFT3xtVt5uoS16gWFogJ3RtgYVK6pGCAfROrAhBXKxzrpvZGTIT0dbmduQ0+M3coWr+9WfB1di/m8LaxuHy72PyzKwZO8D38F1tX3maosQNvRpCWRoA4yt0BN6Bsj/NX287jhX5NhBOpsbOXKxQMiwWu5QajYsPnfYXSzd2WehOBmgDc16yGwiWTyo7+UhBCCCE+iu+GM75uJGpGyOvxdZS95drYMbD1MGJns5fzBR982dvvE8nEbi8AC5Kwfjh7yDypfDLvlgj2THOGlxv595Fr+cGroq878kzBXwaqimV1l+JucTme+uUwJvx0EDqDQr8wQu5xaHHTQ4cO4a+//kJGRgbKy8s5r/3999+KVIwQQggh0jk6xatGFem94nyGt43F8Yy7GNgyRnQ/6xi7e+MoLD+YAQDQKNTtHcDzHvBtM7GOT/rFxeDibctQ8YEtY7DxlHimZSmBOQC0io1Ayg3hZaCIe7hibrVQL7ZRJHu5EBWEe28NRgY/7r7oUF1Ej3FT77irT2Od1f1/61Jw6XYhFj7aXtLxBaWW0UU6gxFaymNCFCQ76P7jjz/w3//+F0OGDEFiYiKGDBmC8+fPIysrC6NHj3ZFHQkhhBCikO8e64iLtwoxqFVNlOqMqBYW6FR53z7aEUYjw7tcGftBgAoqTkBgGvbdrm5VlOmNeHLxQbw0qDleW3XS4brwxddiHers3vLZCS0xoUcD/LzXEnTrDfbDBKlB98bp96Hh6xsk7UtcxxXDr6X0dDvri63n8MXW8w7XxV+Jtdf0WT6ScZf39dQb+XhrzWm8OiQO3RpX57xmuk5M3xGUt4o4S/YjnA8++ACff/451q9fj8DAQHz55ZdITU3F2LFjUb9+fVfUkRBCCCH3cG7+HLjBToiPxdQBzdCiVgTa16uqSJ2E1gdnb7W+Z1WpVBjZvg4aRochrlY49s0aiLFd6jlVD77s7WI93ezex2f6NEawljtU3CAhQrM+xlFt6kTY34k47dLtQsXLFFynW+DfUl29Y8mBICXgdvQ8fJzNKr947yUsTb6sTGWcJPQ5fnLxIRy6nItx3+8HwP0eMzIMGIbBuO/3Y9R3+2CsbJPlieJkB93p6ekYPnw4ACAoKAhFRUVQqVR4+eWX8f333yteQUIIIYTw8/bbQO6cbmXxtV328HI7b6BOwo22dQ8ZH1MiwoWPtUfTCP65ovMfaidahlZNPW1KeHnFCUXLYyCyTjfrAjt46Q56ztsmq+wJPx2QXx8v6Oq+W1yOuetS8NaaMyhiJYRUaskwhmGQZTWUXIzQV0B2AbcMdk4IxggUlRtw8NIdnLh6FzfypZ+PED6yg+6oqCgUFBQAAOrUqYPTp08DAO7evYvi4mKxQwkhhBDiJF8Kvdi9Ze4YncnX4S4WdNvrydZLSKYkJRhuGVvRiz2wRQymteYvMyRQvMc88eW+ds9DPGPZgQze7ez4N69Eh0w7geKOtGzOz5dz5N9XOzNnXamAvVRnucalTNGQ65Mtaeg+bxt+2nOJM2VF7neMdc3Y3x+mnm5z2Q7UkxA2yUH38ePHAQD33XcfEhMTAQBjx47F9OnT8cwzz2D8+PEYOHCgSypJCCGEEFve0KslhtPT7Yaom2+Yu8DIdwD2e7qHtq5l/5wKtUusngDQMDpMkfMQ5Z3NKuDdLjep2ZOLDzk955wvSzrfwyNv/+4Q8+2OdADAu+tTONudaVJJuQFLk6+Yf7Z+eEFTuomzJAfdHTt2RKdOndCyZUuMHz8eADBr1iy8+uqruHnzJsaMGYOffvrJZRUlhBBCCBf7ttAb7wlVAv+Wo53AvPMZg5vbbOMLgMWCfb6gqCZrjXOhc3PLF3/9EYnz1J2dQ0s84+EF+wRf88Q0YL6e7t/2X7HZxt7NdA3zBa23Csow8tu9suqg1Jrk7jR/81l8s+OC+WcD44utIN5MctC9d+9edOzYEZ988gmaNGmCxx9/HDt37sRrr72GtWvX4rPPPkO1atVcWVdCCCGE+BDrnm65PVG7X+uPVc/14H2tQXXbnl++3mK5c6FrVw0x/1tKb6C9YDmuVrjdMupWC+EN3h+8l+HdpDH1dnud3GKd4GtK9CZfvCUv8dvMlbbZ/w9eviN6jKmafLX9LDENJ67elVUHDoGPh7NravOeSuZzK/buey/c5rxmPWKAHooRZ0kOunv06IEffvgBWVlZWLBgAa5du4ZBgwahSZMmeP/993Ht2jVX1pMQQgipNMRuHjnJy32oK8aRW9Z6UaGy1vDmG14udvwDbWujQfVQPNqNf/UVKT2VfD2LMeHS1z6fOTQOq57vafM7rx0ZjAEtanC2LXi8k+Ryiecp8fG8mV/mdBl8ASNf3fgeEhSWKZP8zNqY7/bhSk4R72sl5QZcv1vikvOycbLLWzW9Yk63y6tAKhHZidRCQkLwxBNPICkpCefOncP48eOxaNEiNGrUCAkJCa6oIyGEEEJ4vDo0ztNVsEN+IrWmNSp6c+UEriZ8w8u1IpOlw4I0SHq1Hz4YHS+wh/27br5kbKM71LF7nMmU/k1RMyLYZhg835mjqzi3pjpxMwWCtvE/7MeRK+I91fYUsjKIm2w5k2X+t3l4Oet10zaxjy3DMHhy8UEkfLm7IqmZ0NJpAtvP3eTvxe/78Q70+nA7zt/knysvTPxL5mjGXaQJzr/nsv5c05xu4izZQTdbkyZN8Prrr2P27NmIiIjA5s2blaoXIYQQUmnNvBdMj+/K3wNrMsJq+LG3Yd+oSk049sOEjuhV04jfn+os+3xqnnPY6ykXm/MtpaebL+hmlyn1Xt16v8Y1bIeSuyMZHVGOUh2lT/1y2Knjd567ZbPt+d+Pmv/NMMCPZwPw0p+2Q9PFLNl3GTvSbiHlRj7eXZ+CranZNvtMWnIInd7bisu3+Xu1+WQXVPTu85XHFqCC1ZvM/46rVEBeOTDuh4MY+sUu3n2sHwwYGcb712QkPkXj6IE7d+7Ezz//jFWrVkGtVmPs2LF46qmnlKwbIYQQUimNbF8H3RtXd6i3113WTu1ldx9HQsS61UIwtrERDXnmbAeqA1AusowXX0yqkTmnu2Z4MO+/hQRpneq/MGPXvXnNKvj0P+2RfJE7z9RehnPiXZTKEG5vaTtnZReU4lRuAE7lige51qwTtFXMP6/J2XbkSsXc7fc2pNoc7+z7I+chVK6dUfr2erpdoUxvwG/7M9Avrgaa1Kji8vMRz5IVdF+9ehVLlizBkiVLcOnSJfTs2RNff/01xo4di7AwSu5BCCGEKKVmBH/A17F+VZRd5HnBjQHZfc2i0bZuVbv7cXp8FajfnAda4kJ2IR7qVJf3df7h5fKC4v+NbA29kcHj3eujfvVQhAaqUVwuPK81NjIELw5shq+2nQcAtK4dwXldamDArvtnY9ujVqTt75+SOfkWR9bZ5uPq37qj15V1XGq4F0TzxdJ8uQ8cCWvzSy2J63jWKuA9RlL7bOZ0y6qWQxYkpeOLrefx7nrg8ofDXX9C4lGS/xINHjwYjRo1wnfffYeHH34Yqamp2LNnD5588kmHA+558+ahS5cuCA8PR0xMDEaNGoW0tDSb/VJTUzFixAhERkYiPDwc3bt3R0ZGhvn1srIyTJs2DdHR0QgLC8OIESMosRshhBC/smtmfyya0AkD4mrY39nFXhzYTNJ+nCXDVCqnR2tGhgZi7sg2ggG/mjeRmryAIiYiGD8+0Rn94mIAAEfnDMbSSV1Rh5XV3Bp7+bLn+zXhvCa1N4/7XlX8f52qodx9HOhUF0oSR3yHq6cVfLb1vEPHWV/bBoPwta5XKIp96Y/j5n+rVNIDd7nvoM063TKPl8I0CoBUDpK/vkNCQrBq1Spcu3YNH330EeLinE/esnPnTkyZMgX79+9HYmIi9Ho9hgwZgqIiy7yP9PR09O7dGy1atEBSUhJOnDiBOXPmIDjY8gT4pZdewj///IM//vgDe/bsQWFhIR544AEYDK7JuEgIIYS4W/3qoRjaupZH5/V2aVgNJ98Zgi4NoyTtz1kyTIHz2wtglVgyzFqwVo0+zWsgMkQraf8AlQrD42Pln4jzXlX80LVRFN56oBV+mdTVXLZc0WGUfI1w5ZVwlznLcjBDunUgbRD5fBoVCrq3n7UMgRf7LrT5rlDZvsbehW94OXul7pv5ZRj2xS78yrPmuaMoR0PlInl4+dq1axU/+aZNmzg/L168GDExMThy5Aj69OkDAJg9ezYSEhIwf/58836NGzc2/zsvLw8//fQTfv31VwwaNAgA8Ntvv6FevXrYunUrhg4dqni9CSGEkMqoT7MaiAiWFnwCtonUXD5MlucmNjZSuIdajs/GtcPkX4/g5UHN8dKK44L7hQVpEF83UrROTWqEIf1WEbqyHl4ECAzFn9S7kWW7g3Unvk3p2GzRznT75xR5raTcgF3nb6GglJsVXSyw1lsvfA15Sx7eKSpHcnqO5Dpal83uZTQygPWzOOuebes53XPWnMbZrALMWX0aE7o3kFhrQiyUyf6hkLy8PABAVFTFHyGj0YgNGzagefPmGDp0KGJiYtCtWzesXr3afMyRI0eg0+kwZMgQ87batWujTZs22Ldvn1vrTwghhBALldWSYe5MBvzJf9phz//1R1iQwzljOVrUisDOmf0xSmA5sDeHt8RDHevivqbRdsta/EQnvDSoGb59rKN5G9/wcmuO9HQT36f0b10sR4EUs1efwuRfj9j0mIsNIU+9IXf5rwpFZXokptzEg1/vwZRlRzmvqVTCo1/4lj8zMQXYKk7vt9XxDHfb8at3ZdbcPvo0Vy7K/CVSAMMwmDFjBnr37o02bdoAALKzs1FYWIgPP/wQ7733Hj766CNs2rQJY8aMwY4dO9C3b19kZWUhMDAQ1apV45RXs2ZNZGVl8Z0KZWVlKCuzDKXJz88HAOh0Ouh0Ot5jPM1UL2+tn7P8uX3+3DbAv9vnz20D/Lt9lbJtDOPy9hqMRlnnMBgsPWF6vR4GveVnoXLEfnd6vcHucSZRoWrUrKJ1+XtiKv+J7vUAVLSZPbvNYDDYtCk6VI0pfRtxtuk5741esP1yGXh6GEnllpFjf/kuvaHiGjTyXD9/H73Oe4zu3ueT79q1DtCBimte7PNpMBjwwu9HsPPcbd7XVeD2rhtZnzU9a5UDvV7PCbDLynWAJoA7vNwq6i7T6VCu4w+TlPpOYRhLHR0t05//1gG+0T6pdfOaoHvq1Kk4efIk9uzZY95m+qCPHDkSL7/8MgCgffv22LdvHxYuXIi+ffsKlscwjOBciXnz5mHu3Lk227ds2YLQ0FCeI7xHYmKip6vgUv7cPn9uG+Df7fPntgH+3b7K0LaxjVVYnxGA0bH52Lhxo4vOVnG7cO5cGjYWn5V8VGquCoAaALB92zZcyLf8bK+u3N9dxfmPHz8OzfVjNtu55VVsO3jwEArOuapvne+8/PukpJzBxjunOa/wXZdFOssxu3fvxkWeHLV6I/fcUlw4fwFeNrCRyKQrL4eS/aLb02zX7bZ28MBB5J5lkJkZAKnXz4X0i9i48QLulAFSrtMjR4/CcIXvM1pxbFraWezMUAsebzAYcD3zurl+h48cge5yRXkVOd0qyjl86BCCWMX8++8mBKq5dSwuLgb7Pd67dy+ig/nbodT37K1blvfW2TL9+W8d4N3tq7h27POKoHvatGlYu3Ytdu3ahbp1LcuAREdHQ6PRoFWrVpz9W7ZsaQ7Oa9WqhfLycuTm5nJ6u7Ozs9GzZ0/e882aNQszZsww/5yfn4969ephyJAhiIiI4D3G03Q6HRITEzF48GBotdLn0/kKf26fP7cN8O/2+XPbAP9uX2VqWwKA90QeNCthevIWAEDz5nFI6NfYzt4W4edvY+HZiiGhgwYNRJWLd/DL+VMAgISEBN5j+H53pvO3b98eCe0sScpM29nlmbZ17twZfZu7JtM733mF9mnVqjUSuldkEBe7LvNKdHjj8A4AwH333YcWtcJtyizXG/HKga2y6tqsWVNsvs63xhzxFUFBQSjUl7v1nF27dUWPxtWRWHgSR3P4R45aSysOxZChvZFdUIa5R3fb3b9jx44Y1rqmzXbTZycurgXWZQhnVtdqNKhduwaO3K6oX6dOHTGkVUV5OoMRM/ZXfFY6d+mCU0cPmY8bMnQIQgM1nM9xSEgIUFZq/rlHz56oXy0Esw/vtDnvsGH3I4Ava6NMf98+itS7Fb34Qt8j9vjz3zrAN9pnGjFtj0eDboZhMG3aNPzzzz9ISkpCo0aNOK8HBgaiS5cuNsuInTt3Dg0aVCQx6NSpE7RaLRITEzF27FgAwI0bN3D69GlO8jW2oKAgBAUF2WzXarVe+ws18YU6OsOf2+fPbQP8u33+3DbAv9tHbVNGy9gIpN7Ix4j2dWSdU6Ox3GYEarVQs362Vw5f+9RqteBxtvtq3PL+2DsHX5352qbVs4/hr7sqQP5Q8QC1cE8h8Q2emMqvUWuw8tgNrD8lLeAGgMy8Uvx+6DqGtaklaX+xz7PpdTEqlQoBAZZeeI3G8rlhVEbOdvZbqNFoodVyQyDGaiSBWq2BWsNft3Wns/Fwp7q8r8nBDtyd/a7y5791gHe3T2q9PBp0T5kyBcuWLcOaNWsQHh5unoMdGRlZ8cQJwMyZMzFu3Dj06dMH/fv3x6ZNm7Bu3TokJSWZ933qqafwyiuvoHr16oiKisKrr76K+Ph4czZzQgghhDhu7dReyC/RoXoV2wfWUrl7eRy563O7itRmS9mPlhiqnDzye1cBs/4+JfuwLSk3MbS1tKDbWaLZy1mp1FRWO1tnKhcqQcjOc7cUCbpJ5eLRST4LFixAXl4e+vXrh9jYWPN/K1asMO8zevRoLFy4EPPnz0d8fDx+/PFHrFq1Cr179zbv8/nnn2PUqFEYO3YsevXqhdDQUKxbt87uEzJCCCGE2KdVBzgVcAMV97z21tlWwnN9m6B/XA30bGI/i7ijakZUvBfVFVwDW0pmckdGtFKY7vt86XdYVKbHF1uFh4SzOf11IPLGsMvedOYmZ1e+JOvW301Ghhu4i+0LVAxnf2ftGWw5YxkZcOBiDq7ekTbfl/g/jw8vl2LSpEmYNGmS4OvBwcH4+uuv8fXXXytVNUIIIYQoyF2dda/f38Ll5/j96e74evt5TBvQVLEypbw91NNdOfnSr/1MZj7OZEqb4yoU1EoldQm9JckZmNWOdV6e+MN6C8PwbBTYFwD+OnwNS/ZdxpJ9l3H5w+E4ee0uxn2/HwBw+cPhvOXQ57lyoXSWhBBCCHE5lQL9dc7epCulaUwVfPlIBzSNsU12Zk1qq919//3lI+1Rp2qIe09KfIYSn1epruQUITk9R/Zxjn5mRJYTN2MYed82WfmlnJ9dsa438W0UdBNCCCHEJTg3rdSpI8qdQQ4A1KgShI8eautUGZ0bVLPZ1qCKdzwY8Sfuvjbcre/HSRj/w36M+W4vCsv09g+4526xDkms5c/Y75J1Z7bKzpxu600MzzYx1r8hKb3Y/v1bJdYo6CaEEEKISxhZXUpaBRKbuWFKuMew79GV7NHv3UxgbrsK6Nooyqmy+/AsyRYfZcT6KT2cKpdwWfei+gvrz/PRjLv4IvGcrDLySnSW8thlW32G9KzE/3xBt/U2RmROtxQKrChG/AwF3YQQQghxCZ3BcqerVdMthxhXDS/v0jAKq57vyftaoMa534lQleN41hgnvuWmGwJ9vpD2ikKJx6zj6u/PqgVf46sLA0b4IR/PduvPr1pKTzcF5pUK/QUkhBBCiEvoDJa7Uw11/YhyZAjxwsc72Wx7sF1tm22deIaBKzFkmS9ooN+yf3hpxXGXn0Pp1Qw4w8utXrtbbnnVyDD2z80ILxrG1wNu/XmSluSNPi2VCQXdhBBCCHEJvdHS061Epl6fTPwlsd2OPJMY1sZ2PeSvx3eQdKwSvWx8v1M/ngFA3MAdz+aMDLCZtbQXIDSn2/7VXKoz8G4PEGkIwzDYmnITN/JK7JZP/IdHlwwjhBBCiP/SG5QJwX57qhsuZBegW+PqipTnTmGBavs7gRvAumPuOvWxEW/E7jEuEQhopRALmI1GhpOA7d4RVsfbP8faE5l4cfkxPNW7ESKCtZzXxB4ebD5zE8/9dsT+CYhfoaCbEEIIIS7B7ul2Ru9m0cIJwbzUWw+0wsFLd3iHe/NRIgjuUL+q5H2VGHkQKvGBAiFSsYduf7XtvBPliGMnYANslxETm9Nt2v7i8mMAgJ/2XLLZR2x4+f6L8pdHI76PhpcTQgghxCV0Bvm9R/5iUu9GWDihk+QEcux7dEfj4UU8c7yVNjuhpfnfj3Spb/O6Uj3ofJnRiX/h+z7YfOamy8o24ctebruPY2WbsIeXKz133V0y75Zg3sZUXMtVJrldZUdBNyGEEEJcQm9Qpqe7MpA6vLyHyBD7mIhgGeeTvCsHuycyxIU93doAFSb1auSy8p3RODrM01UgPFTcdfcEGRnbOdfWgTHDME4Nb2cXbxCL4L3YpCWHsGjXRfz3p4OeropfoKCbEEIIIS7RqYFz60BXVpEhWvs7OcnRHml7nXYNqjhYsPV5AGgUWNvdFcSSZBHplFyPHuAGzmJlGxnGZkkv2yXDgCGf7+I9/tT1PPyafBlqkeuAPbzcIKOne//FHAz9fBcOXb4j+RhXOZtVAAC4eLvIwzXxDxR0E0IIIcQl4utGYuVzPZA8awAA9wSTvuyLce3xzoOtUC8qVHCfN+4N736hXxOnzuV4T7ew7x/vgGaRygVS3josl5a/820Mw9j8Dm0uNZFL7/rdEsxZc0a0B5sTdMvo6X7k+/1Iu1mA/yxMNteV+AdKpEYIIYQQl+nc0NLb3bd5DUzo3gBt6kR4sEbea1SHOnb3ia8biXPv3Y9AjbP9JsoHjv3jamBjujJlVaylrExZShPr4SSeI3WKhpGx/R1aJ1ZztheeXXx2fhkaRltCLqmBtNHI4OGF+xAVFoQfn+jsVH2cVaY3ICntFro3rk4PTx1EPd2EEEIIcYuAABXeHdUG43gScBHpHAm4Zw6NU+Tc7gqEjYxn1vyOCgu0uw8F3cpw5bUkVrSRYez+Dp2tG7unu98nSQ7ltziXXYCjGXexNVWZ5HLO+HTLOUz+9Qj++zPN73YUBd2EEEIIIX6kfb2qAIAG1S3D1F/o1wQ7Z/Yz/xzkYE+5kvNwqwQJD7hkXNzT3bNJdYRobRPBVQu134snthwUkc6lQbfoOt325+UvSHJ8yEZuUblNUF+mtwTdUpfrYzeB3Z4FSen4df8Vh+vniL+PXgcAnLh6V7Eyz98swKM/7MfBS56fv+4OFHQTQgghhPiRRRM64YV+TfD7093M21QqFRpUD8PU/k3xUMe6aF27Yoi/KUDnExsZjPBg181ErB8ViqWTuvK+ZmQYxRNtsT3VuxHCgixBd9eGUejbvAZmDm1h91ia062MvBIdNp2+4ZKy7fV02/sdHr6S6/C5+3y8w+badfY5jSnmvpZbjI82ncWc1adlHa83GHE2K9/hOeKueM70zNLD2Jeeg7GLkpUv3AtR0E0IIYQQnzS+a8Uw9Yk9G3q2Il6mZkQwXhvWAnWr2SZke3VoHD4d287c2/bjE53xRkILHH5zEBY/2cW8X+3IYKx8vqfN8YES1x2XQqWqWI875X9DMaBFDOc1o9F1PaFVQ7X3zmeJJBLia+GXSV0RXcX+8PJHu9H0CCX8b30KnvvtqKJlbjh5A6O/24s7ReWC++SV6Fw6WqGgVA+j1WhyhgFm/3MKk5YckrROuDXTEUVllmXM5ATQr606iWFf7Mb3uy5yjl9z/DpSb+TLro8SbuSVeuS8nkJBNyGEEEJ80v9GtsbfL/TEm8NberoqPiu6ShCe7dME0VWCUDsyxLx9+6v9UKdqiM3+47vWR9u6kZgxuLlidQgNtO1NdyQw+fbRjpL2e75vE8EhvlKWAxstIeEdcb9zNwswZdlRHMu4K7jcFwA89uMBpLg40LS+eo0Mg98PZGD72WycyZR/blOAze5Bl7P8t2l4+DfbL5i37b2Qg+l/HMf9X+62ezyN7XAeBd2EEEII8UladQA61q8GjYK9r75m9r0lxKYPbOaaE1jd2IcFabB2am+8KPN8tSKCZZ9W7lDYuFrcRcL7NK/Bu59YJyd7/eYXBzS1eb1xdKjkObnEvT7enCZ5X1fPI7a+dtkBsp4nWt58JgsXbxVythWV6S3lmcph9aA7MlScvWb42Szpwb8rLvnK9jGiJcMIIYQQQnzUM30aY0T72ogJD/J0VURFhwciK194OKn1/TfDyJ/RbR0MCwUlqntnY+9u2pOdAKtLoyhc/nA4Bn22ExeyuQGRPa1iIyT3prarG4kT1/JklU+8m3VcbS9AnvzrEZttb/LM22Z/KhyZfcEeQSJniL2K+rqdVnkfDRNCCCGE+IGaEcGK976ai1OoWLXM+hmM8rOXWwcRjgxRZ5dhCjS4AZO0dvw0Ufq6yjVljgIgvoB77V3LLWG9JO26PJtVIHrIhexC9P9sN/bdlP7ZKtVZusrlLH1X2XqlXYGCbkIIIYQQwulFU7xni+eunb3J+uWKdbrlBc3WMYR1MiuRqpjxBSLcgMd+nXo2qY7YSNv58EIceThAvJv1r/SBr/dYXnOkvHtHscud/c8pXMstwYqLalnZ1vecvw1AWv4CE1fE3JWt95yCbkIIIYQQwmEOTBWIBx/rVh9vPWAv2Z3V0HDIz15u3dPtyJJj7PQApuLkltIoOkzW/nISYhHfoPSvlGGAd9ae4QTv7F7r8T8eklzW4z8dwP6LObJGn7gij0Fl6z2noJsQQgghhHAodT8cXSUQ74+OR6cGUbKOc2ROt3XPnVBPN//57pXBGV5+rxwHhpfLQT3d/mcRa2kuaycdmL9vMDJYsu8yZ5szQesj3+9HJc4/6RH0dhNCCCGEEJeQ2otrO7ycQdMaVfh3FmAzvNxOMMsXs9gfXq48d/R0y+19J845cfWuouW54hKRlUjNFdnLlS/Sq1HQTQghhBBCOJQaTurIskZARS/1hB4NZB1jO7ycn1jbOGWYh5ez5rpLeFvkvnVGN0TdU/vbLn9GfAffAyRnHwaxHzAZ7FyDlW0ouCtQ0E0IIYQQQjiUusdm38vH1QyXfD4jw0Arc/yrdWDQvXEUmsVUwfD4WMllsAMRS/ZyWdWQzcgwGN+1vmtPQnwa3zUoFAiX6gwoZK3xLYR9resMMuZiEIdQ0E0IIYQQQjhMN/RxtYQDZTGDWsYAAJ7o2dC87fdnuuGjh+It5xAJ7R0JdK17urXqAGx5uQ++fawjZ3uLe23iC1r4hpc/26ex/MrIYGQYdG1UzaXnIL5N6ogRhmHQ9p0taPP2ZpTqDKL7lustgXZBqXiQLvZZvVNUjh93X8TtwjKb135NvmzOlm5TZiXrPqegmxBCCCGEcJhuiL9+tINDx3/zaEf89VwPTB/YzLwtukoQxnXh79Hl66WWi2+OKt+Nfa+m0TbbTCENJ5HavX9O6C5vmLtcRqPyyyeter4nnpA5PJ94L77R33xxOMMA5fd6ra/lFouWOXPlSfO/M+6I7ysWH09ddhTvbUjFU0u4GdQPX76DOWvO4PGfDpi3FZTqHJ5y4uso6CaEEEIIIWgcXQXVQrWcpFuxkSFIerUfAtUBeKp3I8llBWvV6NIwirfn2IQ7fdryw6tDmuP/7m9hs//yZ7qLnlPKssNiQ9wBbk+3aR6t3B45uQG0kWEUnzOrUgHqALrN9xd8c7r55mEbHMy0b294uVhJ+9JzAAAnrLKyX79bwvn55LW7iH9nC2b8eUJyvfwJfRoJIYQQQggCNQE48MYgbJ3Rl7O9YXQYUv43FHMeaKXo+djxATvonDqgGUIDNTb792hS3WbbQx3rssqwSqTG06FmEOhlM/W+sdcudleHnJws0lKpQMmv/Alf0K3jWROPvd9LK45JLt/ete7IUHDrYxbuTAcA/HPsesXrskv0bRR0E0IIIYQQABWBN1/vtMZLF/UN0lrqJaWn216mcLVaPKOz6dVvHBx2DwCjO9QBADzXtwliwoPw3ug2DpclRsr7QXzD078cttlm3Tv9/oYU/LTnkvnn09fzwTAM5qw+7fT5HbmUrK8/exnS/Z13foMSQgghhBC/xhle7mCAaDBYbuSl9Bize7r5hoEHaSy3xuygpvu9RGfju9YDADzQtrbgOayr0bF+Vc7Pn41th7T3huH1+1vgwBsD0bxmuEuSSlW2RFX+7KTV0G0A0Om5QewPuy9h/qY0zrbkizn4df8Vu+Uz9lYCd+BSsv48VvYE6RR0E0IIIYQQhzzYriL4lDPfW0l6o7ygm2+YLpuG1T3Hzu78w4SOeLmNHo/fC7oBIDRQLamO1vVSqVQI0qjN/wbkxTRbZ/TB8bcGi+5jZCrf8N3KRs8zvNyavazkZgwwY8VxjPp2L/Q80bEyPd1W5VayC9R2wgwhhBBCCCESfPKftpjQvQE61K8KGMWXKBLjaPZu9o28lI5d9n0/e/9gLTcIBixZoE2vNwwHAliRhFpiT7KU3fgeBdQID8KtAu4yTAse64imMfaXcWMYirr9nc5gf7i20HJd1hgAf9+ba33ocq5N/gTHRk1Y9XRX7tHl1NNNCCGEEEIcE6RRo2ujKGgdmPOtEvzBdSJDtOZ/szu9H+5U12bfMr14T6KSMQTfXPP4OpEI0XJ70++Pj5VUnsHIuCRBG/EefD3S1qQMLQe4n4WCUp3N60r0dLOv8V/3X5HeC+8nPBp0z5s3D126dEF4eDhiYmIwatQopKVx5yJMnDgRKpWK81/37twlI7KysjBhwgTUqlULYWFh6NixI1auXOnOphBCCCGEEBmUCFpjq4ZgSKuaGNGuNoK1asFlxZY82QWdGlTDV+MtCdAiQiwDPoO1tkPF7S2jJMQ6QImNDLF7jNCw93b1IgWPEVtCzcAwqB0ZbKkTxd9+p6jc8ZEl1thzup/99YgiZVonZGQnUlMiuZuv8ejw8p07d2LKlCno0qUL9Ho9Zs+ejSFDhiAlJQVhYZY1IocNG4bFixebfw4MDOSUM2HCBOTl5WHt2rWIjo7GsmXLMG7cOBw+fBgdOjieXZIQQgghhLieozEhwwDf/7ez+Wf2sFh2GNsvLgb94mI4x377aEdM/+M4XhrUjLdsvZ3xsPbqvPjJLth0KgsTejTA2hOZovsKJXbWiKy1HVdLeJi50Qg80rU+LmQXonezGry9l4SY2Ess7sioCfYhDMPYzafg7zwadG/atInz8+LFixETE4MjR46gT58+5u1BQUGoVauWYDnJyclYsGABunbtCgB488038fnnn+Po0aMUdBNCCCGEeCH2bbyjmbbFsi6zh5LzaVYzHBun32ezfWLPhth29ibGdKzjUJ1M+sfFoH9cDNJvFdrdlxEISPiWbzMRe8cMDAOtOgBzR1YsR/b30Wt260AqrztFZaKvsz+eb/xzCq/f3wIRweKfL/Zn+t31qRR0e7oCbHl5Fenwo6KiONuTkpIQExODqlWrom/fvnj//fcRE2N5Wtm7d2+sWLECw4cPR9WqVfHnn3+irKwM/fr14z1PWVkZysosF1d+fj4AQKfTQafzzieBpnp5a/2c5c/t8+e2Af7dPn9uG+Df7aO2+S5/bp8/tw2Q3z6GYcz7MqwMZ0LH63Q6/DutJ/49cxNfbU8HABgMRpv9541ujR1ptzCuY6xD7/Xs+5vjjWHNoFLZtoldnlAIYWS1CwD0Ou7cVb46GQy2Q4UNRqPNvFhOuXrhObHW97R85RNi8vKKE5yf/z6SwfmZ/VBo2YEMGI1GvDuilU05nM8H65r7ee8ltBeZKiH2mRd73RtIrZuKEXq05mYMw2DkyJHIzc3F7t27zdtXrFiBKlWqoEGDBrh06RLmzJkDvV6PI0eOICgoCEBFsD5u3Dhs3rwZGo0GoaGhWLlyJQYP5l9O4Z133sHcuXNtti9btgyhoaGuaSAhhBBCCMH05Io+nwZVGMyIr7gx/+VcAI7mVAyl/rKH3mZfoe39Y40Y1dAzCwB/lxKAtDzb4d99ahnxUCNLnW6WAB8c52+Hyf5sFZanc+eVvxqvx+ZrATiVazkH+9hiPTDrEH//2TMtDGhTzXKLf+y2CkvOS1vijBBrtUMZZBZbngCxP7svJavB3Bt38Uk3PbT3Ltezd1VYkGq55uqHMcgo4h+fwfeZ8BXFxcV49NFHkZeXh4iICMH9vKane+rUqTh58iT27NnD2T5u3Djzv9u0aYPOnTujQYMG2LBhA8aMGQOgYjh5bm4utm7diujoaKxevRr/+c9/sHv3bsTHx9uca9asWZgxY4b55/z8fNSrVw9DhgwRfbM8SafTITExEYMHD4ZWKz6cwxf5c/v8uW2Af7fPn9sG+Hf7qG2+y5/b589tA6S3b3ryFgBAZGQkEhIqkoFtLTqJozlZAICEhASbfYW2N2rUCAn3xynXCAF8bevapwyv/HUKZ28W4Nvx7TH+x0MAgAYNGyIhoYX52PRbRfjg+F7edpgUHbmO5elnzD+3rh2OyWN74PQfJ3Aq9yYA4L6m1ZGQ0Mm8T36JDrMO7eCtb8eOnTCopWVU6GCDEWlLj+LwlVxJS00RwsYOuAFAE1IFCQm9AACvHEiE/t6k8KthcZjavwkAIDI9BwtSLUnZwiMjgKIC3vITEhJwJacYUWFahLOGrfvCd6ZpxLQ9XhF0T5s2DWvXrsWuXbtQt67tkg1ssbGxaNCgAc6fPw8ASE9PxzfffIPTp0+jdevWAIB27dph9+7d+Pbbb7Fw4UKbMoKCgsy95GxardZrf6EmvlBHZ/hz+/y5bYB/t8+f2wb4d/uobb7Ln9vnz20DpLdPpVKZ9wtgJQwTOpZ3uyrAre8lu22x1bRY9mwPm33UAdw6aTTcHma++mrU3H2CNGpotVpoWEuxLZzQGVqt5dY90GrEeJ2qIbh+twQAoLKqg1YLLH+2B4rL9Xj6l8PYl55jr6kO+fjhtpi58qRLyibeo6jMYL6+2PO9L+aUmLcHarhhpljehiu5ZRj0xR4EawNw9t37bV735u9MqfXy6JJhDMNg6tSp+Pvvv7F9+3Y0atTI7jE5OTm4evUqYmMr1iksLi4GwP2yBgC1Wg2j0TPDjQghhBBCCL/ODaoBAMZ3rW/e5nD2ckVXy/Yc63aYftKwJnWHBQkHMSuf64GdM/uZfxZa7Sw0UINlz3TH6A7OJYkTEqjxaGhB3KSozDIcnH0drmNl6bcOsk9fF+4R3nP+FgCgVOe/sZtHe7qnTJmCZcuWYc2aNQgPD0dWVsWwosjISISEhKCwsBDvvPMOHnroIcTGxuLy5ct44403EB0djdGjRwMAWrRogaZNm2Ly5Mn45JNPUL16daxevRqJiYlYv369J5tHCCGEEEKs/PZ0N5y7WYD4OsKJlaTyjsxE0lUJ4r/1FmqHWmTJMHZIExCg4vSKN6gunqPIYG+NKCtdG0Yh404xsvJLZR1H/FMBO+i2eq1UZ0CwVm2TBFCMo6sX+BKPPo5asGAB8vLy0K9fP8TGxpr/W7FiBYCK3upTp05h5MiRaN68OZ544gk0b94cycnJCA+vWJtQq9Vi48aNqFGjBh588EG0bdsWS5cuxS+//MI7Z4YQQgghhHhOsFaNtnWrcm60HVkHGBBeastbPda9Pu9261aYmqURWzKMsw5yxf+vmdLr/9u77/Aoqi4M4Gc2CYGEQCANQiD0Ggi9hAQCAglNmhSRqvRiQboiIlVUEJCi0gSkq4BIkSICKkWlt1ioEgg1oSWkvN8f+XbYTTYhfWcn7+95fCSzM7P37pQzZ+6dO7LglZri95wbGul9fZOrU9q60AIii3vVlt4NfGVUSAXJ72gvFVN5nzjZvqTH7tQfzqV7HaarmLfnL3kYY7sDq6XEqi3dzztR5suXT3bu3Pnc9ZQrV06++eabrCoWEREREeWkDDZ0aTHlTu3+wZDGZdO1Lju71N7Tnfwz/+Ku4l/c9bnrTU/OXb24q7z/YhXpuODX586rKCLNKntJs8peIiIyqHEZGbXhhJy/YXkArbRo6+8tkU9iZX/YrQyvg7IeAFEUJdn+vvrIFZnc3i9dx6bpKj7ZFSbXI5/I9I7VsqKYmsEHL4iIiIjIJtlYQ7c42FtOopPWw9Iz3Ullpkeu8Xn6Wv9/vj41m4Y2FG/XfGlab0iVImZ/2xkUefQ0c62WDUq75ViPhuAKHjnyPXowZ89fkpAAefzUfES/9D66ICLJdubfL93LTNE0iUk3EREREVmVpVbbtEhvN2lrS6meSQdSGxOS+Bo0uzQ/GJu+3yGwnLscGN1E1g6on67lnievQ/J3gWf00QEjRRFxzpMznXN9CqXt5gKJfLr7Lyk9fpvFz7aevC6xKY3mZ8GETafN/o63seM6LZh0ExEREZFNsoVLc9P8IS35Z5ECeSWgrLuIZF9Lt4hI8cJO4mCX9lQgo983tmVFyWchGU/z94pIea/8GV4+fd+l/wG9csKw1cdk/k9/Z3h5HebcTLqJiIiIyLoymtDZ2kBqKTGthlOeZwlqhxo+IiJSuWiBZMvYSoLo6+YsW18PzPDyiiIyKLhMFpaIcsKhf+9meFm9HNemrDqQGhERERFRRiXY2Gt9U7q5gBT+Xdm7gBwa94IUds6T6rq0nqNkpou5oijilMdeXqrlIxv/uCYiiT0A4jLy7PBzvyvLV0kZkA2b1urY0k1EREREVpXRXCfps9BakFoLdEZap4sUzCt57JNfsmdVfhhc3t3s78blLQ8m1rpq0Qx/R3re2dy3YUkpZPKKMksJe1WfzL/jnbTL1sZqSAsm3URERERkVRltYdRii1hqdUnxM5Mko4yHcxq/59nKMvMzVCpq/h7tJb1ry6sNSyWbb1RoBZn3cg35eVSwtPQrkuzz1KSnpXti2yri4eKo/m1cskvt4un6zozQYa5nk/S4HZh0ExEREZFVZfT5ZFu7OE8x5zb597SOVTO1rsyytzOYJb1GjvZ20tbfW3zdnGVhj1rpWmdmum0bl61bqnDGV5JGWuw5kRuxpZuIiIiISCNsYcAl0xIqKWSfptXwdMmbpvVm1fPHlm54ZHXymVK9U5zfpEw5+Zy1DexOuQKTbiIiIiKiLBZaNbG7sqeFFtbU2NqleVbmj2bdyzPxQ1hKsDOyvjypvH4sPc90J5WTo7Tb2v6kVzejYuRhTJy1i5GlOHo5EREREVlVcHkP2TKsofi6pe15ZiNbaOn2Mmm5TnH08kzWw9q/Q68GvtKzvm+Knzs7ZjzlyMmW7tRuHFDOenv9cfmsm7+1i5FluGcRERERkVUpiiLVfFylYD6H589sQosDqSVV0MlBtr0eJHvebpxy9/IcLpOprGhJ/qCdn5Tzcknx8wJ5HeTd1pUytG5Lv1l23WMY2qRs9qyY0m3nmZvWLkKWYtJNRERERDbJ2dHO2kVIxlIKW9m7gJTxyJ9t35nVOWh2tJy3q14szfOa5tmWuqZnV+u3h4ujdK9XIntWTrkak24iIiIisikzO1WTmiVc5e0WFaxdlCyhlV7yn3ROf3fetv7eaZovtRskqb5mLZ0t8X7FCqRr/qRi4xIytTyRJUy6iYiIiMimdKlTXL4d0lDc86dv4LWckJFWWGvm3AFlnr2Kq1MtHxFJ300Al7xpe147n0MqSXcqy1n6PVMrXznPlLu5p0WcLTyzQDaHSTcRERERaVJZz8Qu2cVc81m5JNkr8wOpZXzZeqUKy+tV4uSX0Y2frS9TpbEstdeGpZbnpvceRmZ7nsfGs6VbCxzt9ZWm6qs2RERERKQby/rUkd4NfGV1/3rWLoqulSlg/rq29CTx2d01Pt09B9I4f0rrHamTRxZsnd6Sbr4yjIiIiIg0qXhhJ5nUzs/axSArsjOkL/lK7RnwTjV95NKdR/J+2yoS+SRWeiw5nGyeku7pe20dZY+8qTyOYIv0dQuBiIiIiMjGZLa1GFncITyr15eSFpW9ZGzLiqnO07i8R5Z9Xy3fQvLN4ACp6lMw20ZAr1HCNXtWnMs4OugrTdVXbYiIiIiIrCi1Z5ezS5ECebN0fem5CeDmnCfN8/r7FDT7+5Mu/jKocRlZ8EpNsTcoMvflGsmWyZPObsap/fwJJhXLrq20qEetbFpz7nL17hOJ0dFI8uxeTkRERERkRRltWV7Tv77cehgjpbPxHeAp+ax7DfnhZLgMDi6T5mU+71lbFh/4V16q7SNeLnnFJa+DiIi0qlpUmlf2Ege7xAQ7MzcuPFxSHtHeODBfZr8jNRlZbTWfgnLyWmTWF8bGLf3lkvhauxBZhEk3EREREZEVZbR7eYMybllbkP/rHVBS1h69Im2qpfwO7jbVvFP93JIiBfPKu20qW/zMmHBn1tAmZeXS7Uey/fQNs+kfdfKT+qWf/V7Z1SHBkIEV53zfCNtwNvyB+GbuteuawaSbiIiIiCiL6CGBKuycRw6Ne8EqXeXTIrV7FPkd7WVhj1pScuwP6jQne0j76uY3CLKrZhlar0Z/Z8o6fKabiIiIiMiKcmbYsvSxZsKd5d9s4QfOrvplpKXbwJzbosy+v15LmHQTEREREVmRjnKLLOGW3/LgbI3+P5J5n4BnT/pmNGHNrkQ3Y0k3s269Y/dyIiIiIiIraulXRD7ccd5soK/crH31YnLgr9viniT5XtanjkQ8iJY8Js9/2xkUSYhP/10L0zz3lXolpGqxginPnK4V58giZGOYdBMRERERZZGCTg7pXqaku7P8/m4zKZgv/cvqUceaxaRAPodkibCdQZGiBfPJnYcx6rTEVuLEpPvYhObp+JZnqe7UDlUzU1wzGWlBZ0O3/rF7ORERERFRJn3c2V9aVPaSvgGlMrS8e37HLBvB29YpiiLNK3tJkYKW3z9u+jy2s+OzNsRCKbwz3FI7eHreL54eaekqvnV4oNnfCtu6dY8t3UREREREmfRSLR95qZaPtYuRK5imqM6OdnL3UfrXUdLdWaZ28JPCTlmbfKel1TrpM+ts6dY/3k4jIiIiIiKbVNjZMcPLvlLPV1pWLZqmed9uXl5KezhL6+fMn5aW7qQD53EgNf1j0k1ERERERDbDNEed2LaylPZwlsntqmTZ+tcOqC9VvAuYTSte2En2vh0s3eoWT3PZUpKQJOs2MCPTPW5iIiIiIiKyST6u+WTv28HSs0HJLFtn/dJu8sPrQRY/e97z12l5PjtpS3d6n+l2tM8dKdzOsxG6eZ1e7thiRERERERE6fDRS9XUf+P/w7E9ryU7LaOXJ23pTkvr+NoB9dV/+xd3ff4C2eC1wFLi6+aUo9955WGOfl22YdJNRERERESURIcaxZJNe97z12l5PturgPmo7EoKy6T0CrT0PgHepIJHOpdIaT2eWfc+8zR6Eq+P592ZdBMRERERka5kRWuwaTJsbJy2t3tO9/I05Ih5Hexk0ovPnkFPaZG8DnbJvl/E8ivQLPEumFfOTAqRz7rXTOMSqTMoaf/urBITn8NfmE2YdBMRERERka4s7V070+swTYaNSa/dc/qPp9RqnVTvgJLqv1NapemqIOnPug0GRZwd7cXZ0V48XTI+yvuzAqX9u7PK04Sc/b7sYtWke/r06VKnTh1xcXERT09Pad++vVy4cMFsnj59+oiiKGb/1a9fP9m6fvvtN2natKk4OzuLq6urBAcHy5MnT3KqKkREREREpBFu+Z8lmRnNE82T3kQO2TDUuKVEvXllL/NWc7OcO201Mu3q3q1uiYwWz6wMaf3urKKTcdSsm3T//PPPMnToUDl06JDs2rVL4uLipEWLFvLokfkb7kNDQyU8PFz9b9u2bWaf//bbbxIaGiotWrSQI0eOyNGjR2XYsGFi4Pj7RERERESUAZaSYdOW7m+HBMibzcpl+nsstXQPalwm0+/vbl/dW/33aw1LZWpdIiK+7s6ZXke66STrtrfml+/YscPs72XLlomnp6f88ccf0qhRI3W6o6OjFClSJMX1vPXWW/L666/L2LFj1WnlymX+ACAiIiIiIg1LQ16aFUNx4f/9y02f6fYumE9c8jokm3fz0IbSbv4v6Vi75RImbeguUdhJrtx9LM0qecnRS/dSXFuzSp7SpXZxaVLRU51W0MlBFvWoKYNW/ZmOcplzc86jm1d45TSrJt1JRUZGiohI4cKFzabv27dPPD09xdXVVRo3bixTp04VT8/EnSgiIkIOHz4sr7zyigQEBMg///wjFStWlKlTp0pgYKDF74mJiZGYmBj176ioKBERiY2NldjY2OyoWqYZy6XV8mWWnuun57qJ6Lt+eq6biL7rx7rZLj3XT891E9F3/Vg3bYmNjVP/HRcb99yyQzJfv7j4eImNjRXEPxvZKyE+TuLjzUf6io2NlcpFnKVIAUe5ERWTdDXqPKa8XPIkmyc+Lk7i457VMzYuTrYND5DIJ7Fy4mqk2bzeBfPK9cho9W+XvPbSpLybSEK8xCY8K19cXOZGJYuLi5P4hJx/yFrL+2Zay6YA2rhfAUDatWsn9+7dkwMHDqjT161bJ/nz5xdfX1+5ePGiTJgwQeLi4uSPP/4QR0dHOXTokDRo0EAKFy4sH3/8sVSvXl1WrFghCxYskNOnT1ts8X7//fdl0qRJyaavXr1anJxy9t1zRERERESUdo9iRcb/nth2OLlWnBRInrOKiMisU3Zy+aEiDb0SpEvpjCWLb/yW+D0vl4mX+p6QW09EphxPnDa1dpz8fluR7y49G2V8ToPERHniH3Zy/6nlFmzjPKfuKnLiriJtiifIxD/N20LH+ceJVz6RNw8lTh9SKV4quCambSfuKLI07Nl3jq4WJxsv2sm/DxK/r65HgrxSNnl9ky4nItKnXLws/8su2byWzKofJ8vDDHLybs49wtu9TLzU89REumrR48ePpXv37hIZGSkFChRIcT7NtHQPGzZMTp48KQcPHjSb3rVrV/Xffn5+Urt2bfH19ZUffvhBOnbsKAn/v9sycOBA6du3r4iI1KhRQ/bs2SNLly6V6dOnJ/uucePGyYgRI9S/o6KipHjx4tKiRYtUfyxrio2NlV27dknz5s3FwSF5NxZbp+f66bluIvqun57rJqLv+rFutkvP9dNz3UT0XT/WTVvuPX4q43/fJyIiL7zwgnikMDJ3wyax8vOFm5Jw9WSG6/fGbz+KiEjVqtWkVa1icvnuY5lyPDFfadG8uTw+cV2+u5Q4EPT7bStJq7rFRURkw60/5ODfdyyus1WrVon////fj5/GycQ/94qISLOKHlKjhKu8GpT4DPabhxK/v07duhJY1k1EROzO3JSlYSdERKSdb7z0ad9cfP+6K0PXJE4r5uMjrVr5Jfteh7MRsjTsuNm0OrVryvK/TqTpt2jdsqVsjzwhcjciTfNnFS3vm8Ye08+jiaR7+PDhsmXLFtm/f7/4+PikOm/RokXF19dX/vrrL/VvEZHKlSubzVepUiW5cuWKxXU4OjqKo2Pyg9PBwUGzG9TIFsqYGXqun57rJqLv+um5biL6rh/rZrv0XD89101E3/Vj3bTBwf5Zy6e9g32K5XZ3cJAXq/vItusnM10/ezs7cXBwkDz2z9bh6OggdoZnrcR9GpZW/z2rS3X5+McLsv73a8nLn6Qc+ZRn6+hYq7i0qlo02TJ2//9+ERF7+2fzN/WGODg4iJ3ds7TOoBgs1tV0OaM8JvMFlnWXOd2qS60pu5PNJyLi6Jgnza9EyyoQbe+baS2XVYf3BiDDhg2Tb7/9Vvbu3SulSj1/VL07d+7I1atX1WS7ZMmS4u3tnexVY2FhYeLr65st5SYiIiIiotzD0quy7BRFUspBPQvklZkv+adp3Q4mg7MlpPDkr+lUX7eMjSLuYJ889TMdOd3OoJi9au155cgJCdrtWZ4uVm3pHjp0qKxevVo2b94sLi4ucuPGDRERKViwoOTLl08ePnwo77//vnTq1EmKFi0qly5dkvHjx4u7u7t06NBBRBKH8h81apRMnDhR/P39pXr16vLVV1/J+fPnZePGjdasHhERERERZSMlS8Ymfz5jLmyaZCuKZMlo3qatx2lJMisVLSALX6kpnvkd5NrJtI+SXrlo8sdoTV9LlpZG7JweDYxJdxZYuHChiIgEBwebTV+2bJn06dNH7Ozs5NSpU7JixQq5f/++FC1aVJo0aSLr1q0TFxcXdf4333xToqOj5a233pK7d++Kv7+/7Nq1S8qUKZOT1SEiIiIiIh0zfX7c0UJ37cxKaYzrpNNbVi0qsbGxcu1k2tdtMalO9z2LjGfBE9tWlknfn82hb9MWqybdzxs4PV++fLJz5840rWvs2LFm7+kmIiIiIiL9cXVykBolXCUBIu75Uxi6PIsZs5a8DnZyePwLYlAUsTOk3L08o9LSvTyj7CwU1pCDz2j7FSuY7mXY0k1ERERERJTDFEWRbwcHqP/OCaa5sFeBvNn2PSm+BjsdyWcZT8vPfNsZkv9W6f31MtO9PCMJPpNuIiIiIiIiK8j5UbQtZ39ZXYpyXvkzvY7XAi0PTm3pN0vvz5iZHNhCzp+t36clTLqJiIiIiIgywMkxa9Kp7W8EydW7j6Waj6vFz1NK+i1J6Vlzyy3d6cuEn/d4cGrY0k1ERERERETp0r56Mdlx+oYElHHL1HoqFS0glSyMLm70vFzX0eH5b4K21NKc3tbn9ObAvm5O8igmTtpU87aY9CdVzjO//BXxUP2bSTcREREREVEukFLSm8feIEv71MnZwljQqJyHNKvkafG1YEYWW5qzuZe+m3Me+entYDEYFDlzPTLVeTvX8pGXavlI1y8OqdN0knMz6SYiIiIiIkqNtZO/vA6pv57MzqDI4t6pJ/+Wku4i6RwULr29yxVFEcP/W7if1708aUu4s6OdONrFp+8LNer5/RCIiIiIiIhys8wM250Jo0MrSPvq3tKgdOa6r4tYfqa7eGGndK0jvb+C6Vc62D2/Wd10/QdGNpbgota+3ZE12NJNRERERESkQUOCy2bZuiw9Um06KS33FdI7kJrpQG0Odulr783IaOdaxZZuIiIiIiIinbP0yrCMjCievu989u/0J936ybqZdBMREREREaVCH52ck1OUZ93OA8u6pzhfW39vEcnIM93P/v28pBswX7+eWrrZvZyIiIiIiCgVVnqkO9spiiI/jwqWIxfvyov/T6wtmdutuoik733hIubdy/Okt6VbR1k3k24iIiIiIqJUpPdZZlviU8hJfAqlPqCapa7paWEwybMd7FNfh6KYJ/XsXk5ERERERJRL+Lo5Z2i51FqPbVG6u5dzIDURYdJNRERERERk0boB9WVi28oSXMEjQ8t/2rW6HH2nmfq3rTfeZuaZbvt0ZtEZbV3XInYvJyIiIiIisqBeaTepl4l3ZBsMini4OKp/+xTKlxXFsprnPdN9YHQTuXbvibz85SERMU+c9ZREpxdbuomIiIiIiLLRN4MDJKicuyztXcfaRclWxQs7SYMyz25SJG3cntCmcuor0Omj80y6iYiIiIiIslEt30Ky8rV6Us7Lxarl2D+qiXStXTzDy6f/mW5zz+thrtOcm0k3ERERERFRblDCzUlqlHDN8PJ57NOXPqanS7mee58z6SYiIiIiIqLnmtzOL13zJ23ZNv3zxHst5OwHIWaf53Wwy2DJtI1JNxERERERET1XSXdnGRJcJh1LpNx8XdDJQZzymI/rXbOEq3Su5SOjQipksITaxNHLiYiIiIiIyKImSV6Xlp7W6KQt3c97ZltRFPmos7+IiMTGxqb5e7SOSTcRERERERGZqe1bSGZ3rS7eruavOXPP75jCEsnp+Tnt9GD3ciIiIiIiolwiPYlw8cJOYpekufqlWj4SUsUrbd+VpHt5SXfnVOfWKybdREREREREuUTSRDglKXUFz2NvkM971k6WjFv8riSzBJf3kIltK8u6AfXTVAa9YPdyIiIiIiIiMoP0vpTbAkOSrFtRFOnbsFSm12tr2NJNRERERERE6bKkd21xsHtOa7d+e4ynC5NuIiIiIiIiMvO8du7gCp5y7oNQi58Zk/HG5Twsfp7bsHs5ERERERFRbpHG1ue09C63t7PchntgdFM5fvW+tKictgHX9I4t3URERERERLlE6VRHEH8mrU901yjhKiIiwSbv8y5SMK+E+hURQxoGWzPS8+vF2NJNRERERESUS9QuWVg+7uwvpT3Slnw/z+JeteWHU+HSzr9YlqxPj5h0ExERERER5SIv1fJ5/kxpHL3cLb+j9GpQMnMF0jl2LyciIiIiIiIzmX9hGBkx6SYiIiIiIiIRETE+hl2nZGHrFkRH2L2ciIiIiIiIRERk79vBsvvcTXmlnm+Ofq+Ox1Fj0k1ERERERESJSro7S7+g0tYuhq5YtXv59OnTpU6dOuLi4iKenp7Svn17uXDhgtk8ffr0EUVRzP6rX7++xfUBkJYtW4qiKLJp06YcqAERERERERFRyqyadP/8888ydOhQOXTokOzatUvi4uKkRYsW8ujRI7P5QkNDJTw8XP1v27ZtFtf36aefiqLnF7wRERERERGRTbFq9/IdO3aY/b1s2TLx9PSUP/74Qxo1aqROd3R0lCJFiqS6rhMnTsisWbPk6NGjUrRo0WwpLxEREREREWU9Bzv9jvGtqZpFRkaKiEjhwuYj5e3bt088PT2lfPny0r9/f4mIiDD7/PHjx/Lyyy/LZ5999tzknIiIiIiIiLRhfKuKUs4zvwxrWtbaRck2mhlIDYCMGDFCAgMDxc/PT53esmVL6dy5s/j6+srFixdlwoQJ0rRpU/njjz/E0dFRRETeeustCQgIkHbt2qXpu2JiYiQmJkb9OyoqSkREYmNjJTY2NgtrlXWM5dJq+TJLz/XTc91E9F0/PddNRN/1Y91sl57rp+e6iei7fqyb7dJz/fRSt74NSkjfBiVExLwutlC/tJZNAaCJ954PHTpUfvjhBzl48KD4+PikOF94eLj4+vrK2rVrpWPHjrJlyxZ5++235dixY5I/f34REVEURb777jtp3769xXW8//77MmnSpGTTV69eLU5OTllSHyIiIiIiItKvx48fS/fu3SUyMlIKFCiQ4nyaSLqHDx8umzZtkv3790upUqWeO3+5cuWkX79+MmbMGHnzzTdl7ty5YjA86ykfHx8vBoNBgoKCZN++fcmWt9TSXbx4cbl9+3aqP5Y1xcbGyq5du6R58+bi4OBg7eJkOT3XT891E9F3/fRcNxF91491s116rp+e6yai7/qxbrZLz/XTc91EbKN+UVFR4u7u/tyk26rdywHI8OHD5bvvvpN9+/alKeG+c+eOXL16VR0sbezYsdKvXz+zeapWrSqzZ8+Wtm3bWlyHo6Oj2jXdlIODg2Y3qJEtlDEz9Fw/PddNRN/103PdRPRdP9bNdum5fnqum4i+68e62S4910/PdRPRdv3SWi6rJt1Dhw6V1atXy+bNm8XFxUVu3LghIiIFCxaUfPnyycOHD+X999+XTp06SdGiReXSpUsyfvx4cXd3lw4dOoiISJEiRSwOnlaiRIk0JfFERERERERE2cWqSffChQtFRCQ4ONhs+rJly6RPnz5iZ2cnp06dkhUrVsj9+/elaNGi0qRJE1m3bp24uLhYocREREREREREaWf17uWpyZcvn+zcuTPL10tERERERESUEzT1nm4iIiIiIiIiPWHSTURERERERJRNmHQTERERERERZRMm3URERERERETZhEk3ERERERERUTZh0k1ERERERESUTZh0ExEREREREWUTq76nWyuM7/WOioqycklSFhsbK48fP5aoqChxcHCwdnGynJ7rp+e6iei7fnqum4i+68e62S4910/PdRPRd/1YN9ul5/rpuW4itlE/Y/5ozCdTwqRbRB48eCAiIsWLF7dySYiIiIiIiMiWPHjwQAoWLJji5wqel5bnAgkJCXL9+nVxcXERRVGsXRyLoqKipHjx4nL16lUpUKCAtYuT5fRcPz3XTUTf9dNz3UT0XT/WzXbpuX56rpuIvuvHutkuPddPz3UTsY36AZAHDx6It7e3GAwpP7nNlm4RMRgM4uPjY+1ipEmBAgU0u9NlBT3XT891E9F3/fRcNxF91491s116rp+e6yai7/qxbrZLz/XTc91EtF+/1Fq4jTiQGhEREREREVE2YdJNRERERERElE2YdNsIR0dHmThxojg6Olq7KNlCz/XTc91E9F0/PddNRN/1Y91sl57rp+e6iei7fqyb7dJz/fRcNxF91Y8DqRERERERERFlE7Z0ExEREREREWUTJt1ERERERERE2YRJNxEREREREVE2YdJNRERERERElE2YdJMkJCSIiIitj6ln6+Un/eC+aNty8/ZjPCDKOtwPbV9u3oaMB1mLSTeJwZC4G/z3339WLknmKIoiIiLTp0+XvXv3ioh2DjRKmek20sv2Mu6LN27csHJJKCOM289IL/tlWjAekDXpLR4wFtg+xgPGg6zCpJtERGTTpk3SpUsXuXPnjrWLkmlHjx6VGTNmSExMTLKTpZYlPQEY7zDqnXEbzZ07V44cOSIi+qj7/PnzZdy4cSKiryCth22TFvPnz5fmzZuLSPKLLr1jPLA+xgP9xAO9xgIR2982acV4wHiQFZh0k4iIPHnyRK5duyZRUVEiYtsn0q5du8qdO3fk8uXLImI7dTGeAFasWCFXrlxR7zDmFqtWrZLJkyeLiOii7p6envL111/LsWPHdBWkjdtm165dcuvWLd1dRBp5eXlJVFSUnD171tpFyXGMB9bHeKCfeKDXWCDCeJAbMB5kHds+k1GGmO5kxhPkyy+/LGXKlJERI0aIiG0EuZS6oXXt2lViYmJk5syZImIbdTH6559/5KOPPpJ9+/aJiEh8fLx1C5QDjPvjuHHj5M6dO3Ly5EkRsa0WgaT7YkJCgjRv3lxatmwpW7duFRHbDlSmEhIS5MyZMxISEiJhYWGiKIpNbStLLJW/bt26EhERId99950VSpRzGA+0i/HA9uJBbooFIowHesN4kL20/8tRljPdyUzvug4cOFBu3bol586dExHtBzlj2ZcvXy6ff/65PHz4UP3svffek+PHj8uJEyesVbwMKVOmjFSsWFGWLl0qIiJ2dnZWLlHWS7pfGffHhg0byq1bt2T9+vUiYltduIxlffDggSiKIgaDQVxdXaVatWqyePFiiY6OFoPBoPljKi0MBoNUqVJFunXrJjNmzJCHDx/a1LayxFj+p0+fqtNKlCgh77zzjixdulTOnz9vraJlO8YD7WI8sL14kJtigQjjgd4wHmQvJt251IIFC6Rs2bKybNky+ffff0VEpFmzZnL9+nVZvny5iNhGkIuNjZWVK1fKkiVLpEKFCrJ48WI5efKktGnTRiIjI+WXX34REW2eIJLe7Ta2YkybNk2uXbsma9assUaxsp1xv1q3bp0sWLBAne7p6SnvvvuurF+/Xm3dsCXLly+XFi1ayNatW+XWrVsiIjJ58mRxc3OTqVOniohtHFNJJd1PY2NjRUSkbdu2cv36dbl+/brF+WzNtGnTpE+fPmqCIyISHBwsLi4ucurUKRHRb0sj44H1MR7oJx7oNRaIMB4wHiwXEdvYf7UYD5h050IAJDg4WAICAmTZsmUSEBAgM2fOlKioKJk7d678+OOPcvr0aWsX06KkJ3IHBwfZsWOHbNu2TXr37i1ff/21tGrVShYuXCi1a9eWjz/+WP777z9NniCMdxS3bNkiDx8+VE/ghQsXlqpVq8r+/ftFRJsXiJl1+/ZtWbp0qXz00UdSs2ZNWbRokVy8eFFatWolrq6u6kWWLQU1g8Eg/v7+0rNnT+nTp49MmjRJoqKipFGjRvLvv//aVF1MGffTn376Se7cuSMODg4iktjlLCYmRr2ItIUuZ6mpWLGixMfHy5QpUyQwMFAWL14sJUuWlJCQEJk4caI8ffpUty2NjAfWx3ign3ig11ggwnjAeMB4kCkg3YuPj0/xs7///hsLFixA7dq1UaNGDfj7+6NUqVLYsGHDc5fNaaZl+f333/H777/jzz//NJvn33//xbfffovq1avDz88PiqJg48aNAIC4uLgcLW9ahIWFwcnJCTVq1EC/fv1w7tw5AMD+/fuRN29eHDhwwMolzBqW9qP79+/jzp07GDBgAJo3bw43NzesWLEC9erVg7+/Px4/fmyFkqZNasfFb7/9hg8//BDe3t5o3bo12rRpA0VRsH79+hwsYdbau3cvKlWqhCJFimDx4sU4dOgQAGDjxo1o0KBBsuNQ61Lafg8ePMCVK1fQt29fBAYGwsfHByNGjIC7u7t6TkxISMjJomY5xgPGA2vTUzzIbbEAYDxgPGA8yCgF0OFtU1IlJCSodxyN3bTc3NykevXq0qRJE3W+y5cvyz///CMffvihHDhwQHx9feW3334TV1dXK5XcHAD1btSECRNkzZo1oiiKREREyJQpU2TAgAHi6Oiozn/37l25ePGijB8/XiIiIuTYsWPWKroZ03oYPX78WBYuXCj79++XPXv2SN++faVOnTqyf/9+cXNzk+nTp4uI7d45Nt0HT58+LbGxseLl5SXe3t7qPDdv3pS1a9fK5s2bJTw8XC5cuCBr166VLl26WPzNrMm0Pt98843cuHFDnj59Kr169ZJChQqpnz169EjmzZsnYWFhsnz5cmnXrp0sX75cChQooKn6WJL0N4+Li5NLly7JihUrZNeuXRIRESGdO3cWf39/effdd2XixInSq1cvK5Y47Uy336ZNm+Tq1auiKIp069ZN3N3d1fkiIiJk1apV8s0338iRI0ekQ4cO6vOltorxgPHA2vQUD3JDLBBhPBBhPGA8yLrCkk6Z3oEbNWoUihYtihdffBFNmzZF7dq1sWLFCovLbdmyBQ0bNsT333+fbD3WYHoH64MPPoCXlxf27duHBw8eYOjQoVAUBZMmTUJsbCwA8/KGhYWhYsWK+Pnnn3O83EmZ1uPGjRu4c+cOIiMjATwr88qVKzF48GB4eHhAURT4+vomm8eWmJZ5woQJKF26NEqXLg0XFxesWLECd+7cMZv/ypUrOHz4MPz9/dGyZcucLm66jBkzBl5eXmjRogW8vLzQtGlTbN++3eLd38WLF6Nw4cI4efKkFUqaPqblf/z4MW7dumX2eVhYGLZs2QI/Pz907doViqKgdOnSuHTpUk4XNd1M98cxY8agZMmSqFu3LoKCglC6dGn8+++/yZa5ceMGvv32W7i5uWHbtm05WdwsxXjAeGBteo0Heo0FAONBUowHjAeZwaQ7F5g/fz5KliyJ3377DQCwcOFC5MmTB6VKlcKiRYvU+WJiYgAk7sQNGzbEkCFDrFJeo02bNuHhw4fq3+fOnUPr1q3xww8/qJ8XKlQIPXv2hJ2dHT744ANER0ebrePevXvw9fXFli1bcrTsSZmeGCZPnoxGjRrBx8cH/fr1w65du8zmjYmJwT///IORI0eidOnSGDNmTE4XN0uYdteZNGkSihQpgp07dyI+Ph4vv/wyXF1d8cknn6gXkcCzE+L58+fh5eWlqe6Upttwzpw58PHxwR9//AEA2LBhAxRFQWBgILZt26bOa/obBAcH46233srZQqeTaR2nT5+OZs2aoUSJEhg5ciROnz5tNu+tW7ewd+9eDBw4EIULF1a7nGmx225Sc+bMgbe3N44cOQIg8UJYURR4e3vj7NmzAKAGaSDxPNKkSRPMmzfPKuXNSowHjAfWoKd4kBtiAcB4wHjAeJDVmHTrzJAhQ3D8+HH17ydPnmDo0KH48MMPAQCbN29GwYIFMXHiRLzyyivw9vY2u6NlPMn26NEDPXr0wNOnT3O2Av+3aNEilCxZEh9//LH6LNeNGzewcOFCPH78GPv374ePjw8+++wzAECvXr2gKApGjhxpdpI3BsC//vrLKvVI6t1334WbmxvWrVuHtWvXomnTpqhUqRK2bt2qzmP8zaOjo/Hee+8hJCTE7ISvdZs2bTL7+8yZM2jWrJl6Z9R4Mmzbti0URcEnn3yCu3fvqvMnJCQgIiICFStWxN69e3O07JaMGjUKhw8fBpBYtnv37mHUqFH44osvACQ+x+bq6oqPPvoI1apVQ7Vq1bB161Z1PzQeU40aNcLo0aOtU4nnSHq3+p133kGRIkUwe/ZsfPfdd3B1dUW3bt1w8OBBi8v37t0bdevWzYmiptvHH3+MX3/9Vf375s2bGDhwIFavXg0A+P777+Hi4oLp06ejadOmKF68OMLCwgCYX3Q2bNgQAwcOBGD9u/tpxXjAeGBteooHuSEWAIwHjAeMB9mFSbeOPHr0CBUrVkS5cuVw5swZdfrNmzfx999/4++//0a5cuUwe/ZsAIkHWN68eeHs7KzelQQSB//w8fHBiRMncroKqidPnmDQoEGoW7cuZs6cqd7RioqKAgC88cYb6NWrF548eQIgMRgGBwejUaNGZifAAwcOqCfMnGZ64QAAO3bsQJUqVdRBR3bv3o28efOifv36qFixInbs2KHOa7yoOnLkCDw8PNQBdbTuiy++QOnSpfHRRx+p0y5fvoylS5fi6dOn2L9/P7y9vdU7xC+99BIKFy6MDz74wOyu5YoVK6AoisXuXTnp5MmTCAgIQJ06dXDs2DEAiRfBBw8eREREBM6cOYPy5cvj008/BZC4jR0cHODv7292QXL8+HEoimIW8LTi/v37AJ61SGzbtg0VKlRQy3/kyBHY29vDw8MDzZs3V/df4Nnd7927d6NmzZoIDw/P4dKn7vz583B2dka3bt3w+++/q9P37NmDS5cu4eTJkyhdujTmz58PAFiyZAkURYHBYMDFixfV+fft24eSJUta9ZyYXowHjAfWpqd4kBtiAcB4wHgwGwDjQXZh0q0zkZGRaNSoEUqXLp2s+8+aNWtQp04d3Lt3DwDw448/olOnTli8eHGyLkA3btzIqSInY9oFpH///ggKCjK7o/XkyRMEBwejb9++ABIDX7t27cyer7H2qIpDhgxBhQoVcP36dXXahQsXMGrUKACJgczd3R1ffvkljh49Cl9fX5QrV87s5AYAU6dORbFixXDz5s0cLX9GXbp0CUOHDkX9+vUxffp0dfrt27cBAAMGDMCrr76Kp0+fIiEhAUOGDEG1atXQsGFD9WSYkJCAI0eOaObCcs+ePWjbti1q1aqldiE03uH96quvEBAQoB4v69evR48ePTBo0KBkx1RERETOFjwN3nnnHdSsWVMtf2xsLA4ePKhedGzfvh2FChXC119/jXPnziFPnjzo0qULdu/ebbaeIUOGoEiRIuq5RQuM+9Ovv/6KsmXLolu3bmoXOqOvvvoKzZs3V7u0bt68Gf3798fEiRPNWhNv3LhhdizbCsaDRIwH1qG3eKDnWAAwHjAeMB5kNybdOmF6UFy+fBmVKlVC/fr1zQ6sjRs3wsvLS30Wok2bNnjrrbfUk1FcXJzVL05M70KtWbMGI0eORKFChdSuTY8ePQIALFiwAIqioEOHDvD390fVqlUtDpRgLWFhYShXrhwaNWqE//77T50eGRmJ2NhYtGnTBhMnTlSnt2jRAhUrVkTPnj0BPKvDmDFj1LvqWme8+Lhz5w7efPNNBAYGqndNgcSTYdOmTfH666+r0zp06IATJ06YXWBphWnXqQ0bNiA0NBR169ZVj6mEhATMnDkTlStXxrFjx3D37l20bdvWrFVH68+zLV++HI0bN0ZoaKjaKnH37l1cv34dUVFRaNq0KaZOnQog8ffw8/ODwWDAuHHj1HUkJCRg/PjxardLrTC9SNqxYwdKlCiBXr16mbVwTJ8+HU5OToiMjERUVBTatWuHESNGWFyHLWE8YDywNj3Fg9wQCwDGA8YDxoPsxqRbZ8aMGYMXX3wRtWrVgsFgQKVKldSuJGFhYejSpQsKFSqEUqVKoWrVqmowsfaOmNR7772HQoUKYenSpVi5ciWaNm0KPz8/fPTRR+qB9eWXX6J79+5488031XpoIbAZD+5Lly6hQoUKaNq0Ka5evap+fvv2bZQsWVLtUnfnzh107doVGzZsULeDtU9u6WW6/2zYsAEDBw6Eh4cHihYtijlz5qifvfPOO3BwcECPHj1Qs2ZNVK5cWTMnQ1OmZZkyZQrat2+P6tWrQ1EUs+6Fly9fho+PD0qUKIESJUqgWrVqVnvOKaM2btyIli1bonnz5mYtMBEREfD398eqVasAJHbdGjRoEH766SdNHGepMd1+48ePxxtvvAEfHx8oioIXX3xRvdAKDw9HzZo14ejoiIoVK5rtj3rAeGD9/ZTxwLbjQW6KBQDjAeMB40F2YtKtIwsXLkSBAgVw6NAh/PPPPzh8+DDq1auHMmXKqHe0wsLCsHPnTqxcuVLdAbV0UklISEB4eDiqVKmCxYsXq9NjYmLQq1cvlCxZEnPmzFEPLOMzRIA26mF6cfTjjz9izpw5UBQFrVu3xrVr1wAADx8+xMsvv4ygoCDMmjULzZo1Q0BAgLqsrV1gmRo/fjzc3d2xYMECfP755wgICECNGjUwY8YMdZ73338fXbt2xcCBAzV1MrRk3rx5cHZ2xu7du3Hp0iUsWbJEfaWGMVBfuXIFS5YswfLly9V9UAv7YmpM97Ft27bhjTfeQJ48efDiiy+qF1qXLl1C6dKl0bNnTyxevBghISGoV6+e2Z1vrfv000/h6uqKX375BcePH8fWrVvh7u6Ojh07qhfLxgFYFi9erG43W6jb8zAeWL8ejAf6iQd6jQUA4wHjAeNBTmHSrSMjR45Ex44dzaaFh4ejRo0aqFq1qtngCUZaOJkkvYsWFRWFqlWrqgOSmB4sfn5+qFChAiZMmKAOkqBFo0ePho+PDyZNmoTu3bvDy8sLQUFB6nNA27dvR5cuXeDn54e2bduqFxu2doFluu3+++8/VKpUSR0FFEi8+//aa6+hQoUKmDt3rjrd9LkcrZwMk4qLi0OPHj3Qv39/s+lbt25F9erVUa9ePYuDiWjhmEqrN998E5UrV8bw4cPxwgsvwNvbGyEhIWrXwh9//BFly5ZFjRo10LRpU83e+U7Jyy+/rHbRNTpw4ADy58+Pdu3aqc9lmrKl7ZcaxgPtYDxIZKvxIDfEAoDxgPHAnBbqrrd4wKTbRlkKxv3790flypXVv40746JFi6AoCry8vKw+GnRSpvW4desWgMQAHBwcjNatW6ufGQ/+bt26oVSpUhg+fLhmTvSmd9OAxNFJ3d3dsX37drNppUqVQmBgoDoIRXR0NO7du6fWQ4sXG6kx3XbXrl1DZGQkypYtq3YfNNbrxo0bKFWqFEqXLo0JEyZYpawZNXjwYDRu3DjZ+x3HjRsHRVHg6+uriQF+MuLgwYMoUqQI9u/fr05bsmQJGjRogJCQEHU/vX79Om7duqX5/TTpOTE2NhadO3dGly5d1L+NF4kzZsyAk5MT2rRpk2xAGVvEeMB4YG16jwd6jgUA4wHjAeNBTjAI2ZyEhAQxGBI33a+//iphYWEiItKvXz+Jjo6WyZMni4iIvb29iIgUKVJEBg4cKF26dJESJUpYp9AWmNZj+vTp0qtXLzl//rw4OjrKZ599JgcOHJABAwZIdHS0iIgAEEVRZPbs2fLpp5+KoigCwJpVkCZNmsiuXbvMphnLW758eRFJLLe/v7+sXLlS/vjjDxkyZIhcuXJFHB0dxdXVVRRFkYSEBHV72Qrjtnv77bdl5MiREh4eLj4+PnL8+HF59OiROp+Xl5fUq1dP8ufPL1FRUVbfZpYkJCRYnF6tWjUJDw+XnTt3qttVRKRixYrSsmVLGTBggJQrVy6nipmlHjx4IE+ePBFvb291Ws+ePaVLly6yf/9+GThwoFy/fl2KFi0q7u7umt5PTc8lhw8flujoaLG3t5e2bdvKhg0bZM+ePWJvb6+WPV++fBIQECDOzs5SqVIlaxY90xgPGA+0QC/xIDfGAhHGA8YDxoMcYa1snzLG9M7P2LFjUaNGDaxatQqPHj3CnTt3MHbsWNStWxejR4/GgwcP8O+//6J169YYOXKkupwWuoyYGjVqFIoWLYqVK1fi77//Vqdv3boVBQoUQM2aNdGyZUvUrVsXFSpUUMuvha53U6dOVe98G8sVGRkJNzc3s+fWgMSuPBUrVoSiKBg4cGCOlzU7nDt3DtWqVcMvv/wCANi1axcMBgPGjx+vvhYmOjoanTt3xldffaW5UWkB8/3ou+++w9q1a7FlyxZ1Wps2bVC2bFmsXLkSly5dwt27d9GuXTtMmDDBZp5nM/29jf8+deoUqlSpgrVr15r9Bvfv30f58uXh7u5uNrKwVpmW/d1330WdOnWwbNkyxMXF4cmTJ+jfvz+cnZ3x/fff486dO4iMjETbtm2xbNkyi+uwJYwHjAdaYuvxIDfEAoDxgPGA8cBamHTbqIkTJ8LT0xO7d+9WXwwPJI6EOnXqVPj4+MDZ2Rm+vr7w9/fX7CiaW7duRfHixXHo0CF1WlRUFE6ePAkg8cJk9OjRGDRoEN566y21S4y1D6ik3z916lR8+eWXePDgAYDEUVlr1aqFJUuWqPNERUWhd+/eOH78uOZObBkxbdo09O7dG7179zbbv9atW4c8efKgWbNmaN++PerXr48qVapo8mRoevExcuRIFChQABUrVoSDg4PZBYbxecuCBQuiUqVKqFixouZG2U2J6e/99OlT9Vmnx48fo3nz5qhfv77Z8XflyhV07NgR69at09S2ep5x48bBzc0NP/30k9mou7dv38bw4cNhb2+PsmXLomTJkqhUqZLNPY+YGsYDxgNrs/V4kBtiAcB4wHjAeGBNTLpt0F9//QU/Pz/88MMPABJf5XD06FFMmTIFW7duBZA4IuqGDRvw448/amoUwqQHw9q1a1G1alUAwIkTJzB58mSULVsW9vb26N27N4DkJ0Gt1SM+Ph6vv/46DAaD+jqNf/75B/369UPZsmXx6quvYs6cOWjcuDFq1aqlLmtrF1qWLiwVRYGfn5/aimHcVocPH8bYsWPRvXt3vP7665ofGOi///5D3bp1ceLECVy5cgUbN26Ek5MTXn31VXWew4cPY82aNVizZo267WxpG06bNg0hISFo1qwZ1q1bByCxFaNq1aqoU6cOxo0bh9WrV6NJkyYIDQ21qdGTT5w4gSpVquDAgQMAgHv37uHs2bP49NNP1ecsf/75Z6xcudLmRhZ+HsYDbdWD8cC240FuiAUA4wHjAeOBNTDptkHGEQc/++wz/Pzzz+jduzeqVauG6tWrI0+ePFi4cGGyZbQQEEwPjqFDh+Ktt97Cn3/+CRcXFzRp0gQ+Pj7o3bs3lixZgh07dkBRFPz2229WLLFlpvUYMWKEevAPGzYMjo6OWLFiBYDEO8Rffvkl/Pz8EBgYaNOj0pq6cuWKWv6FCxdCURTMmDHjuV0FtXoynDZtGjp16oRXX33VbBCkrVu3wtnZGf369bO4nBaOqdSY7mPTp0+Hh4cH3n77bXTu3FndZkBi99dBgwahXr16qFy5MkJCQmxuPz137hw8PDywe/dunDp1CoMHD0aFChXg6+sLJycnnDp1KtkyWt9+acV4YF2MB/qJB3qNBQDjAeMB44EWMOnWOEsnuQcPHqB79+6oWbMmDAYD3njjDWzfvh0PHjxA69at8cEHH1ihpKkzPaAOHjyIKlWqYP/+/Xjy5Al27tyJvn37YvXq1eqrKW7duoX69eur77/Uiv/++0/9908//YQaNWqod1MBYMiQIeqFlumdYdPXF2jxYiOtFi9ejHLlyuGnn35S6/fxxx/DYDCoo9Qame67WuqylbRcs2fPRr58+VCrVq1k8xqfGzKOeGqLLly4gFmzZmH37t0AEve/uXPnwmAwYPr06QCgPu8WHh5uc6PSAonnxC5duqBYsWJwcnLCkCFDsGHDBgBA5cqV1XraOsYDxgMtsfV4kNtiAcB4wHjAeGBNTLo1zPSA2rRpE2bNmoUFCxbgwoULiI+Px/Hjx3H06FGzZRo0aICZM2fmdFHTbOPGjejRowfeeusti5/HxsYiMjISrVu3RsOGDTV1Z3XOnDkoUaIE4uPj8c0336Bv374YOnQoAJhdRA0ZMgT58uXDqlWr1Gf6jLRysZFR9+7dg5+fHxo0aICff/7Z7ELLzs7O7N2rWmc8gUdHR2PJkiWwt7fHxIkTk823ceNGvPDCC5raF9Nq7969UBRFvetvat68ebCzs7N4vtBqXU3LtXPnTixbtgyrV6/GvXv38PjxY2zduhUHDhxQ79w/efIE9erVw1dffWWtImcZxgPGA63RSzzIDbEAYDxgPGA8sDYm3TZg1KhRKFasGNq3b4/GjRujbNmyZgOyPHr0CGFhYQgJCUH16tU1dUfSeFERHx+Pq1evomXLlihUqBC6deumzmM8IUZHR2PFihUICgpC7dq1NdWladGiRXB0dMSaNWsAAM2aNUO+fPkQHByszmPaHW3YsGFQFAU7duzI8bJmlZR+98jISFSvXh1169Y1u9D65JNPoCgK1q9fn5PFzJAVK1agYMGCOHLkCIDEfXDBggWws7NL9U6wFvbF9Lh79y4mTZoEBwcHzJ8/H4D5hf78+fOhKAq+/vpraxUxQ0aNGoWyZcuidu3aaNasGQoVKoQzZ86onz958gQXLlxAmzZtULNmTU2dEzOL8cD6xyDjwTO2Hg9ySywAGA8YDxgPrI1Jt8atX78ePj4+6rMLxmBvHPgCAD7//HO0aNECTZo0UXdELTyjYcp4YBw+fBidO3eGp6cnVq5caTbPvXv38M033+CDDz7Q1MAWX3zxBfLkyYPvvvtOnRYVFYUOHTqgUqVK+Oyzz9STh+mF1scff6yJ8mfWqlWrcPr0abNp9+/fV58T2r9/v7p9V69ebRN1fvr0KRo0aIBy5cqpd4ONF1v29vaYMmWKlUuYfikFn4cPH2L06NEwGAxYu3Ztss+/+eYbm9hmRkuXLoWnp6d6kfzll1+aXdzHxcVh+fLlCA0NRWBgoGbPiRnBeGD9/ZTxQF/xQI+xAGA8YDxIxHigLUy6NW7KlCno3r07gMSuFy4uLli0aBGAxJPnxYsX8eDBA2zZskVToxCaWrJkCerXr69egBw9ehSdO3dGUFCQ2ckBMC+7Fk4MP/30ExRFwaRJk8ymv/HGGxg2bBg6duyIwMBAszuLxve0Gmlte6QmISHB7Hd//Pgx8uTJg8aNG+P8+fNm896/fx+enp4ICQnBjh07zO6Ya6nOSbtwmj6jFhQUhFKlSpldbBkHAzJ9b6fWmV5grVmzBjNnzsR7772HEydOqMfdyJEjzS60bG3UT6OxY8fi3XffBQB8++23yJ8/P7744gsAic+zPX78GH///Tc2bNig2XNiRjEeWBfjgW3Hg9wQCwDGA8YDxgOtYtKtUcaT5jvvvIPx48fjxx9/RP78+dWRBxMSErB27VpMnz7d7B17WutqER8fj5UrV6Jq1apo3bq1emD99ttv6NKlCxo1aqQOcKFFYWFhCAoKwosvvqgG444dO6JcuXJ49OgRbt26hU6dOqFRo0ZYunSplUubeX/99Zf67y+//BJXrlzB1atX4e3tjRdeeMHsQis2NhZNmjSBoigYMGCANYr7XKYXvEuXLsWlS5cAmF9sBQYGokyZMur2jY2Nxbfffqu54JQS04ult99+G25ubmjVqhWKFi2KKlWq4L333sOjR4+QkJCA0aNHw8HBwWb2VeP5zLSOffv2xZgxY7Bly5Zk58TPP/8cH374odl50FaDsynGA21gPLDdeJAbYgHAeMB4wHigZUy6NSKlg2HlypVQFAV2dnZYvny5Ov3Bgwdo0aIFRowYkVNFTBNL9YiJicHGjRtRo0YNhISEqAfWoUOH0K1bN1SqVAl79uzJ6aKmWVhYGEJDQ9G6dWsEBgaiZs2auHjxovp5eHg4OnfujIoVK6rvQbRFJ06cgL29PVauXIkxY8bA1dUVFy5cAABcvXoVXl5eaNq0qfqsVEJCAoYOHYpTp05pMpDt3LkTH330EY4ePYqoqCh4enqiZs2auHr1KoBngfvevXsoWbIkAgIC8Msvv5itw5YutrZs2YJixYrhjz/+UKeNHj0aDRs2xEcffYT4+HhERUVh8ODBCAwMtGJJ0+/kyZPqvz/++GNUrFgRBQoUwGeffaZOv3v3Llq1aoX33nvPGkXMUowHjAfWpqd4kNtiAcB4wHjAeKBFTLo1wPSu3caNG7FkyRJs3rxZnT5y5EjY29tjw4YNOHPmDE6dOoUWLVqgRo0amg0EP/zwg9nfMTEx2LBhA2rUqIGWLVuqB9b+/fsxYcIEzQXppMLCwtCsWTMULFjQbGAY413E//77D+PHj9d8PVITHh6OyZMnI1++fChYsCCuX78O4NlIvFevXkXx4sVRr1499OzZE02aNEHVqlXVE6mW6r506VIUK1YMgwcPVlstrly5gipVqqBu3bq4cuWKOu+jR48QEhICRVHw0ksvWavI6bZz5051ICcg8XmuypUr4/79++q54/HjxxgwYABq1Kihbh9jK4et2LdvHxRFUbvsPnr0CAEBAXB3d8e2bdsQHh6uJkK1a9fW7DkxrRgPGA+0QC/xIDfEAoDxgPGA8cAWMOm2MtMDasSIEfDw8EDJkiVRqVIlvPzyy+ozVcOHD4eLiws8PT1Rq1YtTQ+KcPToUZQoUQJ9+vQxm/7kyRMsXrwYBQsWRJcuXZI966a1eiT1999/IyQkBC1btjR7F6tp9x1A+/VIjXEQEicnJ7PXahi31Y0bN9CvXz906NABPXr00OQIkmvWrIGTkxPWrVuHyMhIs8+uXr2KqlWrolatWrhy5Ypa7n79+uHvv//WVD1Sc/DgQSiKgtq1a6vbaeXKlShVqhQiIiIAPNsvL1++DIPBgL1795qtQ6sXWvfv3zf7+/bt2xg7dizs7e3x5ZdfAkh8Xq1+/fqoUqUKnJycUL9+fTRs2FCz58S0Yjx4Rmv1SIrxQPvxIDfEAoDxgPGA8cBWMOm2koSEBLMD6t9//0VoaChOnTqF8PBwLFu2DNWqVUO7du3Uk/+RI0dw8OBBHD9+XJ2mhTtZSU/W9+/fx+zZs1GzZk289tprZp/duHED5cuXR548efD6669bXF7LjHdQQ0NDcfDgQWsXJ9OMv73xhHb58mUcOXIEkydPNhuUIyEhIcWLEC3sg0Y3b95Eo0aNzLqZAYndrQ4dOoRjx47h8uXLCAwMRPHixdG3b180bNgQ/v7+mmuhSc13330HRVHQqFEjtGvXDuvXr8eDBw/g5eWF3r17m8176tQpVK5cGSdOnLBOYdNh8eLFeO2115LtU3fu3MH48eOhKIp6oRUdHY1Dhw5h/fr1OHLkiKbOienFeMB4oAV6ige5JRYAjAcA4wHjgW1g0m0FxgE8jAfGsmXL0LBhQ3Tq1Em9uxMdHY01a9agWrVqaNOmjcUAp4U7saZliI6OxsOHDwEkvrtz3rx5qFatmtmBdePGDfTs2RM//PCDJsqfEWFhYWjdujVq165tE4ErJaa//8OHD80uMC5duoR33nkHLi4ualADgKlTp6qv5gC0d0K8efMmKlWqZPY6nwULFuCll16CoigoWrQoWrZsiadPn2L48OHo0qULevbsqbkWmrTo0aMHGjdujI4dOyIoKAjff/89Dhw4AHd3d3Tq1Anbt2/Hb7/9hlatWqF+/fqar9uiRYugKAr279+PhQsXYtOmTWafm15oJX2diJHW62gJ4wHjgRboLR7kplgAMB5YovU6WsJ4YNvx4HmYdOewGTNmQFEUNTg/fPgQU6ZMQfny5VGlShWzeaOjo7F27VrUqFEDAQEBmgpopt3pgMRXF4SGhqJu3br49ttvASTW7bPPPkOVKlXQpEkTrFq1Ck2aNDE7SdjKneSkzp49ixEjRtjsicG03LNnz0ZISAiaNWuGwYMHq9OvXLmCd999F46Ojhg+fDiaNWuG8uXLa3qb3bx5E8WKFUO/fv2wZ88edOrUCX5+fhg0aBB+/PFHbNiwAcWLF8ecOXOSLauFu8JpYQy8q1atQv/+/XHo0CF07NgRjRs3xqpVq3D8+HH4+/vDx8cH5cqVQ3BwsOYvJFesWAF7e3ts3boVsbGx6NKlC5ydnbFjxw6z+W7cuIGgoCCzFg5bxnjAeKAFeowHuSEWAIwHjAeMB7aESXcO+/3339GhQwf4+Pjg2LFjABKfUZkzZw68vLzQv39/s/mjo6OxdOlS9OrVSzMnyK+++gqKouDrr78GAMycOROenp4YN24cunbtCoPBgBkzZgBIHLhjy5YtqF+/Pvz9/dU7y4B2T/jpZcv1GDt2LIoUKYIZM2ZgwYIFcHd3x4svvqhedNy4cQMLFixAQEAAunfvbhPbbvfu3ShYsCBKly4Nf39/7NmzB7du3QKQOKJp9erV1fd6GmkpYFmyd+9eLF682Gza9evXUaxYMSxZsgTh4eHqhdb27dsRHx+PK1eu4PTp05rqambJsmXLoCgKmjdvrk47d+4cBg4cCFdXV2zbts1s/kGDBqFatWpo2LCh5rfb8zAeMB5oid7igR5jAcB4YIrxgPHAljDptoKzZ8+iR48eKFq0qPq6jTt37mDWrFmoVq0aBg0aZDa/1t6z9+jRI0ybNg2Ojo7YuHEj3nvvPezatUv9fO7cuVAUBdOnTzcr73///Wf2TkzKeaaDU2zatAmVK1dWX42yefNmODs7w8nJCYGBgWb7XUxMjE1tu4iICPz777/Jpt+9exdBQUH4/PPPrVCqjNm7dy8URYGiKAgJCcHChQtx6tQpAIkDBbVt2xYPHjzA6dOn0alTJwQHBye7INPCecOSL774AgaDAf369YO3tzeGDRumfnb+/Hn0798frq6u+PHHHwEkBulu3brh+++/V/dHW7/QYjywjXOKHuWGeKCnWAAwHjAeMB7YMibdOcR051q9ejXeffddKIoCX19ftSvJ7du31QNr6NCh1ipqqpYsWYLevXsjOjoaI0eOhJ2dHTw8PLB9+3az+ebNmweDwYCZM2eqz3EYaeHEkBuZvqsUADZs2IBp06YBSHyFQ+HChTF//nzs3bsXDg4O6NChg/rqBiNbDmgRERFo3bo16tWrZ1Pdlv766y80atQITZs2RZMmTTB8+HC4ublh9uzZmDVrFpo2bap25zpz5gyCg4MxbNgwzW+r2bNnQ1EUteVi0aJFcHd3T3ahNWjQICiKgtDQUFStWhXVq1dXt5/W65gSxoNnGA+sIzfHA1uNBQDjAeMB44EtY9Kdw95++234+vriww8/xODBg+Hn5wdvb2+zriSzZ8+Gl5cXPvroI+sWNgnjwBbff/+9Om3atGlQFAWzZ89ONv/8+fOhKApWrVqVg6UkS0zfVWo68M2lS5cQFRWFgIAATJ48GUBiN7UKFSpAURQMGDDAWkXOMrdu3cL06dPRunVr1KlTR7Ov0kjNhQsX0LFjR7Rt2xa7d+/Gzp070bFjR7Rs2RKKoqB9+/ZqfS5evKgGLi1fhOzbt8/svbL379/H559/Dnd3dwwfPlydfvfuXaxatQr9+vXD2LFj1bvgtrT9UsJ4QNaQW+OBHmIBwHjAeMB4YKuYdOegCxcuoFSpUmY75a+//orWrVvD29tb7SIUERGBtWvXauokYhzYwvhSe9OT97hx4+Dg4IDVq1cnW27jxo267ipiC1J7VymQGJRLlCiBP//8EwAQHh6Onj174ujRo5raBzPq2LFjaNOmDd544w11X7TFffL8+fMIDQ1FixYtcO7cOcTFxeHMmTN47bXXcPz4cQDmx6Wt3DE2LXNkZKTFCy3AvD62uP2SYjwga8jN8UAvsQBgPGA80M6xyHiQdky6c9Dx48eRJ08e/Pzzz2bTd+/ejcKFC6NMmTL4/fffzT7TwoFlaWCLpAfKmDFjUjywLM1POSO1d5UePnwYR44cwb179+Dn54d27drh559/RrNmzfDCCy/oagTJe/fuJXsHrS0KCwtDixYt0KJFC+zfv9/sM1u5qHoe44WWh4cH3nzzTWsXJ9swHlBOYzzQTywAGA/0hPEgd2DSnU0sdeN5+PAhGjZsiHHjxpk9xxAXF4fGjRvDw8MDbdq0SXF5a0g6sIXxhfVA8gN+zJgxyJcvX7JBO8h6nveu0iJFiqBRo0b45ptvUKlSJZQvXx5BQUG6HUFSK8dVZoSFhSE0NBShoaHJXs2hF5GRkfjiiy+gKAo+/fRTaxcn0xgPSAsYD57RyjGVWYwHtofxIPdi0p0NTAPTnTt31JfdA8B7772HatWqYcmSJWogu3v3Ljp06KC5F8KnNLBFagfWoEGD0Lhx45wsJqUiLe8qrVChAqZMmYLIyEicPXtW868UocQLrdatW6N27drqQCt6c+/ePWzatEkTd/Mzg/GgcU4Wk1LBeKBPjAe2g/GgcU4WU3OYdGcx0ztQEydORGBgIFxcXNClSxcsXboUANCnTx/4+/ujTZs2+OCDDxAQEIAGDRqoO6hWDqzUBrZI7cDSyl04SvS8d5X6+/tjwoQJZstoZR+klJ09exYjRozIFdvKVi/4GQ8YD7SG8UCfGA+0j/GA8YBJdxa6ffu2+u9JkybBw8MD3333Hc6fP4+AgACUKVMG165dAwAsXLgQXbt2RVBQELp3767p7lspDWxhemAlPQnm9gNLa9L6rlJuN9ukxfNGbsd4YHkZsj7GA33T4nkjt2M8sLxMbsOkO4vs378fhQsXxs2bN/HkyRPUrVtXHYVw3759yJcvH5YsWZJsuUePHqn/tpW7d7llYAu9s+V3lRJpGeMB2RrGA6LswXhARgahLOHt7S3u7u7y/vvvy507dyQhIUGCgoJk06ZN0qZNG5k1a5a8+uqr8uTJE1m1apVcuHBBREScnJxERASA2NvbW7MKaVagQAHp1q2bTJ06VebMmSNz5syxdpEoHW7fvi0zZsyQvn37SkREhBw4cEDs7OwkPj7e2kUj0gXGA7IVjAdE2YvxgIxsYytqHADx9fWVHj16yDfffCN79+6VmzdvyhtvvCFbtmyRDz/8UAYNGiQiIhcvXpRVq1aJl5eXVKhQQV2HoijWKn6GFChQQDp37iyenp7Spk0baxeH0uHatWvyyy+/SNmyZWXTpk1ib28vcXFxNnNSJ9IyxgPGA1vCeECUfRgPGA9MKQBg7ULYqnPnzkmlSpXUv+/fvy/16tWT0NBQadWqlXTq1Ek6dOggK1euFADy5MkT6dKlizx9+lS2b98udnZ2Vix91mKQti3379+XggULiqIoEh8fr6t9kcgaGA+eYTywLYwHRFmL8eAZxoNn+Ctk0Pfffy/t2rWTkJAQmT9/vhQuXFhcXV3lyy+/lBYtWkiePHlk/Pjx8u677woAMRgMcu3aNbl165b8+eefYmdnJwkJCWIw6KOHPw8o2+Lq6ioiiXdh9XRyJ7IGxgNzjAe2hfGAKOswHphjPHhGH1vUCnx9fcXHx0d+/fVXGTZsmMydO1eOHTsmjRo1kgEDBsjvv/8ugYGBsnnzZnn06JHY29tLkyZN5NixY+Lg4CBxcXG6OaDIdtlatyUiLWI8ID1gPCDKPMYDSgm7l6eD8c5TXFycxMfHy5w5cyQqKkpcXV3l8uXLsnPnTpk5c6bky5dPhg4dKv3795cxY8ZITEyMODo6quth9y0iItvGeEBERCKMB5Q2vJWSDv/995+IJHaVcHR0lOrVq8vBgwelVq1aMm/ePBk9erQMHjxYjh8/Lm5ubjJz5kw5c+aMekAZ72/wgCIism2MB0REJMJ4QGnDpDuNjh49Kr6+vjJq1Ch1OP8WLVpIUFCQdO/eXcLDw6Vfv36yefNmuXnzpjg7O8u9e/dk0aJF6jrYdYuIyPYxHhARkQjjAaUdu5en0f3792XlypXywQcfSOXKlSUkJETGjx8vIiJ9+vQRZ2dnmTFjhri4uMi9e/fk4sWLsnz5cpk1axYHESAi0hHGAyIiEmE8oLRj0p1OYWFhMmPGDNm3b58UKVJE5s2bJ8ePH5cDBw7IoEGDpH79+slGHYyNjRUHBwcrlpqIiLIa4wEREYkwHtDzMenOgMjISDl27JiMGzdObt26Ja1atZIdO3ZIs2bNZMGCBdYuHhER5RDGAyIiEmE8oNQx6c6kd955R06fPi379++XyMhI+fbbb6V9+/bWLhYREeUwxgMiIhJhPKDkmHRnkGkXkSNHjsjWrVtl165dcuDAAT6jQUSUizAeEBGRCOMBpYxJdyYAsDjiYFxcHA8sIqJchPGAiIhEGA/IMibdWSylA42IiHIXxgMiIhJhPCAm3URERERERETZxvD8WYiIiIiIiIgoI5h0ExEREREREWUTJt1ERERERERE2YRJNxEREREREVE2YdJNRERERERElE2YdBMRERERERFlEybdRERERERERNmESTcRERERERFRNmHSTURElAv06dNHFEURRVHEwcFBvLy8pHnz5rJ06VJJSEhI83qWL18urq6u2VdQIiIinWHSTURElEuEhoZKeHi4XLp0SbZv3y5NmjSRN954Q9q0aSNxcXHWLh4REZEuMekmIiLKJRwdHaVIkSJSrFgxqVmzpowfP142b94s27dvl+XLl4uIyKxZs6Rq1ari7OwsxYsXlyFDhsjDhw9FRGTfvn3St29fiYyMVFvN33//fRERefr0qYwePVqKFSsmzs7OUq9ePdm3b591KkpERKQhTLqJiIhysaZNm4q/v798++23IiJiMBhk7ty5cvr0afnqq69k7969Mnr0aBERCQgIkE8//VQKFCgg4eHhEh4eLiNHjhQRkb59+8ovv/wia9eulZMnT0rnzp0lNDRU/vrrL6vVjYiISAsUALB2IYiIiCh79enTR+7fvy+bNm1K9lm3bt3k5MmTcvbs2WSfbdiwQQYPHiy3b98WkcRnut988025f/++Os8///wj5cqVk2vXrom3t7c6vVmzZlK3bl2ZNm1alteHiIjIVthbuwBERERkXQBEURQREfnpp59k2rRpcvbsWYmKipK4uDiJjo6WR48eibOzs8Xl//zzTwEg5cuXN5seExMjbm5u2V5+IiIiLWPSTURElMudO3dOSpUqJZcvX5ZWrVrJoEGDZPLkyVK4cGE5ePCgvPbaaxIbG5vi8gkJCWJnZyd//PGH2NnZmX2WP3/+7C4+ERGRpjHpJiIiysX27t0rp06dkrfeekt+//13iYuLk08++UQMhsRhX9avX282f548eSQ+Pt5sWo0aNSQ+Pl4iIiIkKCgox8pORERkC5h0ExER5RIxMTFy48YNiY+Pl5s3b8qOHTtk+vTp0qZNG+nVq5ecOnVK4uLiZN68edK2bVv55ZdfZNGiRWbrKFmypDx8+FD27Nkj/v7+4uTkJOXLl5dXXnlFevXqJZ988onUqFFDbt++LXv37pWqVatKq1atrFRjIiIi6+Po5URERLnEjh07pGjRolKyZEkJDQ2Vn376SebOnSubN28WOzs7qV69usyaNUs+/PBD8fPzk6+//lqmT59uto6AgAAZNGiQdO3aVTw8PGTmzJkiIrJs2TLp1auXvP3221KhQgV58cUX5fDhw1K8eHFrVJWIiEgzOHo5ERERERERUTZhSzcRERERERFRNmHSTURERERERJRNmHQTERERERERZRMm3URERERERETZhEk3ERERERERUTZh0k1ERERERESUTZh0ExEREREREWUTJt1ERERERERE2YRJNxEREREREVE2YdJNRERERERElE2YdBMRERERERFlEybdRERERERERNnkf9Ee/fkFt8SuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "from dataGen import Gen\n",
    "# from utils import compare, experiment\n",
    "# from train import slidingWindow, criterion, train, test, objective, train_vae\n",
    "# from Encoders import LongShort_TCVAE_Encoder, RnnEncoder, MST_VAE_Encoder, MST_VAE_Encoder_dist\n",
    "# from Decoders import LongShort_TCVAE_Decoder, RnnDecoder, MST_VAE_Decoder, MST_VAE_Decoder_dist\n",
    "# from vae import VariationalAutoencoder, VQ_MST_VAE, VQ_Quantizer\n",
    "\n",
    "import torch; torch.manual_seed(955)\n",
    "import torch.optim as optim\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader\n",
    "# import optuna\n",
    "# from optuna.samplers import TPESampler\n",
    "# import torchaudio\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "# all parameters for generating the time series should be configured in this cell\n",
    "periode = 15 #days\n",
    "step = 5 # mess interval in minutes\n",
    "val = 500\n",
    "n_channels = 1\n",
    "effects = {\n",
    "    \"Pulse\": {\n",
    "        \"occurances\":0,\n",
    "        \"max_amplitude\":1.5,   \n",
    "        \"interval\":40\n",
    "        },\n",
    "    \"Trend\": {\n",
    "        \"occurances\":1,\n",
    "        \"max_slope\":0.005,\n",
    "        \"type\":\"linear\"\n",
    "        },\n",
    "    \"Seasonality\": {\n",
    "        \"occurances\":0,\n",
    "        \"frequency_per_week\":(7, 14), # min and max occurances per week\n",
    "        \"amplitude_range\":(5, 20),\n",
    "        },\n",
    "    \"std_variation\": {\n",
    "        \"occurances\":0,\n",
    "        \"max_value\":10,\n",
    "        \"interval\":1000,\n",
    "        },\n",
    "    \"channels_coupling\":{\n",
    "        \"occurances\":0,\n",
    "        \"coupling_strengh\":20\n",
    "        },\n",
    "    \"Noise\": {\n",
    "        \"occurances\":0,\n",
    "        \"max_slope\":0.005,\n",
    "        \"type\":\"linear\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "X = Gen(periode, step, val, n_channels, effects)\n",
    "x, params, e_params = X.parameters()\n",
    "# pprint.pprint(params)\n",
    "pprint.pprint(e_params)\n",
    "X.show()\n",
    "\n",
    "def lin_size(n, num_layers, first_kernel = None):\n",
    "    \n",
    "    for i in range(0, num_layers):\n",
    "        \n",
    "        if i == 0 and first_kernel != None:\n",
    "            n = 1 + ((n - first_kernel) // 2)\n",
    "        else:\n",
    "            n = 1 + ((n - 2) // 2)\n",
    "            \n",
    "    if n <= 0:\n",
    "        raise ValueError(\"Window Length is too small in relation to the number of Layers\")\n",
    "            \n",
    "    return n * 2 * num_layers\n",
    "\n",
    "class TCVAE_Encoder_unified(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, latent_dims, L=30, slope=0.2, first_kernel=None, modified=True):\n",
    "        super(TCVAE_Encoder_unified, self).__init__()\n",
    "\n",
    "        self.n = lin_size(L, num_layers, first_kernel)\n",
    "        self.cnn_layers = nn.ModuleList()\n",
    "        self.n_channels = input_size\n",
    "        self.L = L\n",
    "        self.num_layers = num_layers\n",
    "        self.latent_dims = latent_dims\n",
    "        self.modified = modified\n",
    "\n",
    "        if self.modified:\n",
    "            self.lin_input = self.n // (2 * self.num_layers)\n",
    "            self.lin_output = self.latent_dims\n",
    "        else:\n",
    "            self.lin_input = self.n * self.n_channels\n",
    "            self.lin_output = self.latent_dims * self.n_channels\n",
    "\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                torch.nn.init.kaiming_uniform_(m.weight, mode=\"fan_in\", nonlinearity=\"leaky_relu\")\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "        # CNN Layers that double the channels each time\n",
    "        for i in range(0, num_layers):\n",
    "            if i == 0:\n",
    "                if first_kernel == None: first_kernel = 2\n",
    "                self.cnn_layers.append(\n",
    "                    nn.Conv1d(input_size, input_size * 2, kernel_size=first_kernel, stride=2, padding=0))\n",
    "                self.cnn_layers.append(nn.LeakyReLU(slope, True))\n",
    "                self.cnn_layers.append(nn.BatchNorm1d(input_size * 2))\n",
    "            else:\n",
    "                self.cnn_layers.append(\n",
    "                    nn.Conv1d(input_size * 2 * i, input_size * 2 * (i + 1), kernel_size=2, stride=2, padding=0))\n",
    "                self.cnn_layers.append(nn.LeakyReLU(slope, True))\n",
    "                self.cnn_layers.append(nn.BatchNorm1d(input_size * 2 * (i + 1)))\n",
    "\n",
    "        # MLP Layers for Mu and logvar output\n",
    "        self.encoder_mu = nn.Linear(self.lin_input, self.lin_output)\n",
    "        self.encoder_logvar = nn.Linear(self.lin_input, self.lin_output)\n",
    "\n",
    "        # Init CNN\n",
    "        self.cnn_layers.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### CNN\n",
    "        for i, cnn in enumerate(self.cnn_layers):\n",
    "            #             print(\"Encoder Cnn\", x.shape)\n",
    "            x = cnn(x)\n",
    "        cnn_shape = x.shape\n",
    "        #         print(\"Encoder after Cnn \", x.shape)\n",
    "        if not self.modified:\n",
    "            x = x.view(x.size(0), -1)\n",
    "        #             print(\"Encoder reshape after Cnn \", x.shape)\n",
    "        # ### MLP\n",
    "        mu = self.encoder_mu(x)\n",
    "        logvar = self.encoder_logvar(x)\n",
    "        #         print(\"Encoder mu after lin \", mu.shape)\n",
    "        if not self.modified:\n",
    "            mu = mu.view(mu.shape[0], self.n_channels, -1)\n",
    "            logvar = logvar.view(logvar.shape[0], self.n_channels, -1)\n",
    "        #             print(\"Encoder mu after reshape \", mu.shape)\n",
    "        # mu.reshape\n",
    "\n",
    "        return mu, logvar\n",
    "    \n",
    "class LongShort_TCVAE_Encoder(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, latent_dims, L=30, slope=0.2, first_kernel=None, modified=True,\n",
    "                 reduction=False):\n",
    "        super(LongShort_TCVAE_Encoder, self).__init__()\n",
    "        self.latent_dims = latent_dims\n",
    "        self.n_channels = input_size\n",
    "        self._modified = modified\n",
    "        self._reduction = reduction\n",
    "        if modified:\n",
    "            self.red_input = 2 * 2 * input_size * num_layers\n",
    "        else:\n",
    "            self.red_input = self.n_channels * 2\n",
    "\n",
    "        self.short_encoder = TCVAE_Encoder_unified(input_size, num_layers, latent_dims, L, slope, first_kernel=None,\n",
    "                                                   modified=self._modified)\n",
    "        self.long_encoder = TCVAE_Encoder_unified(input_size, num_layers, latent_dims, L, slope, first_kernel,\n",
    "                                                  modified=self._modified)\n",
    "\n",
    "        self.reduction_layer = nn.Conv1d(self.red_input, self.red_input // 2, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        short_mu, short_logvar = self.short_encoder(x)\n",
    "        long_mu, long_logvar = self.long_encoder(x)\n",
    "\n",
    "        mu = torch.cat((short_mu, long_mu), axis=1)\n",
    "        logvar = torch.cat((short_logvar, long_logvar), axis=1)\n",
    "\n",
    "        # print(\"Short Encoder mu: \", short_mu.shape)\n",
    "        # print(\"Long Encoder mu: \", long_mu.shape)\n",
    "        #\n",
    "        # print(\"After Cat: \", mu.shape)\n",
    "        if self._reduction:\n",
    "            mu = self.reduction_layer(mu)\n",
    "            logvar = self.reduction_layer(logvar)\n",
    "\n",
    "        return mu, logvar\n",
    "    \n",
    "class VQ_Quantizer(nn.Module):\n",
    "    def __init__(self, num_embed, dim_embed, commit_loss, decay, epsilon=1e-5, inits=None):\n",
    "        super(VQ_Quantizer, self).__init__()\n",
    "\n",
    "        self._num_embed = num_embed\n",
    "        self._dim_embed = dim_embed\n",
    "        self._commit_loss = commit_loss\n",
    "        if inits == None:\n",
    "            self._inits = torch.ones(num_embed)\n",
    "        else:\n",
    "            self._inits = inits\n",
    "        print(\"inits in quantizer\", self._inits)\n",
    "\n",
    "        self._embedding = nn.Embedding(self._num_embed, self._dim_embed)\n",
    "        self._embedding.weight.data.uniform_(-1 / self._num_embed, 1 / self._num_embed)\n",
    "        self._embedding.weight.data[:, 0] = self._inits\n",
    "        self._embedding.weight.data[:, 1] = torch.ones_like(self._embedding.weight.data[:, 1])\n",
    "        print(\"Codebook Init\", self._embedding.weight)\n",
    "\n",
    "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embed))\n",
    "        self._ema_w = nn.Parameter(torch.Tensor(num_embed, dim_embed))\n",
    "        self._ema_w.data.normal_()\n",
    "\n",
    "        self._decay = decay\n",
    "        self._epsilon = epsilon\n",
    "        self._std = 0.8\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : BCL -> BLC\n",
    "#         print(x.shape)\n",
    "        x_shape = x.shape\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        \n",
    "#         print(x.shape)\n",
    "\n",
    "        # flaten the input to have the Channels as embedding space\n",
    "        x_flat = x.view(-1, self._dim_embed)\n",
    "#         print(x_flat.shape)\n",
    "\n",
    "        # Calculate the distance to embeddings\n",
    "\n",
    "#         print(\"the non squared x\", x_flat.shape )\n",
    "#         print(\"the non squared embed weights\", self._embedding.weight.t().shape)\n",
    "#         print(\"the x \", torch.sum(x_flat**2, dim = 1, keepdim = True).shape)\n",
    "#         print(\"the embed \", torch.sum(self._embedding.weight**2, dim = 1).shape)\n",
    "#         print(\"the matmul \", torch.matmul(x_flat, self._embedding.weight.t()).shape)\n",
    "        dist = (torch.sum(x_flat ** 2, dim=1, keepdim=True)\n",
    "                + torch.sum(self._embedding.weight ** 2, dim=1)\n",
    "                - 2 * torch.matmul(x_flat, self._embedding.weight.t()))\n",
    "#         print(dist.shape)\n",
    "\n",
    "        embed_indices = torch.argmin(dist, dim=1).unsqueeze(1)\n",
    "#         print(\"embed indices\",embed_indices)\n",
    "        if self.training:\n",
    "            noise = torch.randn(embed_indices.shape) * self._std\n",
    "            noise = torch.round(noise).to(torch.int32).to(embed_indices)\n",
    "            new_embed_indices = embed_indices + noise\n",
    "            new_embed_indices = torch.clamp(new_embed_indices, max=self._num_embed-1, min =0)\n",
    "#             embed_indices = new_embed_indices\n",
    "        # print(\"noise \",noise.shape)\n",
    "        # print(\"both together\",new_embed_indices)\n",
    "        embed_Matrix = torch.zeros_like(dist)\n",
    "#         embed_Matrix = torch.zeros(embed_indices.shape[0], self._num_embed).to(x)\n",
    "        #         print(embed_Matrix.shape)\n",
    "        embed_Matrix.scatter_(1, embed_indices, 1)\n",
    "#         print(\"embed_indices\", embed_indices)\n",
    "#         print(\"Embedding \", embed_Matrix.shape, embed_Matrix)\n",
    "#         print(\"codebook\", self._embedding.weight.shape, self._embedding.weight)\n",
    "        \n",
    "\n",
    "        # get the corresponding e vectors\n",
    "        quantizer = torch.matmul(embed_Matrix, self._embedding.weight)\n",
    "#         print(\"the quantizer\", quantizer.shape, quantizer)\n",
    "        quantizer = quantizer.view(x_shape).permute(0, 2, 1).contiguous()\n",
    "#         print(\"the quantizer\", quantizer.shape, quantizer)\n",
    "\n",
    "        # Use EMA to update the embedding vectors\n",
    "        if self.training:\n",
    "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
    "                                     (1 - self._decay) * torch.sum(embed_Matrix, 0)\n",
    "\n",
    "            # Laplace smoothing of the cluster size\n",
    "            n = torch.sum(self._ema_cluster_size.data)\n",
    "            self._ema_cluster_size = (\n",
    "                    (self._ema_cluster_size + self._epsilon)\n",
    "                    / (n + self._num_embed * self._epsilon) * n)\n",
    "\n",
    "            dw = torch.matmul(embed_Matrix.t(), x_flat)\n",
    "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
    "#             cb_new_values = self._ema_w / self._ema_cluster_size.unsqueeze(1)\n",
    "#             cb_new_values[:,1] = torch.clamp(cb_new_values[:,1], min = 0, max=1)\n",
    "#             self._embedding.weight = nn.Parameter(cb_new_values)\n",
    "\n",
    "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
    "#             self._embedding.weight = nn.Parameter(torch.clamp(self._embedding.weight[:,1], max=1.5))\n",
    "        #         print(\"quantizer \", quantizer.shape)\n",
    "\n",
    "        # Loss\n",
    "        #         first_loss = F.mse_loss(quantizer, x.detach())\n",
    "        #         second_loss = F.mse_loss(quantizer.detach(), x)\n",
    "        #         loss = first_loss + self._commit_loss * second_loss\n",
    "\n",
    "        # Loss EMA\n",
    "        e_loss = F.mse_loss(quantizer.detach(), x)\n",
    "        loss = self._commit_loss * e_loss\n",
    "        #         print(loss)\n",
    "\n",
    "        # straigh-through gradient\n",
    "        quantizer = x + (quantizer - x).detach()\n",
    "        quantizer = quantizer.permute(0, 2, 1).contiguous()\n",
    "        #         print(\"quantizer \", quantizer.shape)\n",
    "\n",
    "        return quantizer, loss\n",
    "    \n",
    "class VQ_MST_VAE(nn.Module):\n",
    "    def __init__(self, n_channels, num_layers, latent_dims, v_encoder, v_decoder, v_quantizer,\n",
    "                 L=30,\n",
    "                 slope=0.2,\n",
    "                 first_kernel=None,\n",
    "                 commit_loss=0.25,\n",
    "                 modified=True,\n",
    "                 reduction=False,\n",
    "                 inits=None\n",
    "                 ):\n",
    "        super(VQ_MST_VAE, self).__init__()\n",
    "\n",
    "        self._n_channels = n_channels\n",
    "        self._num_layers = num_layers\n",
    "        self._latent_dims = latent_dims\n",
    "        self._v_encoder = v_encoder\n",
    "        self._v_decoder = v_decoder\n",
    "        self._v_quantizer = v_quantizer\n",
    "        self._L = L\n",
    "        self._slope = slope\n",
    "        self._first_kernel = first_kernel\n",
    "        self._commit_loss = commit_loss\n",
    "        self._reduction = reduction\n",
    "        self._modified = modified\n",
    "        if inits == None:\n",
    "            self._inits = torch.ones(n_channels)\n",
    "        else:\n",
    "            self._inits = inits\n",
    "        print(self._inits)\n",
    "        if self._modified:\n",
    "            self._num_embed = self._n_channels * 4 * self._num_layers\n",
    "        else:\n",
    "            self._num_embed = self._n_channels * 2\n",
    "        if self._reduction:\n",
    "            self._num_embed = self._num_embed // 2\n",
    "\n",
    "        self.encoder = self._v_encoder(self._n_channels, self._num_layers, self._latent_dims, self._L, self._slope,\n",
    "                                       self._first_kernel, self._modified, self._reduction)\n",
    "#         self.decoder = self._v_decoder(self._n_channels, self._num_layers, self._latent_dims, self._L, self._slope,\n",
    "#                                        self._first_kernel, self._modified, self._reduction)\n",
    "        self.decoder = self._v_decoder(self._n_channels, self._latent_dims, self._L)\n",
    "        self.quantizer = self._v_quantizer(self._num_embed, self._latent_dims, self._commit_loss, decay=0.99,\n",
    "                                           epsilon=1e-5, inits=self._inits)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(self._num_embed)\n",
    "\n",
    "    def reparametrization_trick(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, v):\n",
    "        mu, logvar = self.encoder(x)\n",
    "\n",
    "        z = self.reparametrization_trick(mu, logvar)\n",
    "        # print(z.shape)\n",
    "        # z = self.bn(self.quantizer._embedding.weight[None,:])\n",
    "        # is_larger = torch.all(torch.gt(z[0], self.quantizer._embedding.weight))\n",
    "#         print(z.shape)\n",
    "        # print(\"Is encoder output larger than the set of vectors?\", is_larger)\n",
    "#         e, loss_quantize = self.quantizer(z)\n",
    "\n",
    "#         print(\"----------------Encoder Output-------------\")\n",
    "#         print(\"mu and logvar\", mu.shape, logvar.shape)\n",
    "#         print(\"----------------Reparametrization-------------\")\n",
    "#         print(\"Z\", z.shape)\n",
    "#         print(\"----------------Quantizer-------------\")\n",
    "#         print(\"quantized shape\", e.shape)\n",
    "#         print(\"loss shape\", loss_quantize)\n",
    "\n",
    "        #         mu_dec, logvar_dec = self.decoder(e)\n",
    "        #         x_rec = self.reparametrization_trick(mu_dec, mu_dec)\n",
    "        x_rec = self.decoder(z)#/v\n",
    "#         print(\"Rec before loss\", x_rec.shape, x_rec)\n",
    "#         print(\"x before loss\",x.shape, x)\n",
    "        loss_rec = F.mse_loss(x_rec, x, reduction='sum')\n",
    "#         print(\"rec loss\", loss_rec)\n",
    "#         print(\"loss_quantize\", loss_quantize)\n",
    "        loss = loss_rec# + loss_quantize\n",
    "\n",
    "#         print(\"----------------Decoding-------------\")\n",
    "#         print(\"----------------Decoder Output-------------\")\n",
    "#         # print(\"mu and logvar Decoder\", mu_dec.shape, logvar_dec.shape)\n",
    "#         print(\"rec shape\", x_rec.shape)\n",
    "        return x_rec, loss, mu, logvar, z#, mu_rec, logvar_rec\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, n_channels, num_layers, latent_dims, v_encoder, v_decoder, v_quantizer,\n",
    "                 L=30,\n",
    "                 slope=0.2,\n",
    "                 first_kernel=None,\n",
    "                 commit_loss=0.25,\n",
    "                 modified=True,\n",
    "                 reduction=False,\n",
    "                 inits=None\n",
    "                 ):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self._n_channels = n_channels\n",
    "        self._num_layers = num_layers\n",
    "        self._latent_dims = latent_dims\n",
    "        self._v_encoder = v_encoder\n",
    "        self._v_decoder = v_decoder\n",
    "        self._v_quantizer = v_quantizer\n",
    "        self._L = L\n",
    "        self._slope = slope\n",
    "        self._first_kernel = first_kernel\n",
    "        self._commit_loss = commit_loss\n",
    "        self._reduction = reduction\n",
    "        self._modified = modified\n",
    "        self.z = None\n",
    "        if inits == None:\n",
    "            self._inits = torch.ones(n_channels)\n",
    "        else:\n",
    "            self._inits = inits\n",
    "        print(self._inits)\n",
    "        if self._modified:\n",
    "            self._num_embed = self._n_channels * 4 * self._num_layers\n",
    "        else:\n",
    "            self._num_embed = self._n_channels * 2\n",
    "        if self._reduction:\n",
    "            self._num_embed = self._num_embed // 2\n",
    "        self.encoder = self._v_encoder(self._n_channels, self._num_layers, self._latent_dims, self._L, self._slope,\n",
    "                                       self._first_kernel, self._modified, self._reduction)\n",
    "        self.decoder = self._v_decoder(self._n_channels, self._latent_dims, self._L)\n",
    "        \n",
    "    def reparametrization_trick(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparametrization_trick(mu, logvar)\n",
    "        self.z= z.detach()\n",
    "#         print(\"Z: \", z.shape)\n",
    "        rec = self.decoder(z)\n",
    "#         print(\"x:\", x)\n",
    "#         print(\"Rec: \", rec)\n",
    "        return rec, mu, logvar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "cd98e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "class Gen_Decoder(nn.Module):\n",
    "    def __init__(self, n_channels, latent_dims, L=30):\n",
    "        super(Gen_Decoder, self).__init__()\n",
    "\n",
    "        self._n_channels = n_channels\n",
    "        self._latent_dims = latent_dims\n",
    "        self._L = L\n",
    "        self._n_params = 5\n",
    "        self._complexity = (self._latent_dims - 2) // self._n_params # for each channel: latent dims - 2 for the mean and std \n",
    "                                                        # divided by the number of parameters to extract\n",
    "#         device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#         self.mean = torch.empty(self._n_channels)\n",
    "#         self.logvar = torch.empty(self._n_channels)\n",
    "#         self.trend_index = torch.empty(self._n_channels, self._complexity)\n",
    "#         self.trend_slope = torch.empty(self._n_channels, self._complexity)\n",
    "        \n",
    "#         self.seasonality_fpw = torch.empty(self._n_channels, self._complexity)\n",
    "#         self.seasonality_amp = torch.empty(self._n_channels, self._complexity)\n",
    "#         self.seasonality_phase = torch.empty(self._n_channels, self._complexity)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.params = {\n",
    "#             \"mean\":torch.empty(self._n_channels),\n",
    "#             \"logvar\":torch.empty(self._n_channels),\n",
    "#             \"trend\":{\n",
    "#                 \"index\":torch.empty(self._n_channels, self._complexity),\n",
    "#                 \"slope\":torch.empty(self._n_channels, self._complexity)\n",
    "#             },\n",
    "#             \"seasonality\":{\n",
    "#                 \"frequency_per_week\":torch.empty(self._n_channels, self._complexity),\n",
    "#                 \"amplitude\":torch.empty(self._n_channels, self._complexity),\n",
    "#                 \"phaseshift\":torch.empty(self._n_channels, self._complexity)\n",
    "#             }\n",
    "#         } \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def reparametrization_trick(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def extract_params(self, codebook):\n",
    "#         codebook = codebook.squeeze(0)\n",
    "#         print(\"Codebook in the gen Decoder\",codebook.shape)\n",
    "        for channel in range(self._n_channels):\n",
    "#             print(\"channel loop index\", channel)\n",
    "#             print(\"Codbook Values\", codebook, codebook.T, codebook.T[0,channel])\n",
    "#             self.mean[channel] = codebook.T[0, channel]\n",
    "#             self.logvar[channel] = codebook.T[1,channel]\n",
    "            mean = codebook.T[0, channel]\n",
    "            logvar = codebook.T[1,channel]\n",
    "#             print(\"exracted mean\", mean.shape, mean)\n",
    "#             print(\"exracted logvar\", logvar.shape, logvar)\n",
    "#             self.params[\"mean\"][channel] = codebook.T[0, channel]\n",
    "#             self.params[\"logvar\"][channel] = 1.0 # torch.clamp(codebook.T[1, channel], max=2)\n",
    "            for i in range(self._complexity):\n",
    "                n_effect = i*self._n_params\n",
    "                trend_index       = codebook.T[2 + n_effect, channel]\n",
    "                trend_slope       = codebook.T[3 + n_effect, channel]\n",
    "                seasonality_fpw   = codebook.T[4 + n_effect, channel]\n",
    "                seasonality_amp   = codebook.T[5 + n_effect, channel]\n",
    "                seasonality_phase = codebook.T[6 + n_effect, channel]\n",
    "#             print(\"exracted trend_index\", trend_index.shape, trend_index)\n",
    "#             print(\"exracted trend_slope\", trend_slope.shape, trend_slope)\n",
    "#             print(\"exracted seasonality_fpw\", seasonality_fpw.shape, seasonality_fpw)\n",
    "#             print(\"exracted seasonality_amp\", seasonality_amp.shape, seasonality_amp)\n",
    "#             print(\"exracted seasonality_phase\", seasonality_phase.shape, seasonality_phase)\n",
    "                \n",
    "#                 self.params[\"trend\"][\"index\"][channel, i]                    = codebook.T[2 + n_effect, channel]\n",
    "#                 self.params[\"trend\"][\"slope\"][channel, i]                    = codebook.T[3 + n_effect, channel]\n",
    "#                 self.params[\"seasonality\"][\"frequency_per_week\"][channel, i] = codebook.T[4 + n_effect, channel]\n",
    "#                 self.params[\"seasonality\"][\"amplitude\"][channel, i]          = codebook.T[5 + n_effect, channel]\n",
    "#                 self.params[\"seasonality\"][\"phaseshift\"][channel, i]         = codebook.T[6 + n_effect, channel]\n",
    "        return mean, logvar, trend_index, trend_slope, seasonality_fpw, seasonality_amp, seasonality_phase\n",
    "    \n",
    "    def gen_rec(self, mean, logvar):\n",
    "#         print(\"extracted mean\", self.mean.shape)\n",
    "#         print(\"extracted std\", self.logvar)\n",
    "        means = mean.unsqueeze(1).expand(-1, self._L)\n",
    "#         print(\"mean after expand and squeeze\", means.shape)\n",
    "#         std = torch.exp(0.5 * self.logvar)\n",
    "        logvars = logvar.unsqueeze(1).expand(-1, self._L)\n",
    "#         means = self.params[\"mean\"].unsqueeze(1).expand(-1, self._L).requires_grad_()\n",
    "#         std = torch.exp(0.5 * self.params[\"logvar\"])\n",
    "#         stds = std.unsqueeze(1).expand(-1, self._L).requires_grad_()\n",
    "#         print(means.shape)\n",
    "#         print(stds.shape)\n",
    "        rec = self.reparametrization_trick(means, logvars)\n",
    "#         print(\"shape of generated rec\",rec.shape)\n",
    "#         print(rec)\n",
    "        return rec\n",
    "    \n",
    "    def add_trend(self, rec, trend_index, trend_slope):    \n",
    "        # generate the trends\n",
    "        trends = torch.zeros_like(rec)\n",
    "#         print(\"trend shape\", trends.shape)\n",
    "#         print(\"trend_index shape\", trend_index.shape)\n",
    "#         print(\"trend_slope shape\", trend_slope.shape)\n",
    "        for channel in range(self._n_channels):\n",
    "            indexes = trend_index.unsqueeze(1)[channel, :]\n",
    "#             print(\"indexes\", indexes.shape)\n",
    "            slopes = trend_slope.unsqueeze(1)[channel, :]\n",
    "#             print(\"slopes\", slopes.shape)\n",
    "            print(self._L)\n",
    "            print(indexes)\n",
    "            mask = torch.arange(self._L).unsqueeze(1) >= indexes.unsqueeze(0)\n",
    "#             print(\"mask\", mask.T.shape)\n",
    "            trends[channel:] = mask.T.sum(dim=1).float() * slopes\n",
    "#             print(\"generated trend\", trends.shape)\n",
    "        rec = rec + trends\n",
    "#         print(\"rec after trend\", rec.shape)\n",
    "#             if trend_type == \"linear\":\n",
    "#                 trends[ch, idx:] += np.linspace(0, slopes[channel] * self.step * shifted, shifted)\n",
    "#             elif trend_type == \"quadratic\":\n",
    "#                 trends[ch, idx:] += np.linspace(0, slopes[channel] * self.step * shifted, shifted)**2\n",
    "#             elif trend_type == \"mixed\":\n",
    "#                 trends[ch, idx:] += np.linspace(0, slopes[channel] * self.step * shifted, shifted)**((channel%2)+1)\n",
    "\n",
    "        # add it to the channels\n",
    "        return rec\n",
    "            \n",
    "\n",
    "    def forward(self, cb):\n",
    "        mean, logvar, trend_index, trend_slope, seasonality_fpw, seasonality_amp, seasonality_phase = self.extract_params(cb)\n",
    "        rec = self.gen_rec(mean, logvar)\n",
    "        rec = self.add_trend(rec, trend_index, trend_slope)\n",
    "        rec = rec.unsqueeze(0)\n",
    "#         print(\"shape of generated rec after unsqueeze\", rec.shape)\n",
    "        return rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "d3c2fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "class slidingWindow(Dataset):\n",
    "    def __init__(self, data, L):\n",
    "        self.data = data\n",
    "        self.L = L\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.data.shape[1] - index >= self.L:            \n",
    "            x = self.data[:,index:index+self.L]    \n",
    "            v = torch.sum(x/self.L, axis=1).unsqueeze(1)\n",
    "#             print(x.shape)\n",
    "#             print(v.unsqueeze(1).shape)\n",
    "            x = x#/v\n",
    "            return (x, v)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[1] - self.L\n",
    "    \n",
    "### Cost function\n",
    "def criterion(recon_x, x, mu, logvar):\n",
    "    ### reconstruction loss\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "\n",
    "    ### KL divergence loss\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    ### total loss\n",
    "    loss = recon_loss + 1.1*kld_loss\n",
    "    print(recon_loss, kld_loss)\n",
    "    return loss\n",
    "\n",
    "def sample_mean(model, batch, n):\n",
    "    batch_size = batch.shape[0]\n",
    "    n_channels = batch.shape[1]\n",
    "    latent_dims = model.encoder.latent_dims\n",
    "    L = model._L\n",
    "\n",
    "    mu, logvar = (torch.empty((batch_size, n_channels, latent_dims, 0)).to(batch) for _ in range(2))\n",
    "    REC = torch.empty(batch_size, n_channels, L, 0).to(batch)\n",
    "#     print(REC.shape)\n",
    "#     print(mu.shape)\n",
    "\n",
    "    for i in range(n):\n",
    "        rec, _mu, _logvar = model(batch)\n",
    "\n",
    "        REC = torch.cat((REC, rec.unsqueeze(-1)), dim=-1)\n",
    "        mu = torch.cat((mu, _mu.unsqueeze(-1)), dim=-1)\n",
    "        logvar = torch.cat((logvar, _logvar.unsqueeze(-1)), dim=-1)\n",
    "\n",
    "    # print(\"shapes after cat: mu, logvar, REC \", mu.shape, logvar.shape, REC.shape)\n",
    "    mu, logvar = (torch.mean(t, dim=-1) for t in [mu, logvar])\n",
    "    REC = torch.mean(REC, dim=-1)\n",
    "    #     print(\"shapes after mean: mu, logvar, REC \", mu.shape, logvar.shape, REC.shape)\n",
    "\n",
    "    return REC, mu, logvar\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device, epoch, VQ = True):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, v) in enumerate(train_loader):\n",
    "#         print(batch_idx, data.shape)\n",
    "        data = data.to(device)\n",
    "        v = v.to(device)\n",
    "        optimizer.zero_grad()        \n",
    "\n",
    "        if VQ:\n",
    "            x_rec, loss, mu, logvar, z = model(data, v)\n",
    "        else:\n",
    "#             x_rec, mu, logvar = model(data)\n",
    "            x_rec, mu, logvar = sample_mean(model, data, 10)\n",
    "            if v.dim() == 1:\n",
    "                v = v.unsqueeze(-1)\n",
    "                v = v.unsqueeze(-1)\n",
    "            # x_rec_window_length = x_rec.shape[2]\n",
    "            loss = criterion((x_rec), data, mu, logvar)\n",
    "        # print(x_rec.shape)\n",
    "        # print(data[:, :, 0].shape)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "#         print(\"here\")\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "    \n",
    "    return  train_loss #/ len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "4dbf674a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([43.0799], device='cpu')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LongShort_TCVAE_Decoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[363], line 43\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# v = vae(n_channels, L, latent_dims)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m v \u001b[38;5;241m=\u001b[39m VariationalAutoencoder(n_channels,\n\u001b[1;32m     27\u001b[0m                             num_layers \u001b[38;5;241m=\u001b[39m  \u001b[38;5;241m3\u001b[39m,\u001b[38;5;66;03m#4, #3\u001b[39;00m\n\u001b[1;32m     28\u001b[0m                             latent_dims\u001b[38;5;241m=\u001b[39m latent_dims,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m                             reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     38\u001b[0m                             inits\u001b[38;5;241m=\u001b[39mx[:,\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     39\u001b[0m v \u001b[38;5;241m=\u001b[39m VQ_MST_VAE(n_channels \u001b[38;5;241m=\u001b[39m n_channels,\n\u001b[1;32m     40\u001b[0m                             num_layers \u001b[38;5;241m=\u001b[39m  \u001b[38;5;241m3\u001b[39m,\u001b[38;5;66;03m#4, #3\u001b[39;00m\n\u001b[1;32m     41\u001b[0m                             latent_dims\u001b[38;5;241m=\u001b[39m latent_dims,\n\u001b[1;32m     42\u001b[0m                             v_encoder \u001b[38;5;241m=\u001b[39m LongShort_TCVAE_Encoder, \u001b[38;5;66;03m#MST_VAE_Encoder,\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m                             v_decoder \u001b[38;5;241m=\u001b[39m \u001b[43mLongShort_TCVAE_Decoder\u001b[49m, \u001b[38;5;66;03m#LongShort_TCVAE_Decoder, #MST_VAE_Decoder,\u001b[39;00m\n\u001b[1;32m     44\u001b[0m                             v_quantizer \u001b[38;5;241m=\u001b[39m VQ_Quantizer,\n\u001b[1;32m     45\u001b[0m                             L\u001b[38;5;241m=\u001b[39mL,\n\u001b[1;32m     46\u001b[0m                             slope \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     47\u001b[0m                             first_kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60\u001b[39m, \u001b[38;5;66;03m#11, #20\u001b[39;00m\n\u001b[1;32m     48\u001b[0m                             commit_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.25\u001b[39m,\n\u001b[1;32m     49\u001b[0m                             modified\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     50\u001b[0m                             reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     51\u001b[0m                             inits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;66;03m#10 5\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# enc = MST_VAE_Encoder_dist(     n_channels,\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#                            num_layers = 3,\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#                            slope = 0.2,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#                            slope = 0.2,\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#                            first_kernel = 15)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LongShort_TCVAE_Decoder' is not defined"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor(x)\n",
    "n = x.shape[1]\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "train_ = x[:, :int(0.8*n)]\n",
    "val_   = x[:, int(0.8*n):int(0.9*n)]\n",
    "test_  = x[:, int(0.9*n):]\n",
    "\n",
    "# train_data = DataLoader(train_,\n",
    "#                         batch_size= 22,# 59, # 22\n",
    "#                         shuffle = False\n",
    "#                         )\n",
    "# val_data = DataLoader(val_,\n",
    "#                         batch_size=22,\n",
    "#                         shuffle = False\n",
    "#                         )\n",
    "# test_data = DataLoader(test_,\n",
    "#                         batch_size=22,\n",
    "#                         shuffle = False\n",
    "#                         )\n",
    "### Init Model\n",
    "latent_dims = 7 # 6 # 17\n",
    "L= 3455# 39 #32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# v = vae(n_channels, L, latent_dims)\n",
    "v = VariationalAutoencoder(n_channels,\n",
    "                            num_layers =  3,#4, #3\n",
    "                            latent_dims= latent_dims,\n",
    "                            v_encoder = LongShort_TCVAE_Encoder, #MST_VAE_Encoder,\n",
    "                            v_decoder = Gen_Decoder, #LongShort_TCVAE_Decoder, #MST_VAE_Decoder,\n",
    "                            v_quantizer = VQ_Quantizer,\n",
    "                            L=L,\n",
    "                            slope = 0,\n",
    "                            first_kernel = 60, #11, #20\n",
    "                            commit_loss = 0.25,\n",
    "                            modified=False,\n",
    "                            reduction = True,\n",
    "                            inits=x[:,0])\n",
    "v = VQ_MST_VAE(n_channels = n_channels,\n",
    "                            num_layers =  3,#4, #3\n",
    "                            latent_dims= latent_dims,\n",
    "                            v_encoder = LongShort_TCVAE_Encoder, #MST_VAE_Encoder,\n",
    "                            v_decoder = LongShort_TCVAE_Decoder, #LongShort_TCVAE_Decoder, #MST_VAE_Decoder,\n",
    "                            v_quantizer = VQ_Quantizer,\n",
    "                            L=L,\n",
    "                            slope = 0,\n",
    "                            first_kernel = 60, #11, #20\n",
    "                            commit_loss = 0.25,\n",
    "                            modified=False,\n",
    "                            reduction = True,\n",
    "                            inits=None) #10 5\n",
    "# enc = MST_VAE_Encoder_dist(     n_channels,\n",
    "#                            num_layers = 3,\n",
    "#                            slope = 0.2,\n",
    "#                            first_kernel = 15)\n",
    "# dec = MST_VAE_Decoder_dist(     n_channels,\n",
    "#                            num_layers = 3,\n",
    "#                            slope = 0.2,\n",
    "#                            first_kernel = 15)\n",
    "\n",
    "\n",
    "v = v.to(device)\n",
    "# enc = enc.to(device)\n",
    "# dec = dec.to(device)\n",
    "opt = optim.Adam(v.parameters(), lr = 0.001043529186448577) # 0.005043529186448577 0.006819850049647945\n",
    "# print(v)\n",
    "# Create the dictionaries of probability distributions\n",
    "# pnet = {'z': D.Normal(loc=torch.zeros(latent_dims), scale=torch.ones(latent_dims)), 'x': dec}\n",
    "# qnet = {'z': enc}\n",
    "# serie = torch.tensor(serie).float()\n",
    "train_ = x[:, :int(0.8*n)]\n",
    "val_   = x[:, int(0.8*n):int(0.9*n)]\n",
    "test_  = x[:, int(0.9*n):]\n",
    "\n",
    "train_data = DataLoader(slidingWindow(train_, L),\n",
    "                        batch_size= 22,# 59, # 22\n",
    "                        shuffle = False\n",
    "                        )\n",
    "val_data = DataLoader(slidingWindow(val_, L),\n",
    "                        batch_size=22,\n",
    "                        shuffle = False\n",
    "                        )\n",
    "test_data = DataLoader(slidingWindow(test_, L),\n",
    "                        batch_size=22,\n",
    "                        shuffle = False\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "3754910c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3455\n",
      "tensor([0.5715], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3242], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.2168], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7625], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1209], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9852], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8018], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5953], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3035], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0499], grad_fn=<SliceBackward0>)\n",
      "tensor(7.5439e+09, grad_fn=<MseLossBackward0>) tensor(1.2639, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 1 [0/1 (0%)]\tLoss: 7543931392.000000\n",
      "====> Epoch: 1 Average loss: 7543931392.0000\n",
      "3455\n",
      "tensor([-0.5563], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6963], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-4.0620], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-3.3613], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1711], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.8011], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0810], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-4.2090], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0502], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9840], grad_fn=<SliceBackward0>)\n",
      "tensor(1.9357e+11, grad_fn=<MseLossBackward0>) tensor(20.8509, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 2 [0/1 (0%)]\tLoss: 193568063488.000000\n",
      "====> Epoch: 2 Average loss: 193568063488.0000\n",
      "3455\n",
      "tensor([0.3897], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3607], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1502], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2719], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0258], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0039], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8702], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0144], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1105], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1626], grad_fn=<SliceBackward0>)\n",
      "tensor(3.0371e+10, grad_fn=<MseLossBackward0>) tensor(18.3048, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 3 [0/1 (0%)]\tLoss: 30370578432.000000\n",
      "====> Epoch: 3 Average loss: 30370578432.0000\n",
      "3455\n",
      "tensor([1.4567], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.4064], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.3215], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0668], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9950], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5034], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0555], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.1847], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0058], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.5239], grad_fn=<SliceBackward0>)\n",
      "tensor(1.1993e+10, grad_fn=<MseLossBackward0>) tensor(29.1897, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 4 [0/1 (0%)]\tLoss: 11992656896.000000\n",
      "====> Epoch: 4 Average loss: 11992656896.0000\n",
      "3455\n",
      "tensor([1.0189], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8278], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.4944], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7497], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2591], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.5880], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2737], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6267], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.3273], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.1489], grad_fn=<SliceBackward0>)\n",
      "tensor(7.8951e+10, grad_fn=<MseLossBackward0>) tensor(43.9834, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 5 [0/1 (0%)]\tLoss: 78951022592.000000\n",
      "====> Epoch: 5 Average loss: 78951022592.0000\n",
      "3455\n",
      "tensor([0.9365], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2729], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9584], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.3215], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0021], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7968], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2793], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.8063], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0227], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.3079], grad_fn=<SliceBackward0>)\n",
      "tensor(8.3748e+10, grad_fn=<MseLossBackward0>) tensor(53.5918, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 6 [0/1 (0%)]\tLoss: 83748347904.000000\n",
      "====> Epoch: 6 Average loss: 83748347904.0000\n",
      "3455\n",
      "tensor([0.3941], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.7430], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8212], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.4029], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6676], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2921], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1096], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8896], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1946], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3648], grad_fn=<SliceBackward0>)\n",
      "tensor(3.8686e+10, grad_fn=<MseLossBackward0>) tensor(59.4386, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 7 [0/1 (0%)]\tLoss: 38685609984.000000\n",
      "====> Epoch: 7 Average loss: 38685609984.0000\n",
      "3455\n",
      "tensor([-0.2831], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2444], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1151], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1118], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0610], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5079], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2583], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6027], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7324], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1411], grad_fn=<SliceBackward0>)\n",
      "tensor(3.1217e+09, grad_fn=<MseLossBackward0>) tensor(65.1958, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 8 [0/1 (0%)]\tLoss: 3121656832.000000\n",
      "====> Epoch: 8 Average loss: 3121656832.0000\n",
      "3455\n",
      "tensor([-0.3415], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1883], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3529], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5308], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8889], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4244], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6267], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0136], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0481], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1263], grad_fn=<SliceBackward0>)\n",
      "tensor(6.8802e+09, grad_fn=<MseLossBackward0>) tensor(71.7613, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 9 [0/1 (0%)]\tLoss: 6880240640.000000\n",
      "====> Epoch: 9 Average loss: 6880240640.0000\n",
      "3455\n",
      "tensor([-2.7023], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7390], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5880], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5094], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.5759], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.3074], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.5859], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8039], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2762], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6276], grad_fn=<SliceBackward0>)\n",
      "tensor(3.2023e+10, grad_fn=<MseLossBackward0>) tensor(77.8413, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 10 [0/1 (0%)]\tLoss: 32022511616.000000\n",
      "====> Epoch: 10 Average loss: 32022511616.0000\n",
      "3455\n",
      "tensor([2.2462], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.6690], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.2842], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8621], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1191], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.7933], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.6750], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8058], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.7709], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5321], grad_fn=<SliceBackward0>)\n",
      "tensor(4.2929e+10, grad_fn=<MseLossBackward0>) tensor(82.6438, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 11 [0/1 (0%)]\tLoss: 42929127424.000000\n",
      "====> Epoch: 11 Average loss: 42929127424.0000\n",
      "3455\n",
      "tensor([-1.5337], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.8642], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1369], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8392], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2542], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0975], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1897], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.8597], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.1977], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.3597], grad_fn=<SliceBackward0>)\n",
      "tensor(3.0047e+10, grad_fn=<MseLossBackward0>) tensor(85.3511, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 12 [0/1 (0%)]\tLoss: 30046535680.000000\n",
      "====> Epoch: 12 Average loss: 30046535680.0000\n",
      "3455\n",
      "tensor([0.6429], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4647], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1421], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4896], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9586], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3455\n",
      "tensor([1.1125], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2690], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.5130], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.5407], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8597], grad_fn=<SliceBackward0>)\n",
      "tensor(9.1361e+09, grad_fn=<MseLossBackward0>) tensor(86.3592, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 13 [0/1 (0%)]\tLoss: 9136136192.000000\n",
      "====> Epoch: 13 Average loss: 9136136192.0000\n",
      "3455\n",
      "tensor([-0.0444], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0840], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0655], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4405], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2429], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0232], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0953], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1285], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.7483], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7183], grad_fn=<SliceBackward0>)\n",
      "tensor(421911.3438, grad_fn=<MseLossBackward0>) tensor(86.2345, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 14 [0/1 (0%)]\tLoss: 422006.187500\n",
      "====> Epoch: 14 Average loss: 422006.1875\n",
      "3455\n",
      "tensor([1.0624], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.1611], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4354], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1409], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1372], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5370], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2228], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2458], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.7066], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3532], grad_fn=<SliceBackward0>)\n",
      "tensor(7.4700e+09, grad_fn=<MseLossBackward0>) tensor(86.0005, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 15 [0/1 (0%)]\tLoss: 7470007808.000000\n",
      "====> Epoch: 15 Average loss: 7470007808.0000\n",
      "3455\n",
      "tensor([2.4595], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1189], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3082], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8505], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8001], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3139], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7688], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.8313], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5168], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4414], grad_fn=<SliceBackward0>)\n",
      "tensor(1.9430e+10, grad_fn=<MseLossBackward0>) tensor(87.1522, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 16 [0/1 (0%)]\tLoss: 19429539840.000000\n",
      "====> Epoch: 16 Average loss: 19429539840.0000\n",
      "3455\n",
      "tensor([0.8404], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.0337], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4295], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.8573], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5219], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.2711], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2019], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.7088], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0529], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.9883], grad_fn=<SliceBackward0>)\n",
      "tensor(2.1653e+10, grad_fn=<MseLossBackward0>) tensor(88.7899, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 17 [0/1 (0%)]\tLoss: 21652664320.000000\n",
      "====> Epoch: 17 Average loss: 21652664320.0000\n",
      "3455\n",
      "tensor([0.5988], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5613], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7878], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0518], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0719], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5753], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2829], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1861], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0630], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9135], grad_fn=<SliceBackward0>)\n",
      "tensor(1.3312e+10, grad_fn=<MseLossBackward0>) tensor(92.2302, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 18 [0/1 (0%)]\tLoss: 13311908864.000000\n",
      "====> Epoch: 18 Average loss: 13311908864.0000\n",
      "3455\n",
      "tensor([0.2311], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1310], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5233], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5614], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4741], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2323], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5843], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5927], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1495], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2275], grad_fn=<SliceBackward0>)\n",
      "tensor(3.3697e+09, grad_fn=<MseLossBackward0>) tensor(95.6925, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 19 [0/1 (0%)]\tLoss: 3369737728.000000\n",
      "====> Epoch: 19 Average loss: 3369737728.0000\n",
      "3455\n",
      "tensor([-0.2527], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3655], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5951], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9517], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1826], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0137], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0647], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7518], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0282], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8594], grad_fn=<SliceBackward0>)\n",
      "tensor(69035888., grad_fn=<MseLossBackward0>) tensor(99.3913, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 20 [0/1 (0%)]\tLoss: 69036000.000000\n",
      "====> Epoch: 20 Average loss: 69036000.0000\n",
      "3455\n",
      "tensor([-0.2218], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1611], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5101], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1471], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.3120], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.6537], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0931], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1076], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.7856], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1888], grad_fn=<SliceBackward0>)\n",
      "tensor(4.2802e+09, grad_fn=<MseLossBackward0>) tensor(103.0120, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 21 [0/1 (0%)]\tLoss: 4280181248.000000\n",
      "====> Epoch: 21 Average loss: 4280181248.0000\n",
      "3455\n",
      "tensor([-2.0218], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0561], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.9195], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4369], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6475], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.6427], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.1009], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.0324], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9622], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2976], grad_fn=<SliceBackward0>)\n",
      "tensor(9.7028e+09, grad_fn=<MseLossBackward0>) tensor(107.2080, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 22 [0/1 (0%)]\tLoss: 9702756352.000000\n",
      "====> Epoch: 22 Average loss: 9702756352.0000\n",
      "3455\n",
      "tensor([-0.5113], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.6859], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6274], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9279], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3330], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6389], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1916], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8317], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3882], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4664], grad_fn=<SliceBackward0>)\n",
      "tensor(1.0130e+10, grad_fn=<MseLossBackward0>) tensor(112.9746, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 23 [0/1 (0%)]\tLoss: 10129950720.000000\n",
      "====> Epoch: 23 Average loss: 10129950720.0000\n",
      "3455\n",
      "tensor([-0.1424], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4908], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2930], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.9660], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2264], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9339], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0068], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5087], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0207], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6266], grad_fn=<SliceBackward0>)\n",
      "tensor(5.7446e+09, grad_fn=<MseLossBackward0>) tensor(119.3682, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 24 [0/1 (0%)]\tLoss: 5744648704.000000\n",
      "====> Epoch: 24 Average loss: 5744648704.0000\n",
      "3455\n",
      "tensor([0.0551], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9290], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1648], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7684], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5030], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4115], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.0485], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9131], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8218], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6601], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2632e+09, grad_fn=<MseLossBackward0>) tensor(125.3633, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 25 [0/1 (0%)]\tLoss: 1263197312.000000\n",
      "====> Epoch: 25 Average loss: 1263197312.0000\n",
      "3455\n",
      "tensor([1.6805], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.4459], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4129], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2377], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4540], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4542], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3368], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1515], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4007], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3964], grad_fn=<SliceBackward0>)\n",
      "tensor(90061664., grad_fn=<MseLossBackward0>) tensor(130.6273, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 26 [0/1 (0%)]\tLoss: 90061808.000000\n",
      "====> Epoch: 26 Average loss: 90061808.0000\n",
      "3455\n",
      "tensor([0.2574], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.8393], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2998], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0565], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1806], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5154], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5638], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7199], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1108], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6402], grad_fn=<SliceBackward0>)\n",
      "tensor(2.1846e+09, grad_fn=<MseLossBackward0>) tensor(134.9603, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 27 [0/1 (0%)]\tLoss: 2184647168.000000\n",
      "====> Epoch: 27 Average loss: 2184647168.0000\n",
      "3455\n",
      "tensor([0.8895], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1507], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4816], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7680], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.8994], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1486], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1243], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0860], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4380], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5894], grad_fn=<SliceBackward0>)\n",
      "tensor(4.4857e+09, grad_fn=<MseLossBackward0>) tensor(138.6646, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 28 [0/1 (0%)]\tLoss: 4485709824.000000\n",
      "====> Epoch: 28 Average loss: 4485709824.0000\n",
      "3455\n",
      "tensor([0.5825], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.5512], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.3088], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4553], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.8864], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1685], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2229], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8986], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4900], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4665], grad_fn=<SliceBackward0>)\n",
      "tensor(4.4648e+09, grad_fn=<MseLossBackward0>) tensor(141.9582, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 29 [0/1 (0%)]\tLoss: 4464784384.000000\n",
      "====> Epoch: 29 Average loss: 4464784384.0000\n",
      "3455\n",
      "tensor([-1.3649], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2677], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2031], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6789], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4300], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0080], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2464], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6193], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5045], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7831], grad_fn=<SliceBackward0>)\n",
      "tensor(2.3481e+09, grad_fn=<MseLossBackward0>) tensor(144.9383, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 30 [0/1 (0%)]\tLoss: 2348098816.000000\n",
      "====> Epoch: 30 Average loss: 2348098816.0000\n",
      "3455\n",
      "tensor([-0.0368], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.8632], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8676], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4652], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5000], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.7488], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3931], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7314], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.2968], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7460], grad_fn=<SliceBackward0>)\n",
      "tensor(3.4382e+08, grad_fn=<MseLossBackward0>) tensor(147.5941, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 31 [0/1 (0%)]\tLoss: 343821024.000000\n",
      "====> Epoch: 31 Average loss: 343821024.0000\n",
      "3455\n",
      "tensor([0.3513], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8272], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3100], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.5986], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1939], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2888], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4596], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0657], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7714], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1153], grad_fn=<SliceBackward0>)\n",
      "tensor(1.8055e+08, grad_fn=<MseLossBackward0>) tensor(149.8237, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 32 [0/1 (0%)]\tLoss: 180554432.000000\n",
      "====> Epoch: 32 Average loss: 180554432.0000\n",
      "3455\n",
      "tensor([-0.5285], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6174], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4806], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2538], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3489], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4210], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7079], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0135], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1167], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8689], grad_fn=<SliceBackward0>)\n",
      "tensor(1.4953e+09, grad_fn=<MseLossBackward0>) tensor(151.6091, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 33 [0/1 (0%)]\tLoss: 1495250304.000000\n",
      "====> Epoch: 33 Average loss: 1495250304.0000\n",
      "3455\n",
      "tensor([-0.0751], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0123], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1053], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2124], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.7353], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2397], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0663], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6094], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.9248], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1367], grad_fn=<SliceBackward0>)\n",
      "tensor(2.5277e+09, grad_fn=<MseLossBackward0>) tensor(152.9637, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 34 [0/1 (0%)]\tLoss: 2527657728.000000\n",
      "====> Epoch: 34 Average loss: 2527657728.0000\n",
      "3455\n",
      "tensor([-0.3342], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3352], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.7396], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7870], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9246], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6381], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0252], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1303], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4535], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.7048], grad_fn=<SliceBackward0>)\n",
      "tensor(2.1648e+09, grad_fn=<MseLossBackward0>) tensor(154.0053, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 35 [0/1 (0%)]\tLoss: 2164840192.000000\n",
      "====> Epoch: 35 Average loss: 2164840192.0000\n",
      "3455\n",
      "tensor([-0.0982], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5544], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0343], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1084], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2388], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2132], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4773], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9321], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.8637], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7101], grad_fn=<SliceBackward0>)\n",
      "tensor(8.5666e+08, grad_fn=<MseLossBackward0>) tensor(154.8623, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 36 [0/1 (0%)]\tLoss: 856663232.000000\n",
      "====> Epoch: 36 Average loss: 856663232.0000\n",
      "3455\n",
      "tensor([0.6930], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2655], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1287], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.5789], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7974], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9752], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.2984], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.0640], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1412], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3868], grad_fn=<SliceBackward0>)\n",
      "tensor(24887660., grad_fn=<MseLossBackward0>) tensor(155.6039, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 37 [0/1 (0%)]\tLoss: 24887832.000000\n",
      "====> Epoch: 37 Average loss: 24887832.0000\n",
      "3455\n",
      "tensor([-1.8814], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.3267], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6863], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6957], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0213], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.4786], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.9154], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0100], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3455\n",
      "tensor([-0.0687], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2847], grad_fn=<SliceBackward0>)\n",
      "tensor(3.5263e+08, grad_fn=<MseLossBackward0>) tensor(156.2790, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 38 [0/1 (0%)]\tLoss: 352626464.000000\n",
      "====> Epoch: 38 Average loss: 352626464.0000\n",
      "3455\n",
      "tensor([1.1794], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5088], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.9473], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.6119], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2172], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1327], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.7207], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8983], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2300], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4167], grad_fn=<SliceBackward0>)\n",
      "tensor(1.2178e+09, grad_fn=<MseLossBackward0>) tensor(156.8783, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 39 [0/1 (0%)]\tLoss: 1217790080.000000\n",
      "====> Epoch: 39 Average loss: 1217790080.0000\n",
      "3455\n",
      "tensor([0.9252], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5339], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2935], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3399], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2016], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7645], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2408], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0421], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3138], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3779], grad_fn=<SliceBackward0>)\n",
      "tensor(1.5231e+09, grad_fn=<MseLossBackward0>) tensor(157.3787, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 40 [0/1 (0%)]\tLoss: 1523100288.000000\n",
      "====> Epoch: 40 Average loss: 1523100288.0000\n",
      "3455\n",
      "tensor([0.8892], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8937], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.5162], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1624], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5260], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1131], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1600], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7130], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1725], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7830], grad_fn=<SliceBackward0>)\n",
      "tensor(9.5173e+08, grad_fn=<MseLossBackward0>) tensor(157.7802, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 41 [0/1 (0%)]\tLoss: 951728576.000000\n",
      "====> Epoch: 41 Average loss: 951728576.0000\n",
      "3455\n",
      "tensor([-1.7330], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3687], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4256], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1342], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6362], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2831], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1013], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0919], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8545], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0887], grad_fn=<SliceBackward0>)\n",
      "tensor(1.8972e+08, grad_fn=<MseLossBackward0>) tensor(158.1293, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 42 [0/1 (0%)]\tLoss: 189720512.000000\n",
      "====> Epoch: 42 Average loss: 189720512.0000\n",
      "3455\n",
      "tensor([-0.3775], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4106], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1005], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9068], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.9732], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9886], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0792], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7954], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0156], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4614], grad_fn=<SliceBackward0>)\n",
      "tensor(37574668., grad_fn=<MseLossBackward0>) tensor(158.4649, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 43 [0/1 (0%)]\tLoss: 37574844.000000\n",
      "====> Epoch: 43 Average loss: 37574844.0000\n",
      "3455\n",
      "tensor([-1.0429], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2849], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7667], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0048], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1064], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5079], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.9797], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3466], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9641], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0839], grad_fn=<SliceBackward0>)\n",
      "tensor(4.9710e+08, grad_fn=<MseLossBackward0>) tensor(158.8000, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 44 [0/1 (0%)]\tLoss: 497098848.000000\n",
      "====> Epoch: 44 Average loss: 497098848.0000\n",
      "3455\n",
      "tensor([-0.6625], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9111], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1423], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7199], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6891], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3398], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4663], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4305], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.1793], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0135], grad_fn=<SliceBackward0>)\n",
      "tensor(8.8743e+08, grad_fn=<MseLossBackward0>) tensor(159.0976, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 45 [0/1 (0%)]\tLoss: 887435072.000000\n",
      "====> Epoch: 45 Average loss: 887435072.0000\n",
      "3455\n",
      "tensor([-0.4279], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2986], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4329], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9792], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4098], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9053], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4272], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1020], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4427], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3034], grad_fn=<SliceBackward0>)\n",
      "tensor(7.2546e+08, grad_fn=<MseLossBackward0>) tensor(159.3203, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 46 [0/1 (0%)]\tLoss: 725462528.000000\n",
      "====> Epoch: 46 Average loss: 725462528.0000\n",
      "3455\n",
      "tensor([-1.1013], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0186], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7878], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.0798], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7381], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8006], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0802], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0879], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0936], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0852], grad_fn=<SliceBackward0>)\n",
      "tensor(2.3968e+08, grad_fn=<MseLossBackward0>) tensor(159.4759, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 47 [0/1 (0%)]\tLoss: 239681728.000000\n",
      "====> Epoch: 47 Average loss: 239681728.0000\n",
      "3455\n",
      "tensor([-0.1762], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.9788], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8708], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4361], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.9689], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9296], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7271], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0661], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6598], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7490], grad_fn=<SliceBackward0>)\n",
      "tensor(158070.1875, grad_fn=<MseLossBackward0>) tensor(159.6123, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 48 [0/1 (0%)]\tLoss: 158245.765625\n",
      "====> Epoch: 48 Average loss: 158245.7656\n",
      "3455\n",
      "tensor([-0.6056], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9882], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6198], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0579], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8989], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3336], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7359], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5804], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8526], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9566], grad_fn=<SliceBackward0>)\n",
      "tensor(2.0159e+08, grad_fn=<MseLossBackward0>) tensor(159.7533, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 49 [0/1 (0%)]\tLoss: 201589488.000000\n",
      "====> Epoch: 49 Average loss: 201589488.0000\n",
      "3455\n",
      "tensor([-0.5224], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2554], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1978], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3128], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8095], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2808], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7339], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4267], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3612], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1135], grad_fn=<SliceBackward0>)\n",
      "tensor(4.9269e+08, grad_fn=<MseLossBackward0>) tensor(159.8835, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 50 [0/1 (0%)]\tLoss: 492685184.000000\n",
      "====> Epoch: 50 Average loss: 492685184.0000\n",
      "3455\n",
      "tensor([-1.1231], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.2293], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2666], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0588], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5660], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.9242], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2552], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.3928], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4010], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5174], grad_fn=<SliceBackward0>)\n",
      "tensor(4.7807e+08, grad_fn=<MseLossBackward0>) tensor(159.9801, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 51 [0/1 (0%)]\tLoss: 478069216.000000\n",
      "====> Epoch: 51 Average loss: 478069216.0000\n",
      "3455\n",
      "tensor([1.3692], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.6676], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4633], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.5304], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1342], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1300], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7412], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6201], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4966], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1757], grad_fn=<SliceBackward0>)\n",
      "tensor(1.9567e+08, grad_fn=<MseLossBackward0>) tensor(160.0447, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 52 [0/1 (0%)]\tLoss: 195669200.000000\n",
      "====> Epoch: 52 Average loss: 195669200.0000\n",
      "3455\n",
      "tensor([-1.4532], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0387], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0669], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1105], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3393], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0990], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8331], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9357], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6860], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7686], grad_fn=<SliceBackward0>)\n",
      "tensor(3934666., grad_fn=<MseLossBackward0>) tensor(160.0984, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 53 [0/1 (0%)]\tLoss: 3934842.000000\n",
      "====> Epoch: 53 Average loss: 3934842.0000\n",
      "3455\n",
      "tensor([-0.2286], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1238], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8123], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5877], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8524], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3095], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1641], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7551], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5399], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3844], grad_fn=<SliceBackward0>)\n",
      "tensor(93751856., grad_fn=<MseLossBackward0>) tensor(160.1497, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 54 [0/1 (0%)]\tLoss: 93752032.000000\n",
      "====> Epoch: 54 Average loss: 93752032.0000\n",
      "3455\n",
      "tensor([0.8126], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6339], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.2366], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7719], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8452], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4819], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7626], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2556], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4410], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0518], grad_fn=<SliceBackward0>)\n",
      "tensor(2.8238e+08, grad_fn=<MseLossBackward0>) tensor(160.1861, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 55 [0/1 (0%)]\tLoss: 282381568.000000\n",
      "====> Epoch: 55 Average loss: 282381568.0000\n",
      "3455\n",
      "tensor([-1.1560], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8321], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.5019], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4019], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.4282], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.6055], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0644], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0237], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4878], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.5823], grad_fn=<SliceBackward0>)\n",
      "tensor(2.9710e+08, grad_fn=<MseLossBackward0>) tensor(160.1966, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 56 [0/1 (0%)]\tLoss: 297103040.000000\n",
      "====> Epoch: 56 Average loss: 297103040.0000\n",
      "3455\n",
      "tensor([0.3723], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3806], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3233], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2861], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6320], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4701], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5694], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.2427], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.0822], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8738], grad_fn=<SliceBackward0>)\n",
      "tensor(1.2977e+08, grad_fn=<MseLossBackward0>) tensor(160.1858, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 57 [0/1 (0%)]\tLoss: 129767952.000000\n",
      "====> Epoch: 57 Average loss: 129767952.0000\n",
      "3455\n",
      "tensor([-0.5239], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.2121], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8870], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8997], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1496], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.5499], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6253], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.2353], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9098], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4063], grad_fn=<SliceBackward0>)\n",
      "tensor(3744435., grad_fn=<MseLossBackward0>) tensor(160.1649, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 58 [0/1 (0%)]\tLoss: 3744611.250000\n",
      "====> Epoch: 58 Average loss: 3744611.2500\n",
      "3455\n",
      "tensor([-1.2842], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9879], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5140], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1218], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2061], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6652], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3926], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8262], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0351], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3751], grad_fn=<SliceBackward0>)\n",
      "tensor(55418600., grad_fn=<MseLossBackward0>) tensor(160.1382, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 59 [0/1 (0%)]\tLoss: 55418776.000000\n",
      "====> Epoch: 59 Average loss: 55418776.0000\n",
      "3455\n",
      "tensor([-0.2996], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7709], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0071], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4856], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.8137], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5301], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7588], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2918], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4404], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6377], grad_fn=<SliceBackward0>)\n",
      "tensor(1.7233e+08, grad_fn=<MseLossBackward0>) tensor(160.1049, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 60 [0/1 (0%)]\tLoss: 172328480.000000\n",
      "====> Epoch: 60 Average loss: 172328480.0000\n",
      "3455\n",
      "tensor([1.2216], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3125], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3472], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3995], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.9001], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4708], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4131], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1178], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8629], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6223], grad_fn=<SliceBackward0>)\n",
      "tensor(1.7787e+08, grad_fn=<MseLossBackward0>) tensor(160.0633, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 61 [0/1 (0%)]\tLoss: 177871520.000000\n",
      "====> Epoch: 61 Average loss: 177871520.0000\n",
      "3455\n",
      "tensor([0.7012], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6445], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2655], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0253], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3029], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4295], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6966], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1039], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6604], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3936], grad_fn=<SliceBackward0>)\n",
      "tensor(72552480., grad_fn=<MseLossBackward0>) tensor(160.0171, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 62 [0/1 (0%)]\tLoss: 72552656.000000\n",
      "====> Epoch: 62 Average loss: 72552656.0000\n",
      "3455\n",
      "tensor([0.4719], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4197], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3921], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.4227], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9515], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2995], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6010], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4209], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3455\n",
      "tensor([-0.6525], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3772], grad_fn=<SliceBackward0>)\n",
      "tensor(899272.8125, grad_fn=<MseLossBackward0>) tensor(159.9703, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 63 [0/1 (0%)]\tLoss: 899448.750000\n",
      "====> Epoch: 63 Average loss: 899448.7500\n",
      "3455\n",
      "tensor([1.8677], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6648], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2988], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4162], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9607], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8806], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1213], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9056], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7610], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9732], grad_fn=<SliceBackward0>)\n",
      "tensor(40930744., grad_fn=<MseLossBackward0>) tensor(159.9210, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 64 [0/1 (0%)]\tLoss: 40930920.000000\n",
      "====> Epoch: 64 Average loss: 40930920.0000\n",
      "3455\n",
      "tensor([0.2711], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1270], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.8624], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6064], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7640], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8292], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.6467], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.5232], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3666], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3465], grad_fn=<SliceBackward0>)\n",
      "tensor(1.1016e+08, grad_fn=<MseLossBackward0>) tensor(159.8619, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 65 [0/1 (0%)]\tLoss: 110155248.000000\n",
      "====> Epoch: 65 Average loss: 110155248.0000\n",
      "3455\n",
      "tensor([1.3985], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1570], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0651], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.5177], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0226], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1703], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1895], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.4225], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.3255], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3532], grad_fn=<SliceBackward0>)\n",
      "tensor(1.0267e+08, grad_fn=<MseLossBackward0>) tensor(159.7884, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 66 [0/1 (0%)]\tLoss: 102665296.000000\n",
      "====> Epoch: 66 Average loss: 102665296.0000\n",
      "3455\n",
      "tensor([-0.7825], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7837], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8769], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1670], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6355], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-3.3703], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1320], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.6025], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8974], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7277], grad_fn=<SliceBackward0>)\n",
      "tensor(33767404., grad_fn=<MseLossBackward0>) tensor(159.7052, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 67 [0/1 (0%)]\tLoss: 33767580.000000\n",
      "====> Epoch: 67 Average loss: 33767580.0000\n",
      "3455\n",
      "tensor([-0.3800], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2046], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0622], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3020], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9351], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0409], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1322], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1944], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2527], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0449], grad_fn=<SliceBackward0>)\n",
      "tensor(328457.8750, grad_fn=<MseLossBackward0>) tensor(159.6194, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 68 [0/1 (0%)]\tLoss: 328633.468750\n",
      "====> Epoch: 68 Average loss: 328633.4688\n",
      "3455\n",
      "tensor([0.8866], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.6814], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5401], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.6899], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5656], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4391], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.6974], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.8126], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2519], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3346], grad_fn=<SliceBackward0>)\n",
      "tensor(34845768., grad_fn=<MseLossBackward0>) tensor(159.5324, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 69 [0/1 (0%)]\tLoss: 34845944.000000\n",
      "====> Epoch: 69 Average loss: 34845944.0000\n",
      "3455\n",
      "tensor([0.9529], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2544], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9980], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8310], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.5028], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8041], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6317], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0186], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0963], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4222], grad_fn=<SliceBackward0>)\n",
      "tensor(72216232., grad_fn=<MseLossBackward0>) tensor(159.4422, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 70 [0/1 (0%)]\tLoss: 72216408.000000\n",
      "====> Epoch: 70 Average loss: 72216408.0000\n",
      "3455\n",
      "tensor([-0.0288], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0194], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9282], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4110], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0644], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6263], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0236], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9215], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.4041], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.1575], grad_fn=<SliceBackward0>)\n",
      "tensor(54767656., grad_fn=<MseLossBackward0>) tensor(159.3495, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 71 [0/1 (0%)]\tLoss: 54767832.000000\n",
      "====> Epoch: 71 Average loss: 54767832.0000\n",
      "3455\n",
      "tensor([0.1214], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7520], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2566], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7251], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1732], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7583], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0692], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1186], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2115], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1151], grad_fn=<SliceBackward0>)\n",
      "tensor(11515222., grad_fn=<MseLossBackward0>) tensor(159.2573, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 72 [0/1 (0%)]\tLoss: 11515397.000000\n",
      "====> Epoch: 72 Average loss: 11515397.0000\n",
      "3455\n",
      "tensor([-0.3836], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.2031], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4053], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5782], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8772], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.9160], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5014], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1405], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0240], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6454], grad_fn=<SliceBackward0>)\n",
      "tensor(2838526.5000, grad_fn=<MseLossBackward0>) tensor(159.1647, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 73 [0/1 (0%)]\tLoss: 2838701.500000\n",
      "====> Epoch: 73 Average loss: 2838701.5000\n",
      "3455\n",
      "tensor([-0.9483], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.9584], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3179], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4079], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6248], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1844], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1408], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8883], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.0092], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0663], grad_fn=<SliceBackward0>)\n",
      "tensor(30937144., grad_fn=<MseLossBackward0>) tensor(159.0686, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 74 [0/1 (0%)]\tLoss: 30937318.000000\n",
      "====> Epoch: 74 Average loss: 30937318.0000\n",
      "3455\n",
      "tensor([0.1946], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0432], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3623], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3529], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7140], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2207], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4765], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7064], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0170], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1568], grad_fn=<SliceBackward0>)\n",
      "tensor(45474196., grad_fn=<MseLossBackward0>) tensor(158.9664, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 75 [0/1 (0%)]\tLoss: 45474372.000000\n",
      "====> Epoch: 75 Average loss: 45474372.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3455\n",
      "tensor([1.5969], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3203], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3689], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5796], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0387], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3371], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4387], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1125], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5244], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1843], grad_fn=<SliceBackward0>)\n",
      "tensor(24987554., grad_fn=<MseLossBackward0>) tensor(158.8591, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 76 [0/1 (0%)]\tLoss: 24987728.000000\n",
      "====> Epoch: 76 Average loss: 24987728.0000\n",
      "3455\n",
      "tensor([-1.4472], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4003], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3350], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2706], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6009], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1534], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0609], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2822], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2043], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0203], grad_fn=<SliceBackward0>)\n",
      "tensor(1800198.1250, grad_fn=<MseLossBackward0>) tensor(158.7485, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 77 [0/1 (0%)]\tLoss: 1800372.750000\n",
      "====> Epoch: 77 Average loss: 1800372.7500\n",
      "3455\n",
      "tensor([-0.4619], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8103], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5758], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.2312], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7315], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9851], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7872], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9573], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4238], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6559], grad_fn=<SliceBackward0>)\n",
      "tensor(6934224., grad_fn=<MseLossBackward0>) tensor(158.6362, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 78 [0/1 (0%)]\tLoss: 6934398.500000\n",
      "====> Epoch: 78 Average loss: 6934398.5000\n",
      "3455\n",
      "tensor([1.7033], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.6686], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5103], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5654], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6739], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3082], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8462], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4555], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2926], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0646], grad_fn=<SliceBackward0>)\n",
      "tensor(25347588., grad_fn=<MseLossBackward0>) tensor(158.5236, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 79 [0/1 (0%)]\tLoss: 25347762.000000\n",
      "====> Epoch: 79 Average loss: 25347762.0000\n",
      "3455\n",
      "tensor([-1.1195], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5968], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9763], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8584], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1973], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0133], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8434], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8820], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.5649], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0748], grad_fn=<SliceBackward0>)\n",
      "tensor(25519230., grad_fn=<MseLossBackward0>) tensor(158.4110, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 80 [0/1 (0%)]\tLoss: 25519404.000000\n",
      "====> Epoch: 80 Average loss: 25519404.0000\n",
      "3455\n",
      "tensor([-1.0022], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1453], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3869], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.2714], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0359], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1097], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8830], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9134], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1184], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8345], grad_fn=<SliceBackward0>)\n",
      "tensor(8302839., grad_fn=<MseLossBackward0>) tensor(158.2987, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 81 [0/1 (0%)]\tLoss: 8303013.000000\n",
      "====> Epoch: 81 Average loss: 8303013.0000\n",
      "3455\n",
      "tensor([0.7414], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4377], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3177], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1249], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1899], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4211], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0274], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5343], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4499], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1950], grad_fn=<SliceBackward0>)\n",
      "tensor(267756.3438, grad_fn=<MseLossBackward0>) tensor(158.1864, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 82 [0/1 (0%)]\tLoss: 267930.343750\n",
      "====> Epoch: 82 Average loss: 267930.3438\n",
      "3455\n",
      "tensor([0.8600], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8355], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0839], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1649], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2557], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8823], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5318], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0306], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6791], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7909], grad_fn=<SliceBackward0>)\n",
      "tensor(9937236., grad_fn=<MseLossBackward0>) tensor(158.0729, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 83 [0/1 (0%)]\tLoss: 9937410.000000\n",
      "====> Epoch: 83 Average loss: 9937410.0000\n",
      "3455\n",
      "tensor([1.0613], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1638], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4287], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5978], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0662], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4434], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4874], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.6953], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3827], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.4233], grad_fn=<SliceBackward0>)\n",
      "tensor(18341456., grad_fn=<MseLossBackward0>) tensor(157.9558, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 84 [0/1 (0%)]\tLoss: 18341630.000000\n",
      "====> Epoch: 84 Average loss: 18341630.0000\n",
      "3455\n",
      "tensor([-0.2335], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3528], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4885], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6354], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1915], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1245], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1168], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1814], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.2492], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.0398], grad_fn=<SliceBackward0>)\n",
      "tensor(11508712., grad_fn=<MseLossBackward0>) tensor(157.8348, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 85 [0/1 (0%)]\tLoss: 11508886.000000\n",
      "====> Epoch: 85 Average loss: 11508886.0000\n",
      "3455\n",
      "tensor([-1.0275], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3450], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0683], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1626], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1421], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0485], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4393], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3891], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.9059], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7900], grad_fn=<SliceBackward0>)\n",
      "tensor(1269785.8750, grad_fn=<MseLossBackward0>) tensor(157.7113, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 86 [0/1 (0%)]\tLoss: 1269959.375000\n",
      "====> Epoch: 86 Average loss: 1269959.3750\n",
      "3455\n",
      "tensor([1.3073], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.2398], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0785], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2302], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8825], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0859], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4420], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2243], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4025], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8292], grad_fn=<SliceBackward0>)\n",
      "tensor(2458990.7500, grad_fn=<MseLossBackward0>) tensor(157.5871, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 87 [0/1 (0%)]\tLoss: 2459164.000000\n",
      "====> Epoch: 87 Average loss: 2459164.0000\n",
      "3455\n",
      "tensor([-1.8612], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1688], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4472], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3088], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0520], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.6101], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1021], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1421], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3455\n",
      "tensor([-0.7510], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1644], grad_fn=<SliceBackward0>)\n",
      "tensor(10129392., grad_fn=<MseLossBackward0>) tensor(157.4632, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 88 [0/1 (0%)]\tLoss: 10129565.000000\n",
      "====> Epoch: 88 Average loss: 10129565.0000\n",
      "3455\n",
      "tensor([0.8790], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3908], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1824], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7190], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5163], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4000], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9519], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1916], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8969], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5368], grad_fn=<SliceBackward0>)\n",
      "tensor(10299526., grad_fn=<MseLossBackward0>) tensor(157.3394, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 89 [0/1 (0%)]\tLoss: 10299699.000000\n",
      "====> Epoch: 89 Average loss: 10299699.0000\n",
      "3455\n",
      "tensor([0.4481], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0908], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0326], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5893], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7043], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3081], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3869], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2219], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1613], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1309], grad_fn=<SliceBackward0>)\n",
      "tensor(3309640.5000, grad_fn=<MseLossBackward0>) tensor(157.2160, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 90 [0/1 (0%)]\tLoss: 3309813.500000\n",
      "====> Epoch: 90 Average loss: 3309813.5000\n",
      "3455\n",
      "tensor([-0.4993], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9010], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9720], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5972], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.0091], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6531], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8978], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1183], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1682], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4787], grad_fn=<SliceBackward0>)\n",
      "tensor(256562.0781, grad_fn=<MseLossBackward0>) tensor(157.0925, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 91 [0/1 (0%)]\tLoss: 256734.875000\n",
      "====> Epoch: 91 Average loss: 256734.8750\n",
      "3455\n",
      "tensor([-1.0851], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1902], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0228], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1071], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9861], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.2223], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2068], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0100], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7883], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7387], grad_fn=<SliceBackward0>)\n",
      "tensor(4527659., grad_fn=<MseLossBackward0>) tensor(156.9680, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 92 [0/1 (0%)]\tLoss: 4527831.500000\n",
      "====> Epoch: 92 Average loss: 4527831.5000\n",
      "3455\n",
      "tensor([0.3194], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.7401], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1713], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7057], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1961], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0194], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7098], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7669], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0412], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0489], grad_fn=<SliceBackward0>)\n",
      "tensor(7455806.5000, grad_fn=<MseLossBackward0>) tensor(156.8418, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 93 [0/1 (0%)]\tLoss: 7455979.000000\n",
      "====> Epoch: 93 Average loss: 7455979.0000\n",
      "3455\n",
      "tensor([-0.3393], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6986], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8034], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.4384], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.6925], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0739], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2557], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0949], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7797], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3252], grad_fn=<SliceBackward0>)\n",
      "tensor(4155416.5000, grad_fn=<MseLossBackward0>) tensor(156.7133, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 94 [0/1 (0%)]\tLoss: 4155589.000000\n",
      "====> Epoch: 94 Average loss: 4155589.0000\n",
      "3455\n",
      "tensor([-0.0212], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3937], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1194], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8085], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4758], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3924], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1768], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8056], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4071], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6208], grad_fn=<SliceBackward0>)\n",
      "tensor(336967.2500, grad_fn=<MseLossBackward0>) tensor(156.5831, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 95 [0/1 (0%)]\tLoss: 337139.500000\n",
      "====> Epoch: 95 Average loss: 337139.5000\n",
      "3455\n",
      "tensor([1.4683], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2273], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2331], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3875], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.4649], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0588], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4590], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3714], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3829], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6867], grad_fn=<SliceBackward0>)\n",
      "tensor(1689270.3750, grad_fn=<MseLossBackward0>) tensor(156.4522, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 96 [0/1 (0%)]\tLoss: 1689442.500000\n",
      "====> Epoch: 96 Average loss: 1689442.5000\n",
      "3455\n",
      "tensor([-0.4954], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1243], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6979], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6075], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9503], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1045], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1028], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2414], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3755], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1888], grad_fn=<SliceBackward0>)\n",
      "tensor(4686641., grad_fn=<MseLossBackward0>) tensor(156.3216, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 97 [0/1 (0%)]\tLoss: 4686813.000000\n",
      "====> Epoch: 97 Average loss: 4686813.0000\n",
      "3455\n",
      "tensor([-1.2570], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3076], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.6720], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7084], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4075], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9242], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6875], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0074], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4326], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4091], grad_fn=<SliceBackward0>)\n",
      "tensor(3787304.5000, grad_fn=<MseLossBackward0>) tensor(156.1913, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 98 [0/1 (0%)]\tLoss: 3787476.250000\n",
      "====> Epoch: 98 Average loss: 3787476.2500\n",
      "3455\n",
      "tensor([1.1731], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8060], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0259], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.3778], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6606], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0521], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.6129], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1740], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1568], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4050], grad_fn=<SliceBackward0>)\n",
      "tensor(820139., grad_fn=<MseLossBackward0>) tensor(156.0612, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 99 [0/1 (0%)]\tLoss: 820310.687500\n",
      "====> Epoch: 99 Average loss: 820310.6875\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 100):\n",
    "    train(v, train_data, criterion, opt, device, epoch, VQ = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "ff1e8584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(dataset, model, VQ=True):\n",
    "    model.eval()\n",
    "    rec = []\n",
    "    x = []\n",
    "    T = dataset.dataset.data.shape[1]- L\n",
    "    idx = 0\n",
    "    _mu_rec, _logvar_rec = [], []\n",
    "    _mu, _logvar = [], []\n",
    "#     _mu, _logvar = (torch.empty((n_channels * 2, T, L)) for _ in range(2))\n",
    "#     _mu_rec, _logvar_rec = (torch.empty((T, n_channels)) for _ in range(2))\n",
    "    with torch.no_grad():\n",
    "        for i, (data, v) in enumerate(dataset):\n",
    "            bs = data.shape[0]\n",
    "            if VQ:\n",
    "                x_rec, loss, mu, logvar, z = model(data, v)\n",
    "            else:\n",
    "                x_rec, mu, logvar = model(data)\n",
    "            z = model.reparametrization_trick(mu, logvar)\n",
    "            if v.dim() == 1:\n",
    "                v = v.unsqueeze(0)\n",
    "                v = v.T\n",
    "                v = v.unsqueeze(-1)\n",
    "#             print(v.shape)\n",
    "#             print(x_rec.shape)\n",
    "#             print((x_rec * v).shape)\n",
    "#             print(i)\n",
    "#             print(mu.shape)\n",
    "\n",
    "            x.extend((data)[:,:,:].detach().numpy())\n",
    "            rec.extend(((x_rec)[:,:,:]).detach().numpy())\n",
    "#             _mu_rec.extend((mu_rec*v)[:,:,0].detach().numpy())\n",
    "#             _logvar_rec.extend(((torch.exp(0.5 * logvar_rec))*v)[:,:,0].detach().numpy())\n",
    "            \n",
    "#             _mu.extend((mu*v)[:,:,0].detach().numpy())\n",
    "#             _logvar.extend(((torch.exp(0.5 * logvar))*v)[:,:,0].detach().numpy())\n",
    "#             _mu[idx: idx + bs, :, :] = mu\n",
    "#             _mu_rec[idx: idx + bs, :] = (mu_rec*v)[:,:,0]\n",
    "#     print(np.array(rec).shape)\n",
    "#     _mu = np.array(_mu)\n",
    "#     _logvar = np.array(_logvar)\n",
    "    \n",
    "    rec = np.array(rec).squeeze(0).T\n",
    "    x = np.array(x).squeeze(0).T\n",
    "    print(x.shape)\n",
    "    print(rec)\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(10, 4))\n",
    "    ax = axs#[0]\n",
    "    ax.plot(rec, \"r-\")\n",
    "    ax.plot(x, \"b-\", alpha=0.2)\n",
    "#     ax.plot(_mu_rec)\n",
    "#     ax.plot(params[\"mu\"].T[:_mu.shape[0],:])\n",
    "    ax.grid()\n",
    "    \n",
    "#     ax_std = axs[1]\n",
    "#     ax_std.plot(_logvar_rec, \"r\")\n",
    "#     ax_std.plot(params[\"cov\"][0,:,:T].T, \"b\")\n",
    "#     ax_std.grid()\n",
    "    \n",
    "#     ax_enc = axs[2]\n",
    "#     for (i,l) in enumerate(_mu.T):        \n",
    "#         ax_enc.plot(l.T, alpha = (0.2 + 0.1*i))\n",
    "# #     ax_enc.plot(_mu, alpha = 0.2)\n",
    "#     ax_enc.grid()\n",
    "    \n",
    "#     ax_log = axs[3]\n",
    "#     for (i,l) in enumerate(_logvar.T):        \n",
    "#         ax_log.plot(l.T, alpha = (0.2 + 0.1*i))\n",
    "#     ax_log.grid()\n",
    "    \n",
    "    # Setting legend\n",
    "    blue_handle = plt.Line2D([], [], color='b', label='Original Data', alpha=0.2)\n",
    "    red_handle = plt.Line2D([], [], color='r', label='Reconstructions mean')\n",
    "#     for ax in axs:\n",
    "        # ax.set_title('Reconstruction of Train Data')\n",
    "    ax.set_title('Prediction of Test (unseen) Data')\n",
    "    ax.legend(handles=[blue_handle, red_handle], loc=\"upper right\")\n",
    "    \n",
    "    \n",
    "    # Axes labels\n",
    "#     ax_std.set_xlabel('Time')\n",
    "#     ax.set_ylabel('Values')\n",
    "#     ax_std.set_ylabel('Values')\n",
    "#     plt.ylim(-100,500)\n",
    "    plt.grid(True)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "4a25a5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3455\n",
      "tensor([0.6128], device='cpu')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[362], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# v.quantizer._embedding.weight.detach()[4] = a\u001b[39;00m\n\u001b[1;32m      3\u001b[0m v\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mcompare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVQ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m v\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[360], line 17\u001b[0m, in \u001b[0;36mcompare\u001b[0;34m(dataset, model, VQ)\u001b[0m\n\u001b[1;32m     15\u001b[0m     x_rec, loss, mu, logvar, z \u001b[38;5;241m=\u001b[39m model(data, v)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 17\u001b[0m     x_rec, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mreparametrization_trick(mu, logvar)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[314], line 438\u001b[0m, in \u001b[0;36mVariationalAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz\u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m#         print(\"Z: \", z.shape)\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m         rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;66;03m#         print(\"x:\", x)\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m#         print(\"Rec: \", rec)\u001b[39;00m\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rec, mu, logvar\n",
      "File \u001b[0;32m~/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[356], line 130\u001b[0m, in \u001b[0;36mGen_Decoder.forward\u001b[0;34m(self, cb)\u001b[0m\n\u001b[1;32m    128\u001b[0m         mean, logvar, trend_index, trend_slope, seasonality_fpw, seasonality_amp, seasonality_phase \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_params(cb)\n\u001b[1;32m    129\u001b[0m         rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_rec(mean, logvar)\n\u001b[0;32m--> 130\u001b[0m         rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_trend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrend_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrend_slope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m         rec \u001b[38;5;241m=\u001b[39m rec\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m#         print(\"shape of generated rec after unsqueeze\", rec.shape)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[356], line 110\u001b[0m, in \u001b[0;36mGen_Decoder.add_trend\u001b[0;34m(self, rec, trend_index, trend_slope)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_L)\n\u001b[1;32m    109\u001b[0m             \u001b[38;5;28mprint\u001b[39m(indexes)\n\u001b[0;32m--> 110\u001b[0m             mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_L\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mindexes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m#             print(\"mask\", mask.T.shape)\u001b[39;00m\n\u001b[1;32m    112\u001b[0m             trends[channel:] \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m*\u001b[39m slopes\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,2).to(x)\n",
    "# v.quantizer._embedding.weight.detach()[4] = a\n",
    "v.cpu()\n",
    "compare(train_data, v, VQ=False)\n",
    "v.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "f2e22afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(ax_heatmap, codebook):\n",
    "    ax_heatmap.clear()\n",
    "    heatmap = ax_heatmap.imshow(codebook)\n",
    "    ax_heatmap.set_title('Codebook Heatmap')\n",
    "    ax_heatmap\n",
    "\n",
    "\n",
    "    return heatmap\n",
    "def create_heatmap(codebook):\n",
    "    fig, ax_heatmap = plt.subplots(figsize=(12, 6), dpi=100)\n",
    "    heatmap = plot_heatmap(ax_heatmap, codebook.T)\n",
    "#     heatmap_canvas = FigureCanvasTkAgg(fig, master=heatmap_frame)\n",
    "#     heatmap_canvas.draw()\n",
    "#     heatmap_canvas.get_tk_widget().pack(fill=\"both\", expand=True)\n",
    "    cbar = fig.colorbar(heatmap)\n",
    "    ax_heatmap.set_xlabel('Num of Embeddings')\n",
    "    ax_heatmap.set_ylabel('Latent Dimensions')\n",
    "    \n",
    "    return ax_heatmap\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "9185de3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[320.44104  ,  -4.2680726]]], dtype=float32)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.figure import Figure\n",
    "codebook = v.z.cpu().detach().numpy()\n",
    "heatmap = create_heatmap(codebook)\n",
    "codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a5cd46cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4320])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c7ccc8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([408.2006])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "583dbbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951229424500714"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(0.5 * -0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eea6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
