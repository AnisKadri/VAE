{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "3e433565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "{'Channels_Coupling': {'amplitude': [], 'channels': []},\n",
      " 'Noise': {'channel': [], 'index': [], 'slope': []},\n",
      " 'Pulse': {'amplitude': [], 'channel': [], 'index': []},\n",
      " 'Seasonality': {'amplitude': [],\n",
      "                 'channel': [],\n",
      "                 'frequency_per_week': [],\n",
      "                 'phaseshift': []},\n",
      " 'Std_variation': {'amplitude': [], 'channel': [], 'interval': []},\n",
      " 'Trend': {'channel': [0],\n",
      "           'index': ['2023-03-05T00:10:00'],\n",
      "           'slope': [0.0017179206823066482]}}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "from dataGen import Gen\n",
    "# from utils import compare, experiment\n",
    "# from train import slidingWindow, criterion, train, test, objective, train_vae\n",
    "# from Encoders import LongShort_TCVAE_Encoder, RnnEncoder, MST_VAE_Encoder, MST_VAE_Encoder_dist\n",
    "# from Decoders import LongShort_TCVAE_Decoder, RnnDecoder, MST_VAE_Decoder, MST_VAE_Decoder_dist\n",
    "# from vae import VariationalAutoencoder, VQ_MST_VAE, VQ_Quantizer\n",
    "\n",
    "import torch; torch.manual_seed(955)\n",
    "import torch.optim as optim\n",
    "import torch.distributions as D\n",
    "from torch.utils.data import DataLoader\n",
    "# import optuna\n",
    "# from optuna.samplers import TPESampler\n",
    "# import torchaudio\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "# all parameters for generating the time series should be configured in this cell\n",
    "periode = 15 #days\n",
    "step = 5 # mess interval in minutes\n",
    "val = 500\n",
    "n_channels = 1\n",
    "effects = {\n",
    "    \"Pulse\": {\n",
    "        \"occurances\":0,\n",
    "        \"max_amplitude\":1.5,   \n",
    "        \"interval\":40\n",
    "        },\n",
    "    \"Trend\": {\n",
    "        \"occurances\":1,\n",
    "        \"max_slope\":0.005,\n",
    "        \"type\":\"linear\"\n",
    "        },\n",
    "    \"Seasonality\": {\n",
    "        \"occurances\":0,\n",
    "        \"frequency_per_week\":(7, 14), # min and max occurances per week\n",
    "        \"amplitude_range\":(5, 20),\n",
    "        },\n",
    "    \"std_variation\": {\n",
    "        \"occurances\":0,\n",
    "        \"max_value\":10,\n",
    "        \"interval\":1000,\n",
    "        },\n",
    "    \"channels_coupling\":{\n",
    "        \"occurances\":0,\n",
    "        \"coupling_strengh\":20\n",
    "        },\n",
    "    \"Noise\": {\n",
    "        \"occurances\":0,\n",
    "        \"max_slope\":0.005,\n",
    "        \"type\":\"linear\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "X = Gen(periode, step, val, n_channels, effects)\n",
    "x, params, e_params = X.parameters()\n",
    "# pprint.pprint(params)\n",
    "pprint.pprint(e_params)\n",
    "X.show()\n",
    "\n",
    "def lin_size(n, num_layers, first_kernel = None):\n",
    "    \n",
    "    for i in range(0, num_layers):\n",
    "        \n",
    "        if i == 0 and first_kernel != None:\n",
    "            n = 1 + ((n - first_kernel) // 2)\n",
    "        else:\n",
    "            n = 1 + ((n - 2) // 2)\n",
    "            \n",
    "    if n <= 0:\n",
    "        raise ValueError(\"Window Length is too small in relation to the number of Layers\")\n",
    "            \n",
    "    return n * 2 * num_layers\n",
    "\n",
    "class TCVAE_Encoder_unified(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, latent_dims, L=30, slope=0.2, first_kernel=None, modified=True):\n",
    "        super(TCVAE_Encoder_unified, self).__init__()\n",
    "\n",
    "        self.n = lin_size(L, num_layers, first_kernel)\n",
    "        self.cnn_layers = nn.ModuleList()\n",
    "        self.n_channels = input_size\n",
    "        self.L = L\n",
    "        self.num_layers = num_layers\n",
    "        self.latent_dims = latent_dims\n",
    "        self.modified = modified\n",
    "\n",
    "        if self.modified:\n",
    "            self.lin_input = self.n // (2 * self.num_layers)\n",
    "            self.lin_output = self.latent_dims\n",
    "        else:\n",
    "            self.lin_input = self.n * self.n_channels\n",
    "            self.lin_output = self.latent_dims * self.n_channels\n",
    "\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                torch.nn.init.kaiming_uniform_(m.weight, mode=\"fan_in\", nonlinearity=\"leaky_relu\")\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "        # CNN Layers that double the channels each time\n",
    "        for i in range(0, num_layers):\n",
    "            if i == 0:\n",
    "                if first_kernel == None: first_kernel = 2\n",
    "                self.cnn_layers.append(\n",
    "                    nn.Conv1d(input_size, input_size * 2, kernel_size=first_kernel, stride=2, padding=0))\n",
    "                self.cnn_layers.append(nn.LeakyReLU(slope, True))\n",
    "                self.cnn_layers.append(nn.BatchNorm1d(input_size * 2))\n",
    "            else:\n",
    "                self.cnn_layers.append(\n",
    "                    nn.Conv1d(input_size * 2 * i, input_size * 2 * (i + 1), kernel_size=2, stride=2, padding=0))\n",
    "                self.cnn_layers.append(nn.LeakyReLU(slope, True))\n",
    "                self.cnn_layers.append(nn.BatchNorm1d(input_size * 2 * (i + 1)))\n",
    "\n",
    "        # MLP Layers for Mu and logvar output\n",
    "        self.encoder_mu = nn.Linear(self.lin_input, self.lin_output)\n",
    "        self.encoder_logvar = nn.Linear(self.lin_input, self.lin_output)\n",
    "\n",
    "        # Init CNN\n",
    "        self.cnn_layers.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### CNN\n",
    "        for i, cnn in enumerate(self.cnn_layers):\n",
    "            #             print(\"Encoder Cnn\", x.shape)\n",
    "            x = cnn(x)\n",
    "        cnn_shape = x.shape\n",
    "        #         print(\"Encoder after Cnn \", x.shape)\n",
    "        if not self.modified:\n",
    "            x = x.view(x.size(0), -1)\n",
    "        #             print(\"Encoder reshape after Cnn \", x.shape)\n",
    "        # ### MLP\n",
    "        mu = self.encoder_mu(x)\n",
    "        logvar = self.encoder_logvar(x)\n",
    "        #         print(\"Encoder mu after lin \", mu.shape)\n",
    "        if not self.modified:\n",
    "            mu = mu.view(mu.shape[0], self.n_channels, -1)\n",
    "            logvar = logvar.view(logvar.shape[0], self.n_channels, -1)\n",
    "        #             print(\"Encoder mu after reshape \", mu.shape)\n",
    "        # mu.reshape\n",
    "\n",
    "        return mu, logvar\n",
    "    \n",
    "class LongShort_TCVAE_Encoder(nn.Module):\n",
    "    def __init__(self, input_size, num_layers, latent_dims, L=30, slope=0.2, first_kernel=None, modified=True,\n",
    "                 reduction=False):\n",
    "        super(LongShort_TCVAE_Encoder, self).__init__()\n",
    "        self.latent_dims = latent_dims\n",
    "        self.n_channels = input_size\n",
    "        self._modified = modified\n",
    "        self._reduction = reduction\n",
    "        if modified:\n",
    "            self.red_input = 2 * 2 * input_size * num_layers\n",
    "        else:\n",
    "            self.red_input = self.n_channels * 2\n",
    "\n",
    "        self.short_encoder = TCVAE_Encoder_unified(input_size, num_layers, latent_dims, L, slope, first_kernel=None,\n",
    "                                                   modified=self._modified)\n",
    "        self.long_encoder = TCVAE_Encoder_unified(input_size, num_layers, latent_dims, L, slope, first_kernel,\n",
    "                                                  modified=self._modified)\n",
    "\n",
    "        self.reduction_layer = nn.Conv1d(self.red_input, self.red_input // 2, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        short_mu, short_logvar = self.short_encoder(x)\n",
    "        long_mu, long_logvar = self.long_encoder(x)\n",
    "\n",
    "        mu = torch.cat((short_mu, long_mu), axis=1)\n",
    "        logvar = torch.cat((short_logvar, long_logvar), axis=1)\n",
    "\n",
    "        # print(\"Short Encoder mu: \", short_mu.shape)\n",
    "        # print(\"Long Encoder mu: \", long_mu.shape)\n",
    "        #\n",
    "        # print(\"After Cat: \", mu.shape)\n",
    "        if self._reduction:\n",
    "            mu = self.reduction_layer(mu)\n",
    "            logvar = self.reduction_layer(logvar)\n",
    "\n",
    "        return mu, logvar\n",
    "    \n",
    "class VQ_Quantizer(nn.Module):\n",
    "    def __init__(self, num_embed, dim_embed, commit_loss, decay, epsilon=1e-5, inits=None):\n",
    "        super(VQ_Quantizer, self).__init__()\n",
    "\n",
    "        self._num_embed = num_embed\n",
    "        self._dim_embed = dim_embed\n",
    "        self._commit_loss = commit_loss\n",
    "        if inits == None:\n",
    "            self._inits = torch.ones(num_embed)\n",
    "        else:\n",
    "            self._inits = inits\n",
    "        print(\"inits in quantizer\", self._inits)\n",
    "\n",
    "        self._embedding = nn.Embedding(self._num_embed, self._dim_embed)\n",
    "        self._embedding.weight.data.uniform_(-1 / self._num_embed, 1 / self._num_embed)\n",
    "        self._embedding.weight.data[:, 0] = self._inits\n",
    "        self._embedding.weight.data[:, 1] = torch.ones_like(self._embedding.weight.data[:, 1])\n",
    "        print(\"Codebook Init\", self._embedding.weight)\n",
    "\n",
    "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embed))\n",
    "        self._ema_w = nn.Parameter(torch.Tensor(num_embed, dim_embed))\n",
    "        self._ema_w.data.normal_()\n",
    "\n",
    "        self._decay = decay\n",
    "        self._epsilon = epsilon\n",
    "        self._std = 0.8\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : BCL -> BLC\n",
    "#         print(x.shape)\n",
    "        x_shape = x.shape\n",
    "        x = x.permute(0, 2, 1).contiguous()\n",
    "        \n",
    "#         print(x.shape)\n",
    "\n",
    "        # flaten the input to have the Channels as embedding space\n",
    "        x_flat = x.view(-1, self._dim_embed)\n",
    "#         print(x_flat.shape)\n",
    "\n",
    "        # Calculate the distance to embeddings\n",
    "\n",
    "#         print(\"the non squared x\", x_flat.shape )\n",
    "#         print(\"the non squared embed weights\", self._embedding.weight.t().shape)\n",
    "#         print(\"the x \", torch.sum(x_flat**2, dim = 1, keepdim = True).shape)\n",
    "#         print(\"the embed \", torch.sum(self._embedding.weight**2, dim = 1).shape)\n",
    "#         print(\"the matmul \", torch.matmul(x_flat, self._embedding.weight.t()).shape)\n",
    "        dist = (torch.sum(x_flat ** 2, dim=1, keepdim=True)\n",
    "                + torch.sum(self._embedding.weight ** 2, dim=1)\n",
    "                - 2 * torch.matmul(x_flat, self._embedding.weight.t()))\n",
    "#         print(dist.shape)\n",
    "\n",
    "        embed_indices = torch.argmin(dist, dim=1).unsqueeze(1)\n",
    "#         print(\"embed indices\",embed_indices)\n",
    "        if self.training:\n",
    "            noise = torch.randn(embed_indices.shape) * self._std\n",
    "            noise = torch.round(noise).to(torch.int32).to(embed_indices)\n",
    "            new_embed_indices = embed_indices + noise\n",
    "            new_embed_indices = torch.clamp(new_embed_indices, max=self._num_embed-1, min =0)\n",
    "#             embed_indices = new_embed_indices\n",
    "        # print(\"noise \",noise.shape)\n",
    "        # print(\"both together\",new_embed_indices)\n",
    "        embed_Matrix = torch.zeros_like(dist)\n",
    "#         embed_Matrix = torch.zeros(embed_indices.shape[0], self._num_embed).to(x)\n",
    "        #         print(embed_Matrix.shape)\n",
    "        embed_Matrix.scatter_(1, embed_indices, 1)\n",
    "#         print(\"embed_indices\", embed_indices)\n",
    "#         print(\"Embedding \", embed_Matrix.shape, embed_Matrix)\n",
    "#         print(\"codebook\", self._embedding.weight.shape, self._embedding.weight)\n",
    "        \n",
    "\n",
    "        # get the corresponding e vectors\n",
    "        quantizer = torch.matmul(embed_Matrix, self._embedding.weight)\n",
    "#         print(\"the quantizer\", quantizer.shape, quantizer)\n",
    "        quantizer = quantizer.view(x_shape).permute(0, 2, 1).contiguous()\n",
    "#         print(\"the quantizer\", quantizer.shape, quantizer)\n",
    "\n",
    "        # Use EMA to update the embedding vectors\n",
    "        if self.training:\n",
    "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
    "                                     (1 - self._decay) * torch.sum(embed_Matrix, 0)\n",
    "\n",
    "            # Laplace smoothing of the cluster size\n",
    "            n = torch.sum(self._ema_cluster_size.data)\n",
    "            self._ema_cluster_size = (\n",
    "                    (self._ema_cluster_size + self._epsilon)\n",
    "                    / (n + self._num_embed * self._epsilon) * n)\n",
    "\n",
    "            dw = torch.matmul(embed_Matrix.t(), x_flat)\n",
    "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
    "#             cb_new_values = self._ema_w / self._ema_cluster_size.unsqueeze(1)\n",
    "#             cb_new_values[:,1] = torch.clamp(cb_new_values[:,1], min = 0, max=1)\n",
    "#             self._embedding.weight = nn.Parameter(cb_new_values)\n",
    "\n",
    "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
    "#             self._embedding.weight = nn.Parameter(torch.clamp(self._embedding.weight[:,1], max=1.5))\n",
    "        #         print(\"quantizer \", quantizer.shape)\n",
    "\n",
    "        # Loss\n",
    "        #         first_loss = F.mse_loss(quantizer, x.detach())\n",
    "        #         second_loss = F.mse_loss(quantizer.detach(), x)\n",
    "        #         loss = first_loss + self._commit_loss * second_loss\n",
    "\n",
    "        # Loss EMA\n",
    "        e_loss = F.mse_loss(quantizer.detach(), x)\n",
    "        loss = self._commit_loss * e_loss\n",
    "        #         print(loss)\n",
    "\n",
    "        # straigh-through gradient\n",
    "        quantizer = x + (quantizer - x).detach()\n",
    "        quantizer = quantizer.permute(0, 2, 1).contiguous()\n",
    "        #         print(\"quantizer \", quantizer.shape)\n",
    "\n",
    "        return quantizer, loss\n",
    "    \n",
    "class VQ_MST_VAE(nn.Module):\n",
    "    def __init__(self, n_channels, num_layers, latent_dims, v_encoder, v_decoder, v_quantizer,\n",
    "                 L=30,\n",
    "                 slope=0.2,\n",
    "                 first_kernel=None,\n",
    "                 commit_loss=0.25,\n",
    "                 modified=True,\n",
    "                 reduction=False,\n",
    "                 inits=None\n",
    "                 ):\n",
    "        super(VQ_MST_VAE, self).__init__()\n",
    "\n",
    "        self._n_channels = n_channels\n",
    "        self._num_layers = num_layers\n",
    "        self._latent_dims = latent_dims\n",
    "        self._v_encoder = v_encoder\n",
    "        self._v_decoder = v_decoder\n",
    "        self._v_quantizer = v_quantizer\n",
    "        self._L = L\n",
    "        self._slope = slope\n",
    "        self._first_kernel = first_kernel\n",
    "        self._commit_loss = commit_loss\n",
    "        self._reduction = reduction\n",
    "        self._modified = modified\n",
    "        if inits == None:\n",
    "            self._inits = torch.ones(n_channels)\n",
    "        else:\n",
    "            self._inits = inits\n",
    "        print(self._inits)\n",
    "        if self._modified:\n",
    "            self._num_embed = self._n_channels * 4 * self._num_layers\n",
    "        else:\n",
    "            self._num_embed = self._n_channels * 2\n",
    "        if self._reduction:\n",
    "            self._num_embed = self._num_embed // 2\n",
    "\n",
    "        self.encoder = self._v_encoder(self._n_channels, self._num_layers, self._latent_dims, self._L, self._slope,\n",
    "                                       self._first_kernel, self._modified, self._reduction)\n",
    "#         self.decoder = self._v_decoder(self._n_channels, self._num_layers, self._latent_dims, self._L, self._slope,\n",
    "#                                        self._first_kernel, self._modified, self._reduction)\n",
    "        self.decoder = self._v_decoder(self._n_channels, self._latent_dims, self._L)\n",
    "        self.quantizer = self._v_quantizer(self._num_embed, self._latent_dims, self._commit_loss, decay=0.99,\n",
    "                                           epsilon=1e-5, inits=self._inits)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(self._num_embed)\n",
    "\n",
    "    def reparametrization_trick(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, v):\n",
    "        mu, logvar = self.encoder(x)\n",
    "\n",
    "        z = self.reparametrization_trick(mu, logvar)\n",
    "        # print(z.shape)\n",
    "        # z = self.bn(self.quantizer._embedding.weight[None,:])\n",
    "        # is_larger = torch.all(torch.gt(z[0], self.quantizer._embedding.weight))\n",
    "#         print(z.shape)\n",
    "        # print(\"Is encoder output larger than the set of vectors?\", is_larger)\n",
    "#         e, loss_quantize = self.quantizer(z)\n",
    "\n",
    "#         print(\"----------------Encoder Output-------------\")\n",
    "#         print(\"mu and logvar\", mu.shape, logvar.shape)\n",
    "#         print(\"----------------Reparametrization-------------\")\n",
    "#         print(\"Z\", z.shape)\n",
    "#         print(\"----------------Quantizer-------------\")\n",
    "#         print(\"quantized shape\", e.shape)\n",
    "#         print(\"loss shape\", loss_quantize)\n",
    "\n",
    "        #         mu_dec, logvar_dec = self.decoder(e)\n",
    "        #         x_rec = self.reparametrization_trick(mu_dec, mu_dec)\n",
    "        x_rec = self.decoder(z)#/v\n",
    "#         print(\"Rec before loss\", x_rec.shape, x_rec)\n",
    "#         print(\"x before loss\",x.shape, x)\n",
    "        loss_rec = F.mse_loss(x_rec, x, reduction='sum')\n",
    "#         print(\"rec loss\", loss_rec)\n",
    "#         print(\"loss_quantize\", loss_quantize)\n",
    "        loss = loss_rec# + loss_quantize\n",
    "\n",
    "#         print(\"----------------Decoding-------------\")\n",
    "#         print(\"----------------Decoder Output-------------\")\n",
    "#         # print(\"mu and logvar Decoder\", mu_dec.shape, logvar_dec.shape)\n",
    "#         print(\"rec shape\", x_rec.shape)\n",
    "        return x_rec, loss, mu, logvar, z#, mu_rec, logvar_rec\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, n_channels, num_layers, latent_dims, v_encoder, v_decoder, v_quantizer,\n",
    "                 L=30,\n",
    "                 slope=0.2,\n",
    "                 first_kernel=None,\n",
    "                 commit_loss=0.25,\n",
    "                 modified=True,\n",
    "                 reduction=False,\n",
    "                 inits=None\n",
    "                 ):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self._n_channels = n_channels\n",
    "        self._num_layers = num_layers\n",
    "        self._latent_dims = latent_dims\n",
    "        self._v_encoder = v_encoder\n",
    "        self._v_decoder = v_decoder\n",
    "        self._v_quantizer = v_quantizer\n",
    "        self._L = L\n",
    "        self._slope = slope\n",
    "        self._first_kernel = first_kernel\n",
    "        self._commit_loss = commit_loss\n",
    "        self._reduction = reduction\n",
    "        self._modified = modified\n",
    "        self.z = None\n",
    "        if inits == None:\n",
    "            self._inits = torch.ones(n_channels)\n",
    "        else:\n",
    "            self._inits = inits\n",
    "        print(self._inits)\n",
    "        if self._modified:\n",
    "            self._num_embed = self._n_channels * 4 * self._num_layers\n",
    "        else:\n",
    "            self._num_embed = self._n_channels * 2\n",
    "        if self._reduction:\n",
    "            self._num_embed = self._num_embed // 2\n",
    "        self.encoder = self._v_encoder(self._n_channels, self._num_layers, self._latent_dims, self._L, self._slope,\n",
    "                                       self._first_kernel, self._modified, self._reduction)\n",
    "        self.decoder = self._v_decoder(self._n_channels, self._latent_dims, self._L)\n",
    "        \n",
    "    def reparametrization_trick(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparametrization_trick(mu, logvar)\n",
    "        self.z= z.detach()\n",
    "#         print(\"Z: \", z.shape)\n",
    "        rec = self.decoder(z)\n",
    "#         print(\"x:\", x)\n",
    "#         print(\"Rec: \", rec)\n",
    "        return rec, mu, logvar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "cd98e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "class Gen_Decoder(nn.Module):\n",
    "    def __init__(self, n_channels, latent_dims, L=30):\n",
    "        super(Gen_Decoder, self).__init__()\n",
    "\n",
    "        self._n_channels = n_channels\n",
    "        self._latent_dims = latent_dims\n",
    "        self._L = L\n",
    "        self._n_params = 5\n",
    "        self._complexity = (self._latent_dims - 2) // self._n_params # for each channel: latent dims - 2 for the mean and std \n",
    "                                                        # divided by the number of parameters to extract\n",
    "#         device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#         self.mean = torch.empty(self._n_channels)\n",
    "#         self.logvar = torch.empty(self._n_channels)\n",
    "#         self.trend_index = torch.empty(self._n_channels, self._complexity)\n",
    "#         self.trend_slope = torch.empty(self._n_channels, self._complexity)\n",
    "        \n",
    "#         self.seasonality_fpw = torch.empty(self._n_channels, self._complexity)\n",
    "#         self.seasonality_amp = torch.empty(self._n_channels, self._complexity)\n",
    "#         self.seasonality_phase = torch.empty(self._n_channels, self._complexity)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.params = {\n",
    "#             \"mean\":torch.empty(self._n_channels),\n",
    "#             \"logvar\":torch.empty(self._n_channels),\n",
    "#             \"trend\":{\n",
    "#                 \"index\":torch.empty(self._n_channels, self._complexity),\n",
    "#                 \"slope\":torch.empty(self._n_channels, self._complexity)\n",
    "#             },\n",
    "#             \"seasonality\":{\n",
    "#                 \"frequency_per_week\":torch.empty(self._n_channels, self._complexity),\n",
    "#                 \"amplitude\":torch.empty(self._n_channels, self._complexity),\n",
    "#                 \"phaseshift\":torch.empty(self._n_channels, self._complexity)\n",
    "#             }\n",
    "#         } \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def reparametrization_trick(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def extract_params(self, codebook):\n",
    "#         codebook = codebook.squeeze(0)\n",
    "#         print(\"Codebook in the gen Decoder\",codebook.shape)\n",
    "        for channel in range(self._n_channels):\n",
    "#             print(\"channel loop index\", channel)\n",
    "#             print(\"Codbook Values\", codebook, codebook.T, codebook.T[0,channel])\n",
    "#             self.mean[channel] = codebook.T[0, channel]\n",
    "#             self.logvar[channel] = codebook.T[1,channel]\n",
    "            mean = codebook.T[0, channel]\n",
    "            logvar = codebook.T[1,channel]\n",
    "#             print(\"exracted mean\", mean.shape, mean)\n",
    "#             print(\"exracted logvar\", logvar.shape, logvar)\n",
    "#             self.params[\"mean\"][channel] = codebook.T[0, channel]\n",
    "#             self.params[\"logvar\"][channel] = 1.0 # torch.clamp(codebook.T[1, channel], max=2)\n",
    "            for i in range(self._complexity):\n",
    "                n_effect = i*self._n_params\n",
    "                trend_index       = codebook.T[2 + n_effect, channel]\n",
    "                trend_slope       = codebook.T[3 + n_effect, channel]\n",
    "                seasonality_fpw   = codebook.T[4 + n_effect, channel]\n",
    "                seasonality_amp   = codebook.T[5 + n_effect, channel]\n",
    "                seasonality_phase = codebook.T[6 + n_effect, channel]\n",
    "#             print(\"exracted trend_index\", trend_index.shape, trend_index)\n",
    "#             print(\"exracted trend_slope\", trend_slope.shape, trend_slope)\n",
    "#             print(\"exracted seasonality_fpw\", seasonality_fpw.shape, seasonality_fpw)\n",
    "#             print(\"exracted seasonality_amp\", seasonality_amp.shape, seasonality_amp)\n",
    "#             print(\"exracted seasonality_phase\", seasonality_phase.shape, seasonality_phase)\n",
    "                \n",
    "#                 self.params[\"trend\"][\"index\"][channel, i]                    = codebook.T[2 + n_effect, channel]\n",
    "#                 self.params[\"trend\"][\"slope\"][channel, i]                    = codebook.T[3 + n_effect, channel]\n",
    "#                 self.params[\"seasonality\"][\"frequency_per_week\"][channel, i] = codebook.T[4 + n_effect, channel]\n",
    "#                 self.params[\"seasonality\"][\"amplitude\"][channel, i]          = codebook.T[5 + n_effect, channel]\n",
    "#                 self.params[\"seasonality\"][\"phaseshift\"][channel, i]         = codebook.T[6 + n_effect, channel]\n",
    "        return mean, logvar, trend_index, trend_slope, seasonality_fpw, seasonality_amp, seasonality_phase\n",
    "    \n",
    "    def gen_rec(self, mean, logvar):\n",
    "#         print(\"extracted mean\", self.mean.shape)\n",
    "#         print(\"extracted std\", self.logvar)\n",
    "        means = mean.unsqueeze(1).expand(-1, self._L)\n",
    "#         print(\"mean after expand and squeeze\", means.shape)\n",
    "#         std = torch.exp(0.5 * self.logvar)\n",
    "        logvars = logvar.unsqueeze(1).expand(-1, self._L)\n",
    "#         means = self.params[\"mean\"].unsqueeze(1).expand(-1, self._L).requires_grad_()\n",
    "#         std = torch.exp(0.5 * self.params[\"logvar\"])\n",
    "#         stds = std.unsqueeze(1).expand(-1, self._L).requires_grad_()\n",
    "#         print(means.shape)\n",
    "#         print(stds.shape)\n",
    "        rec = self.reparametrization_trick(means, logvars)\n",
    "#         print(\"shape of generated rec\",rec.shape)\n",
    "#         print(rec)\n",
    "        return rec\n",
    "    \n",
    "    def add_trend(self, rec, trend_index, trend_slope):    \n",
    "        # generate the trends\n",
    "        trends = torch.zeros_like(rec)\n",
    "#         print(\"trend shape\", trends.shape)\n",
    "#         print(\"trend_index shape\", trend_index.shape)\n",
    "#         print(\"trend_slope shape\", trend_slope.shape)\n",
    "        for channel in range(self._n_channels):\n",
    "            indexes = trend_index.unsqueeze(1)[channel, :]\n",
    "#             print(\"indexes\", indexes.shape)\n",
    "            slopes = trend_slope.unsqueeze(1)[channel, :]\n",
    "#             print(\"slopes\", slopes.shape)\n",
    "            print(self._L)\n",
    "            print(indexes)\n",
    "            mask = torch.arange(self._L).unsqueeze(1) >= indexes.unsqueeze(0)\n",
    "#             print(\"mask\", mask.T.shape)\n",
    "            trends[channel:] = mask.T.sum(dim=1).float() * slopes\n",
    "#             print(\"generated trend\", trends.shape)\n",
    "        rec = rec + trends\n",
    "#         print(\"rec after trend\", rec.shape)\n",
    "#             if trend_type == \"linear\":\n",
    "#                 trends[ch, idx:] += np.linspace(0, slopes[channel] * self.step * shifted, shifted)\n",
    "#             elif trend_type == \"quadratic\":\n",
    "#                 trends[ch, idx:] += np.linspace(0, slopes[channel] * self.step * shifted, shifted)**2\n",
    "#             elif trend_type == \"mixed\":\n",
    "#                 trends[ch, idx:] += np.linspace(0, slopes[channel] * self.step * shifted, shifted)**((channel%2)+1)\n",
    "\n",
    "        # add it to the channels\n",
    "        return rec\n",
    "            \n",
    "\n",
    "    def forward(self, cb):\n",
    "        mean, logvar, trend_index, trend_slope, seasonality_fpw, seasonality_amp, seasonality_phase = self.extract_params(cb)\n",
    "        rec = self.gen_rec(mean, logvar)\n",
    "        rec = self.add_trend(rec, trend_index, trend_slope)\n",
    "        rec = rec.unsqueeze(0)\n",
    "#         print(\"shape of generated rec after unsqueeze\", rec.shape)\n",
    "        return rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "d3c2fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "class slidingWindow(Dataset):\n",
    "    def __init__(self, data, L):\n",
    "        self.data = data\n",
    "        self.L = L\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.data.shape[1] - index >= self.L:            \n",
    "            x = self.data[:,index:index+self.L]    \n",
    "            v = torch.sum(x/self.L, axis=1).unsqueeze(1)\n",
    "#             print(x.shape)\n",
    "#             print(v.unsqueeze(1).shape)\n",
    "            x = x#/v\n",
    "            return (x, v)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[1] - self.L\n",
    "    \n",
    "### Cost function\n",
    "def criterion(recon_x, x, mu, logvar):\n",
    "    ### reconstruction loss\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "\n",
    "    ### KL divergence loss\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    ### total loss\n",
    "    loss = recon_loss + 1.1*kld_loss\n",
    "    print(recon_loss, kld_loss)\n",
    "    return loss\n",
    "\n",
    "def sample_mean(model, batch, n):\n",
    "    batch_size = batch.shape[0]\n",
    "    n_channels = batch.shape[1]\n",
    "    latent_dims = model.encoder.latent_dims\n",
    "    L = model._L\n",
    "\n",
    "    mu, logvar = (torch.empty((batch_size, n_channels, latent_dims, 0)).to(batch) for _ in range(2))\n",
    "    REC = torch.empty(batch_size, n_channels, L, 0).to(batch)\n",
    "#     print(REC.shape)\n",
    "#     print(mu.shape)\n",
    "\n",
    "    for i in range(n):\n",
    "        rec, _mu, _logvar = model(batch)\n",
    "\n",
    "        REC = torch.cat((REC, rec.unsqueeze(-1)), dim=-1)\n",
    "        mu = torch.cat((mu, _mu.unsqueeze(-1)), dim=-1)\n",
    "        logvar = torch.cat((logvar, _logvar.unsqueeze(-1)), dim=-1)\n",
    "\n",
    "    # print(\"shapes after cat: mu, logvar, REC \", mu.shape, logvar.shape, REC.shape)\n",
    "    mu, logvar = (torch.mean(t, dim=-1) for t in [mu, logvar])\n",
    "    REC = torch.mean(REC, dim=-1)\n",
    "    #     print(\"shapes after mean: mu, logvar, REC \", mu.shape, logvar.shape, REC.shape)\n",
    "\n",
    "    return REC, mu, logvar\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, device, epoch, VQ = True):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, v) in enumerate(train_loader):\n",
    "#         print(batch_idx, data.shape)\n",
    "        data = data.to(device)\n",
    "        v = v.to(device)\n",
    "        optimizer.zero_grad()        \n",
    "\n",
    "        if VQ:\n",
    "            x_rec, loss, mu, logvar, z = model(data, v)\n",
    "        else:\n",
    "#             x_rec, mu, logvar = model(data)\n",
    "            x_rec, mu, logvar = sample_mean(model, data, 10)\n",
    "            if v.dim() == 1:\n",
    "                v = v.unsqueeze(-1)\n",
    "                v = v.unsqueeze(-1)\n",
    "            # x_rec_window_length = x_rec.shape[2]\n",
    "            loss = criterion((x_rec), data, mu, logvar)\n",
    "        # print(x_rec.shape)\n",
    "        # print(data[:, :, 0].shape)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "#         print(\"here\")\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "    \n",
    "    return  train_loss #/ len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "4dbf674a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([43.0799], device='cpu')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LongShort_TCVAE_Decoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[363], line 43\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# v = vae(n_channels, L, latent_dims)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m v \u001b[38;5;241m=\u001b[39m VariationalAutoencoder(n_channels,\n\u001b[1;32m     27\u001b[0m                             num_layers \u001b[38;5;241m=\u001b[39m  \u001b[38;5;241m3\u001b[39m,\u001b[38;5;66;03m#4, #3\u001b[39;00m\n\u001b[1;32m     28\u001b[0m                             latent_dims\u001b[38;5;241m=\u001b[39m latent_dims,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m                             reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     38\u001b[0m                             inits\u001b[38;5;241m=\u001b[39mx[:,\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     39\u001b[0m v \u001b[38;5;241m=\u001b[39m VQ_MST_VAE(n_channels \u001b[38;5;241m=\u001b[39m n_channels,\n\u001b[1;32m     40\u001b[0m                             num_layers \u001b[38;5;241m=\u001b[39m  \u001b[38;5;241m3\u001b[39m,\u001b[38;5;66;03m#4, #3\u001b[39;00m\n\u001b[1;32m     41\u001b[0m                             latent_dims\u001b[38;5;241m=\u001b[39m latent_dims,\n\u001b[1;32m     42\u001b[0m                             v_encoder \u001b[38;5;241m=\u001b[39m LongShort_TCVAE_Encoder, \u001b[38;5;66;03m#MST_VAE_Encoder,\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m                             v_decoder \u001b[38;5;241m=\u001b[39m \u001b[43mLongShort_TCVAE_Decoder\u001b[49m, \u001b[38;5;66;03m#LongShort_TCVAE_Decoder, #MST_VAE_Decoder,\u001b[39;00m\n\u001b[1;32m     44\u001b[0m                             v_quantizer \u001b[38;5;241m=\u001b[39m VQ_Quantizer,\n\u001b[1;32m     45\u001b[0m                             L\u001b[38;5;241m=\u001b[39mL,\n\u001b[1;32m     46\u001b[0m                             slope \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     47\u001b[0m                             first_kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60\u001b[39m, \u001b[38;5;66;03m#11, #20\u001b[39;00m\n\u001b[1;32m     48\u001b[0m                             commit_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.25\u001b[39m,\n\u001b[1;32m     49\u001b[0m                             modified\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     50\u001b[0m                             reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     51\u001b[0m                             inits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;66;03m#10 5\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# enc = MST_VAE_Encoder_dist(     n_channels,\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#                            num_layers = 3,\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#                            slope = 0.2,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#                            slope = 0.2,\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#                            first_kernel = 15)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LongShort_TCVAE_Decoder' is not defined"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor(x)\n",
    "n = x.shape[1]\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "train_ = x[:, :int(0.8*n)]\n",
    "val_   = x[:, int(0.8*n):int(0.9*n)]\n",
    "test_  = x[:, int(0.9*n):]\n",
    "\n",
    "# train_data = DataLoader(train_,\n",
    "#                         batch_size= 22,# 59, # 22\n",
    "#                         shuffle = False\n",
    "#                         )\n",
    "# val_data = DataLoader(val_,\n",
    "#                         batch_size=22,\n",
    "#                         shuffle = False\n",
    "#                         )\n",
    "# test_data = DataLoader(test_,\n",
    "#                         batch_size=22,\n",
    "#                         shuffle = False\n",
    "#                         )\n",
    "### Init Model\n",
    "latent_dims = 7 # 6 # 17\n",
    "L= 3455# 39 #32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# v = vae(n_channels, L, latent_dims)\n",
    "v = VariationalAutoencoder(n_channels,\n",
    "                            num_layers =  3,#4, #3\n",
    "                            latent_dims= latent_dims,\n",
    "                            v_encoder = LongShort_TCVAE_Encoder, #MST_VAE_Encoder,\n",
    "                            v_decoder = Gen_Decoder, #LongShort_TCVAE_Decoder, #MST_VAE_Decoder,\n",
    "                            v_quantizer = VQ_Quantizer,\n",
    "                            L=L,\n",
    "                            slope = 0,\n",
    "                            first_kernel = 60, #11, #20\n",
    "                            commit_loss = 0.25,\n",
    "                            modified=False,\n",
    "                            reduction = True,\n",
    "                            inits=x[:,0])\n",
    "v = VQ_MST_VAE(n_channels = n_channels,\n",
    "                            num_layers =  3,#4, #3\n",
    "                            latent_dims= latent_dims,\n",
    "                            v_encoder = LongShort_TCVAE_Encoder, #MST_VAE_Encoder,\n",
    "                            v_decoder = LongShort_TCVAE_Decoder, #LongShort_TCVAE_Decoder, #MST_VAE_Decoder,\n",
    "                            v_quantizer = VQ_Quantizer,\n",
    "                            L=L,\n",
    "                            slope = 0,\n",
    "                            first_kernel = 60, #11, #20\n",
    "                            commit_loss = 0.25,\n",
    "                            modified=False,\n",
    "                            reduction = True,\n",
    "                            inits=None) #10 5\n",
    "# enc = MST_VAE_Encoder_dist(     n_channels,\n",
    "#                            num_layers = 3,\n",
    "#                            slope = 0.2,\n",
    "#                            first_kernel = 15)\n",
    "# dec = MST_VAE_Decoder_dist(     n_channels,\n",
    "#                            num_layers = 3,\n",
    "#                            slope = 0.2,\n",
    "#                            first_kernel = 15)\n",
    "\n",
    "\n",
    "v = v.to(device)\n",
    "# enc = enc.to(device)\n",
    "# dec = dec.to(device)\n",
    "opt = optim.Adam(v.parameters(), lr = 0.001043529186448577) # 0.005043529186448577 0.006819850049647945\n",
    "# print(v)\n",
    "# Create the dictionaries of probability distributions\n",
    "# pnet = {'z': D.Normal(loc=torch.zeros(latent_dims), scale=torch.ones(latent_dims)), 'x': dec}\n",
    "# qnet = {'z': enc}\n",
    "# serie = torch.tensor(serie).float()\n",
    "train_ = x[:, :int(0.8*n)]\n",
    "val_   = x[:, int(0.8*n):int(0.9*n)]\n",
    "test_  = x[:, int(0.9*n):]\n",
    "\n",
    "train_data = DataLoader(slidingWindow(train_, L),\n",
    "                        batch_size= 22,# 59, # 22\n",
    "                        shuffle = False\n",
    "                        )\n",
    "val_data = DataLoader(slidingWindow(val_, L),\n",
    "                        batch_size=22,\n",
    "                        shuffle = False\n",
    "                        )\n",
    "test_data = DataLoader(slidingWindow(test_, L),\n",
    "                        batch_size=22,\n",
    "                        shuffle = False\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "3754910c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3455\n",
      "tensor([0.5715], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3242], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.2168], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7625], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1209], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9852], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8018], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5953], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3035], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0499], grad_fn=<SliceBackward0>)\n",
      "tensor(7.5439e+09, grad_fn=<MseLossBackward0>) tensor(1.2639, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 1 [0/1 (0%)]\tLoss: 7543931392.000000\n",
      "====> Epoch: 1 Average loss: 7543931392.0000\n",
      "3455\n",
      "tensor([-0.5563], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6963], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-4.0620], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-3.3613], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1711], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.8011], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0810], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-4.2090], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0502], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9840], grad_fn=<SliceBackward0>)\n",
      "tensor(1.9357e+11, grad_fn=<MseLossBackward0>) tensor(20.8509, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 2 [0/1 (0%)]\tLoss: 193568063488.000000\n",
      "====> Epoch: 2 Average loss: 193568063488.0000\n",
      "3455\n",
      "tensor([0.3897], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3607], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1502], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2719], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0258], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0039], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8702], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0144], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1105], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1626], grad_fn=<SliceBackward0>)\n",
      "tensor(3.0371e+10, grad_fn=<MseLossBackward0>) tensor(18.3048, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 3 [0/1 (0%)]\tLoss: 30370578432.000000\n",
      "====> Epoch: 3 Average loss: 30370578432.0000\n",
      "3455\n",
      "tensor([1.4567], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.4064], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.3215], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0668], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9950], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5034], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0555], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.1847], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0058], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.5239], grad_fn=<SliceBackward0>)\n",
      "tensor(1.1993e+10, grad_fn=<MseLossBackward0>) tensor(29.1897, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 4 [0/1 (0%)]\tLoss: 11992656896.000000\n",
      "====> Epoch: 4 Average loss: 11992656896.0000\n",
      "3455\n",
      "tensor([1.0189], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8278], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.4944], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7497], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2591], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.5880], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2737], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6267], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.3273], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.1489], grad_fn=<SliceBackward0>)\n",
      "tensor(7.8951e+10, grad_fn=<MseLossBackward0>) tensor(43.9834, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 5 [0/1 (0%)]\tLoss: 78951022592.000000\n",
      "====> Epoch: 5 Average loss: 78951022592.0000\n",
      "3455\n",
      "tensor([0.9365], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2729], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9584], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.3215], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0021], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7968], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2793], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.8063], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0227], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.3079], grad_fn=<SliceBackward0>)\n",
      "tensor(8.3748e+10, grad_fn=<MseLossBackward0>) tensor(53.5918, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 6 [0/1 (0%)]\tLoss: 83748347904.000000\n",
      "====> Epoch: 6 Average loss: 83748347904.0000\n",
      "3455\n",
      "tensor([0.3941], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.7430], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8212], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.4029], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6676], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2921], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1096], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8896], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1946], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3648], grad_fn=<SliceBackward0>)\n",
      "tensor(3.8686e+10, grad_fn=<MseLossBackward0>) tensor(59.4386, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 7 [0/1 (0%)]\tLoss: 38685609984.000000\n",
      "====> Epoch: 7 Average loss: 38685609984.0000\n",
      "3455\n",
      "tensor([-0.2831], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2444], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1151], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1118], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0610], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5079], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2583], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6027], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7324], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1411], grad_fn=<SliceBackward0>)\n",
      "tensor(3.1217e+09, grad_fn=<MseLossBackward0>) tensor(65.1958, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 8 [0/1 (0%)]\tLoss: 3121656832.000000\n",
      "====> Epoch: 8 Average loss: 3121656832.0000\n",
      "3455\n",
      "tensor([-0.3415], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1883], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3529], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5308], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8889], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4244], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6267], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0136], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0481], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1263], grad_fn=<SliceBackward0>)\n",
      "tensor(6.8802e+09, grad_fn=<MseLossBackward0>) tensor(71.7613, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 9 [0/1 (0%)]\tLoss: 6880240640.000000\n",
      "====> Epoch: 9 Average loss: 6880240640.0000\n",
      "3455\n",
      "tensor([-2.7023], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7390], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5880], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5094], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.5759], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.3074], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.5859], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8039], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2762], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6276], grad_fn=<SliceBackward0>)\n",
      "tensor(3.2023e+10, grad_fn=<MseLossBackward0>) tensor(77.8413, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 10 [0/1 (0%)]\tLoss: 32022511616.000000\n",
      "====> Epoch: 10 Average loss: 32022511616.0000\n",
      "3455\n",
      "tensor([2.2462], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.6690], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.2842], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8621], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1191], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.7933], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.6750], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8058], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.7709], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5321], grad_fn=<SliceBackward0>)\n",
      "tensor(4.2929e+10, grad_fn=<MseLossBackward0>) tensor(82.6438, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 11 [0/1 (0%)]\tLoss: 42929127424.000000\n",
      "====> Epoch: 11 Average loss: 42929127424.0000\n",
      "3455\n",
      "tensor([-1.5337], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.8642], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1369], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8392], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2542], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0975], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1897], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.8597], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.1977], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.3597], grad_fn=<SliceBackward0>)\n",
      "tensor(3.0047e+10, grad_fn=<MseLossBackward0>) tensor(85.3511, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 12 [0/1 (0%)]\tLoss: 30046535680.000000\n",
      "====> Epoch: 12 Average loss: 30046535680.0000\n",
      "3455\n",
      "tensor([0.6429], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4647], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1421], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4896], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9586], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3455\n",
      "tensor([1.1125], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2690], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.5130], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.5407], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8597], grad_fn=<SliceBackward0>)\n",
      "tensor(9.1361e+09, grad_fn=<MseLossBackward0>) tensor(86.3592, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 13 [0/1 (0%)]\tLoss: 9136136192.000000\n",
      "====> Epoch: 13 Average loss: 9136136192.0000\n",
      "3455\n",
      "tensor([-0.0444], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0840], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0655], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4405], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2429], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0232], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0953], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1285], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.7483], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7183], grad_fn=<SliceBackward0>)\n",
      "tensor(421911.3438, grad_fn=<MseLossBackward0>) tensor(86.2345, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 14 [0/1 (0%)]\tLoss: 422006.187500\n",
      "====> Epoch: 14 Average loss: 422006.1875\n",
      "3455\n",
      "tensor([1.0624], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.1611], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4354], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1409], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1372], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5370], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2228], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2458], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.7066], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3532], grad_fn=<SliceBackward0>)\n",
      "tensor(7.4700e+09, grad_fn=<MseLossBackward0>) tensor(86.0005, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 15 [0/1 (0%)]\tLoss: 7470007808.000000\n",
      "====> Epoch: 15 Average loss: 7470007808.0000\n",
      "3455\n",
      "tensor([2.4595], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1189], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3082], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8505], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8001], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3139], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7688], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.8313], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5168], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4414], grad_fn=<SliceBackward0>)\n",
      "tensor(1.9430e+10, grad_fn=<MseLossBackward0>) tensor(87.1522, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 16 [0/1 (0%)]\tLoss: 19429539840.000000\n",
      "====> Epoch: 16 Average loss: 19429539840.0000\n",
      "3455\n",
      "tensor([0.8404], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.0337], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4295], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.8573], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5219], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.2711], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2019], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.7088], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0529], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.9883], grad_fn=<SliceBackward0>)\n",
      "tensor(2.1653e+10, grad_fn=<MseLossBackward0>) tensor(88.7899, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 17 [0/1 (0%)]\tLoss: 21652664320.000000\n",
      "====> Epoch: 17 Average loss: 21652664320.0000\n",
      "3455\n",
      "tensor([0.5988], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5613], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7878], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0518], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0719], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5753], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2829], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1861], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0630], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9135], grad_fn=<SliceBackward0>)\n",
      "tensor(1.3312e+10, grad_fn=<MseLossBackward0>) tensor(92.2302, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 18 [0/1 (0%)]\tLoss: 13311908864.000000\n",
      "====> Epoch: 18 Average loss: 13311908864.0000\n",
      "3455\n",
      "tensor([0.2311], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1310], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5233], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5614], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4741], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2323], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5843], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5927], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1495], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2275], grad_fn=<SliceBackward0>)\n",
      "tensor(3.3697e+09, grad_fn=<MseLossBackward0>) tensor(95.6925, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 19 [0/1 (0%)]\tLoss: 3369737728.000000\n",
      "====> Epoch: 19 Average loss: 3369737728.0000\n",
      "3455\n",
      "tensor([-0.2527], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3655], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5951], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9517], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1826], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0137], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0647], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7518], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0282], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8594], grad_fn=<SliceBackward0>)\n",
      "tensor(69035888., grad_fn=<MseLossBackward0>) tensor(99.3913, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 20 [0/1 (0%)]\tLoss: 69036000.000000\n",
      "====> Epoch: 20 Average loss: 69036000.0000\n",
      "3455\n",
      "tensor([-0.2218], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1611], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5101], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1471], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.3120], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.6537], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0931], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1076], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.7856], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1888], grad_fn=<SliceBackward0>)\n",
      "tensor(4.2802e+09, grad_fn=<MseLossBackward0>) tensor(103.0120, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 21 [0/1 (0%)]\tLoss: 4280181248.000000\n",
      "====> Epoch: 21 Average loss: 4280181248.0000\n",
      "3455\n",
      "tensor([-2.0218], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0561], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.9195], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4369], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6475], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.6427], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.1009], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.0324], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9622], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2976], grad_fn=<SliceBackward0>)\n",
      "tensor(9.7028e+09, grad_fn=<MseLossBackward0>) tensor(107.2080, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 22 [0/1 (0%)]\tLoss: 9702756352.000000\n",
      "====> Epoch: 22 Average loss: 9702756352.0000\n",
      "3455\n",
      "tensor([-0.5113], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.6859], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6274], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9279], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3330], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6389], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1916], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8317], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3882], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4664], grad_fn=<SliceBackward0>)\n",
      "tensor(1.0130e+10, grad_fn=<MseLossBackward0>) tensor(112.9746, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 23 [0/1 (0%)]\tLoss: 10129950720.000000\n",
      "====> Epoch: 23 Average loss: 10129950720.0000\n",
      "3455\n",
      "tensor([-0.1424], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4908], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2930], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.9660], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2264], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9339], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0068], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5087], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0207], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6266], grad_fn=<SliceBackward0>)\n",
      "tensor(5.7446e+09, grad_fn=<MseLossBackward0>) tensor(119.3682, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 24 [0/1 (0%)]\tLoss: 5744648704.000000\n",
      "====> Epoch: 24 Average loss: 5744648704.0000\n",
      "3455\n",
      "tensor([0.0551], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9290], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1648], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7684], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5030], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4115], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.0485], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9131], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8218], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6601], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2632e+09, grad_fn=<MseLossBackward0>) tensor(125.3633, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 25 [0/1 (0%)]\tLoss: 1263197312.000000\n",
      "====> Epoch: 25 Average loss: 1263197312.0000\n",
      "3455\n",
      "tensor([1.6805], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.4459], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4129], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2377], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4540], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4542], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3368], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1515], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4007], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3964], grad_fn=<SliceBackward0>)\n",
      "tensor(90061664., grad_fn=<MseLossBackward0>) tensor(130.6273, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 26 [0/1 (0%)]\tLoss: 90061808.000000\n",
      "====> Epoch: 26 Average loss: 90061808.0000\n",
      "3455\n",
      "tensor([0.2574], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.8393], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2998], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0565], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1806], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5154], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5638], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7199], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1108], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6402], grad_fn=<SliceBackward0>)\n",
      "tensor(2.1846e+09, grad_fn=<MseLossBackward0>) tensor(134.9603, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 27 [0/1 (0%)]\tLoss: 2184647168.000000\n",
      "====> Epoch: 27 Average loss: 2184647168.0000\n",
      "3455\n",
      "tensor([0.8895], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1507], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4816], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7680], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.8994], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1486], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1243], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0860], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4380], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5894], grad_fn=<SliceBackward0>)\n",
      "tensor(4.4857e+09, grad_fn=<MseLossBackward0>) tensor(138.6646, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 28 [0/1 (0%)]\tLoss: 4485709824.000000\n",
      "====> Epoch: 28 Average loss: 4485709824.0000\n",
      "3455\n",
      "tensor([0.5825], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.5512], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.3088], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4553], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.8864], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1685], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2229], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8986], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4900], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4665], grad_fn=<SliceBackward0>)\n",
      "tensor(4.4648e+09, grad_fn=<MseLossBackward0>) tensor(141.9582, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 29 [0/1 (0%)]\tLoss: 4464784384.000000\n",
      "====> Epoch: 29 Average loss: 4464784384.0000\n",
      "3455\n",
      "tensor([-1.3649], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2677], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2031], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6789], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4300], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0080], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2464], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6193], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5045], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7831], grad_fn=<SliceBackward0>)\n",
      "tensor(2.3481e+09, grad_fn=<MseLossBackward0>) tensor(144.9383, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 30 [0/1 (0%)]\tLoss: 2348098816.000000\n",
      "====> Epoch: 30 Average loss: 2348098816.0000\n",
      "3455\n",
      "tensor([-0.0368], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.8632], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8676], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4652], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5000], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.7488], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3931], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7314], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.2968], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7460], grad_fn=<SliceBackward0>)\n",
      "tensor(3.4382e+08, grad_fn=<MseLossBackward0>) tensor(147.5941, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 31 [0/1 (0%)]\tLoss: 343821024.000000\n",
      "====> Epoch: 31 Average loss: 343821024.0000\n",
      "3455\n",
      "tensor([0.3513], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8272], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3100], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.5986], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1939], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2888], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4596], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0657], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7714], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1153], grad_fn=<SliceBackward0>)\n",
      "tensor(1.8055e+08, grad_fn=<MseLossBackward0>) tensor(149.8237, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 32 [0/1 (0%)]\tLoss: 180554432.000000\n",
      "====> Epoch: 32 Average loss: 180554432.0000\n",
      "3455\n",
      "tensor([-0.5285], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6174], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4806], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2538], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3489], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4210], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7079], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0135], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1167], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8689], grad_fn=<SliceBackward0>)\n",
      "tensor(1.4953e+09, grad_fn=<MseLossBackward0>) tensor(151.6091, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 33 [0/1 (0%)]\tLoss: 1495250304.000000\n",
      "====> Epoch: 33 Average loss: 1495250304.0000\n",
      "3455\n",
      "tensor([-0.0751], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0123], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1053], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2124], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.7353], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2397], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0663], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6094], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.9248], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1367], grad_fn=<SliceBackward0>)\n",
      "tensor(2.5277e+09, grad_fn=<MseLossBackward0>) tensor(152.9637, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 34 [0/1 (0%)]\tLoss: 2527657728.000000\n",
      "====> Epoch: 34 Average loss: 2527657728.0000\n",
      "3455\n",
      "tensor([-0.3342], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3352], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.7396], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7870], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9246], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6381], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0252], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1303], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4535], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.7048], grad_fn=<SliceBackward0>)\n",
      "tensor(2.1648e+09, grad_fn=<MseLossBackward0>) tensor(154.0053, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 35 [0/1 (0%)]\tLoss: 2164840192.000000\n",
      "====> Epoch: 35 Average loss: 2164840192.0000\n",
      "3455\n",
      "tensor([-0.0982], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5544], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0343], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1084], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2388], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2132], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4773], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9321], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.8637], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7101], grad_fn=<SliceBackward0>)\n",
      "tensor(8.5666e+08, grad_fn=<MseLossBackward0>) tensor(154.8623, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 36 [0/1 (0%)]\tLoss: 856663232.000000\n",
      "====> Epoch: 36 Average loss: 856663232.0000\n",
      "3455\n",
      "tensor([0.6930], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2655], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1287], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.5789], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7974], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9752], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.2984], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.0640], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1412], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3868], grad_fn=<SliceBackward0>)\n",
      "tensor(24887660., grad_fn=<MseLossBackward0>) tensor(155.6039, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 37 [0/1 (0%)]\tLoss: 24887832.000000\n",
      "====> Epoch: 37 Average loss: 24887832.0000\n",
      "3455\n",
      "tensor([-1.8814], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.3267], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6863], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6957], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0213], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.4786], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.9154], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0100], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3455\n",
      "tensor([-0.0687], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2847], grad_fn=<SliceBackward0>)\n",
      "tensor(3.5263e+08, grad_fn=<MseLossBackward0>) tensor(156.2790, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 38 [0/1 (0%)]\tLoss: 352626464.000000\n",
      "====> Epoch: 38 Average loss: 352626464.0000\n",
      "3455\n",
      "tensor([1.1794], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5088], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.9473], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.6119], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2172], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1327], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.7207], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8983], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2300], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4167], grad_fn=<SliceBackward0>)\n",
      "tensor(1.2178e+09, grad_fn=<MseLossBackward0>) tensor(156.8783, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 39 [0/1 (0%)]\tLoss: 1217790080.000000\n",
      "====> Epoch: 39 Average loss: 1217790080.0000\n",
      "3455\n",
      "tensor([0.9252], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5339], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2935], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3399], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2016], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7645], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2408], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0421], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3138], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3779], grad_fn=<SliceBackward0>)\n",
      "tensor(1.5231e+09, grad_fn=<MseLossBackward0>) tensor(157.3787, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 40 [0/1 (0%)]\tLoss: 1523100288.000000\n",
      "====> Epoch: 40 Average loss: 1523100288.0000\n",
      "3455\n",
      "tensor([0.8892], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8937], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.5162], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1624], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5260], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1131], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1600], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7130], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1725], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7830], grad_fn=<SliceBackward0>)\n",
      "tensor(9.5173e+08, grad_fn=<MseLossBackward0>) tensor(157.7802, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 41 [0/1 (0%)]\tLoss: 951728576.000000\n",
      "====> Epoch: 41 Average loss: 951728576.0000\n",
      "3455\n",
      "tensor([-1.7330], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3687], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4256], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1342], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6362], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2831], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1013], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0919], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8545], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0887], grad_fn=<SliceBackward0>)\n",
      "tensor(1.8972e+08, grad_fn=<MseLossBackward0>) tensor(158.1293, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 42 [0/1 (0%)]\tLoss: 189720512.000000\n",
      "====> Epoch: 42 Average loss: 189720512.0000\n",
      "3455\n",
      "tensor([-0.3775], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4106], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1005], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9068], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.9732], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9886], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0792], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7954], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0156], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4614], grad_fn=<SliceBackward0>)\n",
      "tensor(37574668., grad_fn=<MseLossBackward0>) tensor(158.4649, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 43 [0/1 (0%)]\tLoss: 37574844.000000\n",
      "====> Epoch: 43 Average loss: 37574844.0000\n",
      "3455\n",
      "tensor([-1.0429], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2849], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7667], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0048], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1064], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5079], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.9797], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3466], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9641], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0839], grad_fn=<SliceBackward0>)\n",
      "tensor(4.9710e+08, grad_fn=<MseLossBackward0>) tensor(158.8000, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 44 [0/1 (0%)]\tLoss: 497098848.000000\n",
      "====> Epoch: 44 Average loss: 497098848.0000\n",
      "3455\n",
      "tensor([-0.6625], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9111], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1423], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7199], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6891], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3398], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4663], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4305], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.1793], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0135], grad_fn=<SliceBackward0>)\n",
      "tensor(8.8743e+08, grad_fn=<MseLossBackward0>) tensor(159.0976, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 45 [0/1 (0%)]\tLoss: 887435072.000000\n",
      "====> Epoch: 45 Average loss: 887435072.0000\n",
      "3455\n",
      "tensor([-0.4279], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2986], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4329], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9792], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4098], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9053], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4272], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1020], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4427], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3034], grad_fn=<SliceBackward0>)\n",
      "tensor(7.2546e+08, grad_fn=<MseLossBackward0>) tensor(159.3203, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 46 [0/1 (0%)]\tLoss: 725462528.000000\n",
      "====> Epoch: 46 Average loss: 725462528.0000\n",
      "3455\n",
      "tensor([-1.1013], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0186], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7878], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.0798], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7381], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8006], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0802], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0879], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0936], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0852], grad_fn=<SliceBackward0>)\n",
      "tensor(2.3968e+08, grad_fn=<MseLossBackward0>) tensor(159.4759, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 47 [0/1 (0%)]\tLoss: 239681728.000000\n",
      "====> Epoch: 47 Average loss: 239681728.0000\n",
      "3455\n",
      "tensor([-0.1762], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.9788], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8708], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4361], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.9689], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9296], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7271], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0661], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6598], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7490], grad_fn=<SliceBackward0>)\n",
      "tensor(158070.1875, grad_fn=<MseLossBackward0>) tensor(159.6123, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 48 [0/1 (0%)]\tLoss: 158245.765625\n",
      "====> Epoch: 48 Average loss: 158245.7656\n",
      "3455\n",
      "tensor([-0.6056], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9882], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6198], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0579], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8989], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3336], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7359], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5804], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8526], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9566], grad_fn=<SliceBackward0>)\n",
      "tensor(2.0159e+08, grad_fn=<MseLossBackward0>) tensor(159.7533, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 49 [0/1 (0%)]\tLoss: 201589488.000000\n",
      "====> Epoch: 49 Average loss: 201589488.0000\n",
      "3455\n",
      "tensor([-0.5224], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2554], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1978], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3128], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8095], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2808], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7339], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4267], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3612], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1135], grad_fn=<SliceBackward0>)\n",
      "tensor(4.9269e+08, grad_fn=<MseLossBackward0>) tensor(159.8835, grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 50 [0/1 (0%)]\tLoss: 492685184.000000\n",
      "====> Epoch: 50 Average loss: 492685184.0000\n",
      "3455\n",
      "tensor([-1.1231], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.2293], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2666], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0588], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5660], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.9242], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2552], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.3928], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4010], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5174], grad_fn=<SliceBackward0>)\n",
      "tensor(4.7807e+08, grad_fn=<MseLossBackward0>) tensor(159.9801, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 51 [0/1 (0%)]\tLoss: 478069216.000000\n",
      "====> Epoch: 51 Average loss: 478069216.0000\n",
      "3455\n",
      "tensor([1.3692], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.6676], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4633], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.5304], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1342], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1300], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7412], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6201], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4966], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1757], grad_fn=<SliceBackward0>)\n",
      "tensor(1.9567e+08, grad_fn=<MseLossBackward0>) tensor(160.0447, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 52 [0/1 (0%)]\tLoss: 195669200.000000\n",
      "====> Epoch: 52 Average loss: 195669200.0000\n",
      "3455\n",
      "tensor([-1.4532], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0387], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0669], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1105], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3393], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0990], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8331], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9357], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6860], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7686], grad_fn=<SliceBackward0>)\n",
      "tensor(3934666., grad_fn=<MseLossBackward0>) tensor(160.0984, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 53 [0/1 (0%)]\tLoss: 3934842.000000\n",
      "====> Epoch: 53 Average loss: 3934842.0000\n",
      "3455\n",
      "tensor([-0.2286], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1238], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8123], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5877], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8524], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3095], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1641], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7551], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5399], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3844], grad_fn=<SliceBackward0>)\n",
      "tensor(93751856., grad_fn=<MseLossBackward0>) tensor(160.1497, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 54 [0/1 (0%)]\tLoss: 93752032.000000\n",
      "====> Epoch: 54 Average loss: 93752032.0000\n",
      "3455\n",
      "tensor([0.8126], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6339], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.2366], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7719], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8452], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4819], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7626], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2556], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4410], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0518], grad_fn=<SliceBackward0>)\n",
      "tensor(2.8238e+08, grad_fn=<MseLossBackward0>) tensor(160.1861, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 55 [0/1 (0%)]\tLoss: 282381568.000000\n",
      "====> Epoch: 55 Average loss: 282381568.0000\n",
      "3455\n",
      "tensor([-1.1560], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8321], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.5019], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4019], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.4282], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.6055], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0644], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0237], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4878], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.5823], grad_fn=<SliceBackward0>)\n",
      "tensor(2.9710e+08, grad_fn=<MseLossBackward0>) tensor(160.1966, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 56 [0/1 (0%)]\tLoss: 297103040.000000\n",
      "====> Epoch: 56 Average loss: 297103040.0000\n",
      "3455\n",
      "tensor([0.3723], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3806], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3233], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2861], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6320], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4701], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5694], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.2427], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.0822], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8738], grad_fn=<SliceBackward0>)\n",
      "tensor(1.2977e+08, grad_fn=<MseLossBackward0>) tensor(160.1858, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 57 [0/1 (0%)]\tLoss: 129767952.000000\n",
      "====> Epoch: 57 Average loss: 129767952.0000\n",
      "3455\n",
      "tensor([-0.5239], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.2121], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8870], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8997], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1496], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.5499], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6253], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.2353], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9098], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4063], grad_fn=<SliceBackward0>)\n",
      "tensor(3744435., grad_fn=<MseLossBackward0>) tensor(160.1649, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 58 [0/1 (0%)]\tLoss: 3744611.250000\n",
      "====> Epoch: 58 Average loss: 3744611.2500\n",
      "3455\n",
      "tensor([-1.2842], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9879], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5140], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1218], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2061], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6652], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3926], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8262], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0351], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3751], grad_fn=<SliceBackward0>)\n",
      "tensor(55418600., grad_fn=<MseLossBackward0>) tensor(160.1382, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 59 [0/1 (0%)]\tLoss: 55418776.000000\n",
      "====> Epoch: 59 Average loss: 55418776.0000\n",
      "3455\n",
      "tensor([-0.2996], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7709], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0071], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4856], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.8137], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5301], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7588], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2918], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4404], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6377], grad_fn=<SliceBackward0>)\n",
      "tensor(1.7233e+08, grad_fn=<MseLossBackward0>) tensor(160.1049, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 60 [0/1 (0%)]\tLoss: 172328480.000000\n",
      "====> Epoch: 60 Average loss: 172328480.0000\n",
      "3455\n",
      "tensor([1.2216], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3125], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3472], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3995], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.9001], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4708], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4131], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1178], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8629], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6223], grad_fn=<SliceBackward0>)\n",
      "tensor(1.7787e+08, grad_fn=<MseLossBackward0>) tensor(160.0633, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 61 [0/1 (0%)]\tLoss: 177871520.000000\n",
      "====> Epoch: 61 Average loss: 177871520.0000\n",
      "3455\n",
      "tensor([0.7012], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6445], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2655], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0253], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3029], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4295], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6966], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1039], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6604], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3936], grad_fn=<SliceBackward0>)\n",
      "tensor(72552480., grad_fn=<MseLossBackward0>) tensor(160.0171, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 62 [0/1 (0%)]\tLoss: 72552656.000000\n",
      "====> Epoch: 62 Average loss: 72552656.0000\n",
      "3455\n",
      "tensor([0.4719], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4197], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3921], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.4227], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9515], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2995], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6010], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4209], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3455\n",
      "tensor([-0.6525], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3772], grad_fn=<SliceBackward0>)\n",
      "tensor(899272.8125, grad_fn=<MseLossBackward0>) tensor(159.9703, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 63 [0/1 (0%)]\tLoss: 899448.750000\n",
      "====> Epoch: 63 Average loss: 899448.7500\n",
      "3455\n",
      "tensor([1.8677], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6648], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2988], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4162], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9607], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8806], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1213], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9056], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7610], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9732], grad_fn=<SliceBackward0>)\n",
      "tensor(40930744., grad_fn=<MseLossBackward0>) tensor(159.9210, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 64 [0/1 (0%)]\tLoss: 40930920.000000\n",
      "====> Epoch: 64 Average loss: 40930920.0000\n",
      "3455\n",
      "tensor([0.2711], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1270], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.8624], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6064], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7640], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8292], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.6467], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.5232], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3666], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3465], grad_fn=<SliceBackward0>)\n",
      "tensor(1.1016e+08, grad_fn=<MseLossBackward0>) tensor(159.8619, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 65 [0/1 (0%)]\tLoss: 110155248.000000\n",
      "====> Epoch: 65 Average loss: 110155248.0000\n",
      "3455\n",
      "tensor([1.3985], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1570], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0651], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.5177], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0226], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1703], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1895], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.4225], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.3255], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3532], grad_fn=<SliceBackward0>)\n",
      "tensor(1.0267e+08, grad_fn=<MseLossBackward0>) tensor(159.7884, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 66 [0/1 (0%)]\tLoss: 102665296.000000\n",
      "====> Epoch: 66 Average loss: 102665296.0000\n",
      "3455\n",
      "tensor([-0.7825], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7837], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8769], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1670], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6355], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-3.3703], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1320], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.6025], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8974], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7277], grad_fn=<SliceBackward0>)\n",
      "tensor(33767404., grad_fn=<MseLossBackward0>) tensor(159.7052, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 67 [0/1 (0%)]\tLoss: 33767580.000000\n",
      "====> Epoch: 67 Average loss: 33767580.0000\n",
      "3455\n",
      "tensor([-0.3800], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2046], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0622], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3020], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9351], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0409], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1322], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1944], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2527], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0449], grad_fn=<SliceBackward0>)\n",
      "tensor(328457.8750, grad_fn=<MseLossBackward0>) tensor(159.6194, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 68 [0/1 (0%)]\tLoss: 328633.468750\n",
      "====> Epoch: 68 Average loss: 328633.4688\n",
      "3455\n",
      "tensor([0.8866], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.6814], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5401], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.6899], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5656], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4391], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.6974], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.8126], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2519], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3346], grad_fn=<SliceBackward0>)\n",
      "tensor(34845768., grad_fn=<MseLossBackward0>) tensor(159.5324, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 69 [0/1 (0%)]\tLoss: 34845944.000000\n",
      "====> Epoch: 69 Average loss: 34845944.0000\n",
      "3455\n",
      "tensor([0.9529], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2544], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9980], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8310], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.5028], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8041], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6317], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0186], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0963], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4222], grad_fn=<SliceBackward0>)\n",
      "tensor(72216232., grad_fn=<MseLossBackward0>) tensor(159.4422, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 70 [0/1 (0%)]\tLoss: 72216408.000000\n",
      "====> Epoch: 70 Average loss: 72216408.0000\n",
      "3455\n",
      "tensor([-0.0288], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0194], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9282], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4110], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0644], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6263], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0236], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9215], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.4041], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.1575], grad_fn=<SliceBackward0>)\n",
      "tensor(54767656., grad_fn=<MseLossBackward0>) tensor(159.3495, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 71 [0/1 (0%)]\tLoss: 54767832.000000\n",
      "====> Epoch: 71 Average loss: 54767832.0000\n",
      "3455\n",
      "tensor([0.1214], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7520], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2566], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7251], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1732], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7583], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0692], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1186], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2115], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1151], grad_fn=<SliceBackward0>)\n",
      "tensor(11515222., grad_fn=<MseLossBackward0>) tensor(159.2573, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 72 [0/1 (0%)]\tLoss: 11515397.000000\n",
      "====> Epoch: 72 Average loss: 11515397.0000\n",
      "3455\n",
      "tensor([-0.3836], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.2031], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4053], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5782], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8772], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.9160], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5014], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1405], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0240], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6454], grad_fn=<SliceBackward0>)\n",
      "tensor(2838526.5000, grad_fn=<MseLossBackward0>) tensor(159.1647, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 73 [0/1 (0%)]\tLoss: 2838701.500000\n",
      "====> Epoch: 73 Average loss: 2838701.5000\n",
      "3455\n",
      "tensor([-0.9483], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.9584], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3179], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4079], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6248], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1844], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1408], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8883], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.0092], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0663], grad_fn=<SliceBackward0>)\n",
      "tensor(30937144., grad_fn=<MseLossBackward0>) tensor(159.0686, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 74 [0/1 (0%)]\tLoss: 30937318.000000\n",
      "====> Epoch: 74 Average loss: 30937318.0000\n",
      "3455\n",
      "tensor([0.1946], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0432], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3623], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3529], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7140], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2207], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4765], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7064], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0170], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1568], grad_fn=<SliceBackward0>)\n",
      "tensor(45474196., grad_fn=<MseLossBackward0>) tensor(158.9664, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 75 [0/1 (0%)]\tLoss: 45474372.000000\n",
      "====> Epoch: 75 Average loss: 45474372.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3455\n",
      "tensor([1.5969], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3203], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3689], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5796], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0387], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3371], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4387], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1125], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5244], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1843], grad_fn=<SliceBackward0>)\n",
      "tensor(24987554., grad_fn=<MseLossBackward0>) tensor(158.8591, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 76 [0/1 (0%)]\tLoss: 24987728.000000\n",
      "====> Epoch: 76 Average loss: 24987728.0000\n",
      "3455\n",
      "tensor([-1.4472], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4003], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3350], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2706], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6009], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1534], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0609], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2822], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2043], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0203], grad_fn=<SliceBackward0>)\n",
      "tensor(1800198.1250, grad_fn=<MseLossBackward0>) tensor(158.7485, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 77 [0/1 (0%)]\tLoss: 1800372.750000\n",
      "====> Epoch: 77 Average loss: 1800372.7500\n",
      "3455\n",
      "tensor([-0.4619], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8103], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5758], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.2312], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7315], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9851], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7872], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9573], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4238], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6559], grad_fn=<SliceBackward0>)\n",
      "tensor(6934224., grad_fn=<MseLossBackward0>) tensor(158.6362, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 78 [0/1 (0%)]\tLoss: 6934398.500000\n",
      "====> Epoch: 78 Average loss: 6934398.5000\n",
      "3455\n",
      "tensor([1.7033], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.6686], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5103], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5654], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6739], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3082], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8462], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4555], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2926], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0646], grad_fn=<SliceBackward0>)\n",
      "tensor(25347588., grad_fn=<MseLossBackward0>) tensor(158.5236, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 79 [0/1 (0%)]\tLoss: 25347762.000000\n",
      "====> Epoch: 79 Average loss: 25347762.0000\n",
      "3455\n",
      "tensor([-1.1195], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5968], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9763], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8584], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1973], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0133], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8434], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8820], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.5649], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0748], grad_fn=<SliceBackward0>)\n",
      "tensor(25519230., grad_fn=<MseLossBackward0>) tensor(158.4110, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 80 [0/1 (0%)]\tLoss: 25519404.000000\n",
      "====> Epoch: 80 Average loss: 25519404.0000\n",
      "3455\n",
      "tensor([-1.0022], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1453], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3869], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.2714], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0359], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1097], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8830], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9134], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1184], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8345], grad_fn=<SliceBackward0>)\n",
      "tensor(8302839., grad_fn=<MseLossBackward0>) tensor(158.2987, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 81 [0/1 (0%)]\tLoss: 8303013.000000\n",
      "====> Epoch: 81 Average loss: 8303013.0000\n",
      "3455\n",
      "tensor([0.7414], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4377], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3177], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1249], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1899], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4211], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0274], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5343], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4499], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1950], grad_fn=<SliceBackward0>)\n",
      "tensor(267756.3438, grad_fn=<MseLossBackward0>) tensor(158.1864, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 82 [0/1 (0%)]\tLoss: 267930.343750\n",
      "====> Epoch: 82 Average loss: 267930.3438\n",
      "3455\n",
      "tensor([0.8600], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8355], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0839], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1649], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2557], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8823], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5318], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0306], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6791], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7909], grad_fn=<SliceBackward0>)\n",
      "tensor(9937236., grad_fn=<MseLossBackward0>) tensor(158.0729, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 83 [0/1 (0%)]\tLoss: 9937410.000000\n",
      "====> Epoch: 83 Average loss: 9937410.0000\n",
      "3455\n",
      "tensor([1.0613], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1638], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4287], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5978], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0662], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4434], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4874], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.6953], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3827], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.4233], grad_fn=<SliceBackward0>)\n",
      "tensor(18341456., grad_fn=<MseLossBackward0>) tensor(157.9558, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 84 [0/1 (0%)]\tLoss: 18341630.000000\n",
      "====> Epoch: 84 Average loss: 18341630.0000\n",
      "3455\n",
      "tensor([-0.2335], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3528], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4885], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6354], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1915], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1245], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1168], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1814], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.2492], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.0398], grad_fn=<SliceBackward0>)\n",
      "tensor(11508712., grad_fn=<MseLossBackward0>) tensor(157.8348, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 85 [0/1 (0%)]\tLoss: 11508886.000000\n",
      "====> Epoch: 85 Average loss: 11508886.0000\n",
      "3455\n",
      "tensor([-1.0275], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3450], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0683], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1626], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1421], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0485], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4393], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3891], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.9059], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7900], grad_fn=<SliceBackward0>)\n",
      "tensor(1269785.8750, grad_fn=<MseLossBackward0>) tensor(157.7113, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 86 [0/1 (0%)]\tLoss: 1269959.375000\n",
      "====> Epoch: 86 Average loss: 1269959.3750\n",
      "3455\n",
      "tensor([1.3073], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.2398], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0785], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2302], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8825], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.0859], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4420], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2243], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4025], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8292], grad_fn=<SliceBackward0>)\n",
      "tensor(2458990.7500, grad_fn=<MseLossBackward0>) tensor(157.5871, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 87 [0/1 (0%)]\tLoss: 2459164.000000\n",
      "====> Epoch: 87 Average loss: 2459164.0000\n",
      "3455\n",
      "tensor([-1.8612], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.1688], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4472], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3088], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0520], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.6101], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1021], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1421], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3455\n",
      "tensor([-0.7510], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1644], grad_fn=<SliceBackward0>)\n",
      "tensor(10129392., grad_fn=<MseLossBackward0>) tensor(157.4632, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 88 [0/1 (0%)]\tLoss: 10129565.000000\n",
      "====> Epoch: 88 Average loss: 10129565.0000\n",
      "3455\n",
      "tensor([0.8790], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3908], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1824], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7190], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5163], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4000], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9519], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1916], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.8969], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.5368], grad_fn=<SliceBackward0>)\n",
      "tensor(10299526., grad_fn=<MseLossBackward0>) tensor(157.3394, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 89 [0/1 (0%)]\tLoss: 10299699.000000\n",
      "====> Epoch: 89 Average loss: 10299699.0000\n",
      "3455\n",
      "tensor([0.4481], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0908], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0326], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5893], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7043], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3081], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3869], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2219], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1613], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1309], grad_fn=<SliceBackward0>)\n",
      "tensor(3309640.5000, grad_fn=<MseLossBackward0>) tensor(157.2160, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 90 [0/1 (0%)]\tLoss: 3309813.500000\n",
      "====> Epoch: 90 Average loss: 3309813.5000\n",
      "3455\n",
      "tensor([-0.4993], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.9010], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9720], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.5972], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.0091], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6531], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8978], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1183], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1682], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4787], grad_fn=<SliceBackward0>)\n",
      "tensor(256562.0781, grad_fn=<MseLossBackward0>) tensor(157.0925, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 91 [0/1 (0%)]\tLoss: 256734.875000\n",
      "====> Epoch: 91 Average loss: 256734.8750\n",
      "3455\n",
      "tensor([-1.0851], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1902], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0228], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1071], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9861], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.2223], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.2068], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0100], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7883], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7387], grad_fn=<SliceBackward0>)\n",
      "tensor(4527659., grad_fn=<MseLossBackward0>) tensor(156.9680, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 92 [0/1 (0%)]\tLoss: 4527831.500000\n",
      "====> Epoch: 92 Average loss: 4527831.5000\n",
      "3455\n",
      "tensor([0.3194], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.7401], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.1713], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7057], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1961], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0194], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7098], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.7669], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0412], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0489], grad_fn=<SliceBackward0>)\n",
      "tensor(7455806.5000, grad_fn=<MseLossBackward0>) tensor(156.8418, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 93 [0/1 (0%)]\tLoss: 7455979.000000\n",
      "====> Epoch: 93 Average loss: 7455979.0000\n",
      "3455\n",
      "tensor([-0.3393], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6986], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8034], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([2.4384], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.6925], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.0739], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2557], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0949], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7797], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3252], grad_fn=<SliceBackward0>)\n",
      "tensor(4155416.5000, grad_fn=<MseLossBackward0>) tensor(156.7133, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 94 [0/1 (0%)]\tLoss: 4155589.000000\n",
      "====> Epoch: 94 Average loss: 4155589.0000\n",
      "3455\n",
      "tensor([-0.0212], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3937], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1194], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.8085], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4758], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3924], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1768], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8056], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4071], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6208], grad_fn=<SliceBackward0>)\n",
      "tensor(336967.2500, grad_fn=<MseLossBackward0>) tensor(156.5831, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 95 [0/1 (0%)]\tLoss: 337139.500000\n",
      "====> Epoch: 95 Average loss: 337139.5000\n",
      "3455\n",
      "tensor([1.4683], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2273], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.2331], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.3875], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.4649], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0588], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4590], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.3714], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3829], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.6867], grad_fn=<SliceBackward0>)\n",
      "tensor(1689270.3750, grad_fn=<MseLossBackward0>) tensor(156.4522, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 96 [0/1 (0%)]\tLoss: 1689442.500000\n",
      "====> Epoch: 96 Average loss: 1689442.5000\n",
      "3455\n",
      "tensor([-0.4954], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1243], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6979], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6075], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9503], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1045], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1028], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.2414], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3755], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.1888], grad_fn=<SliceBackward0>)\n",
      "tensor(4686641., grad_fn=<MseLossBackward0>) tensor(156.3216, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 97 [0/1 (0%)]\tLoss: 4686813.000000\n",
      "====> Epoch: 97 Average loss: 4686813.0000\n",
      "3455\n",
      "tensor([-1.2570], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.3076], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.6720], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.7084], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.4075], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.9242], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6875], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0074], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4326], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.4091], grad_fn=<SliceBackward0>)\n",
      "tensor(3787304.5000, grad_fn=<MseLossBackward0>) tensor(156.1913, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 98 [0/1 (0%)]\tLoss: 3787476.250000\n",
      "====> Epoch: 98 Average loss: 3787476.2500\n",
      "3455\n",
      "tensor([1.1731], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.8060], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.0259], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-2.3778], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.6606], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-1.0521], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([1.6129], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1740], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([0.1568], grad_fn=<SliceBackward0>)\n",
      "3455\n",
      "tensor([-0.4050], grad_fn=<SliceBackward0>)\n",
      "tensor(820139., grad_fn=<MseLossBackward0>) tensor(156.0612, grad_fn=<MulBackward0>)\n",
      "Train Epoch: 99 [0/1 (0%)]\tLoss: 820310.687500\n",
      "====> Epoch: 99 Average loss: 820310.6875\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 100):\n",
    "    train(v, train_data, criterion, opt, device, epoch, VQ = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "ff1e8584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(dataset, model, VQ=True):\n",
    "    model.eval()\n",
    "    rec = []\n",
    "    x = []\n",
    "    T = dataset.dataset.data.shape[1]- L\n",
    "    idx = 0\n",
    "    _mu_rec, _logvar_rec = [], []\n",
    "    _mu, _logvar = [], []\n",
    "#     _mu, _logvar = (torch.empty((n_channels * 2, T, L)) for _ in range(2))\n",
    "#     _mu_rec, _logvar_rec = (torch.empty((T, n_channels)) for _ in range(2))\n",
    "    with torch.no_grad():\n",
    "        for i, (data, v) in enumerate(dataset):\n",
    "            bs = data.shape[0]\n",
    "            if VQ:\n",
    "                x_rec, loss, mu, logvar, z = model(data, v)\n",
    "            else:\n",
    "                x_rec, mu, logvar = model(data)\n",
    "            z = model.reparametrization_trick(mu, logvar)\n",
    "            if v.dim() == 1:\n",
    "                v = v.unsqueeze(0)\n",
    "                v = v.T\n",
    "                v = v.unsqueeze(-1)\n",
    "#             print(v.shape)\n",
    "#             print(x_rec.shape)\n",
    "#             print((x_rec * v).shape)\n",
    "#             print(i)\n",
    "#             print(mu.shape)\n",
    "\n",
    "            x.extend((data)[:,:,:].detach().numpy())\n",
    "            rec.extend(((x_rec)[:,:,:]).detach().numpy())\n",
    "#             _mu_rec.extend((mu_rec*v)[:,:,0].detach().numpy())\n",
    "#             _logvar_rec.extend(((torch.exp(0.5 * logvar_rec))*v)[:,:,0].detach().numpy())\n",
    "            \n",
    "#             _mu.extend((mu*v)[:,:,0].detach().numpy())\n",
    "#             _logvar.extend(((torch.exp(0.5 * logvar))*v)[:,:,0].detach().numpy())\n",
    "#             _mu[idx: idx + bs, :, :] = mu\n",
    "#             _mu_rec[idx: idx + bs, :] = (mu_rec*v)[:,:,0]\n",
    "#     print(np.array(rec).shape)\n",
    "#     _mu = np.array(_mu)\n",
    "#     _logvar = np.array(_logvar)\n",
    "    \n",
    "    rec = np.array(rec).squeeze(0).T\n",
    "    x = np.array(x).squeeze(0).T\n",
    "    print(x.shape)\n",
    "    print(rec)\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(10, 4))\n",
    "    ax = axs#[0]\n",
    "    ax.plot(rec, \"r-\")\n",
    "    ax.plot(x, \"b-\", alpha=0.2)\n",
    "#     ax.plot(_mu_rec)\n",
    "#     ax.plot(params[\"mu\"].T[:_mu.shape[0],:])\n",
    "    ax.grid()\n",
    "    \n",
    "#     ax_std = axs[1]\n",
    "#     ax_std.plot(_logvar_rec, \"r\")\n",
    "#     ax_std.plot(params[\"cov\"][0,:,:T].T, \"b\")\n",
    "#     ax_std.grid()\n",
    "    \n",
    "#     ax_enc = axs[2]\n",
    "#     for (i,l) in enumerate(_mu.T):        \n",
    "#         ax_enc.plot(l.T, alpha = (0.2 + 0.1*i))\n",
    "# #     ax_enc.plot(_mu, alpha = 0.2)\n",
    "#     ax_enc.grid()\n",
    "    \n",
    "#     ax_log = axs[3]\n",
    "#     for (i,l) in enumerate(_logvar.T):        \n",
    "#         ax_log.plot(l.T, alpha = (0.2 + 0.1*i))\n",
    "#     ax_log.grid()\n",
    "    \n",
    "    # Setting legend\n",
    "    blue_handle = plt.Line2D([], [], color='b', label='Original Data', alpha=0.2)\n",
    "    red_handle = plt.Line2D([], [], color='r', label='Reconstructions mean')\n",
    "#     for ax in axs:\n",
    "        # ax.set_title('Reconstruction of Train Data')\n",
    "    ax.set_title('Prediction of Test (unseen) Data')\n",
    "    ax.legend(handles=[blue_handle, red_handle], loc=\"upper right\")\n",
    "    \n",
    "    \n",
    "    # Axes labels\n",
    "#     ax_std.set_xlabel('Time')\n",
    "#     ax.set_ylabel('Values')\n",
    "#     ax_std.set_ylabel('Values')\n",
    "#     plt.ylim(-100,500)\n",
    "    plt.grid(True)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "4a25a5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3455\n",
      "tensor([0.6128], device='cpu')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[362], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# v.quantizer._embedding.weight.detach()[4] = a\u001b[39;00m\n\u001b[1;32m      3\u001b[0m v\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mcompare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVQ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m v\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[360], line 17\u001b[0m, in \u001b[0;36mcompare\u001b[0;34m(dataset, model, VQ)\u001b[0m\n\u001b[1;32m     15\u001b[0m     x_rec, loss, mu, logvar, z \u001b[38;5;241m=\u001b[39m model(data, v)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 17\u001b[0m     x_rec, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mreparametrization_trick(mu, logvar)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[314], line 438\u001b[0m, in \u001b[0;36mVariationalAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz\u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    437\u001b[0m \u001b[38;5;66;03m#         print(\"Z: \", z.shape)\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m         rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;66;03m#         print(\"x:\", x)\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m#         print(\"Rec: \", rec)\u001b[39;00m\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rec, mu, logvar\n",
      "File \u001b[0;32m~/testenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[356], line 130\u001b[0m, in \u001b[0;36mGen_Decoder.forward\u001b[0;34m(self, cb)\u001b[0m\n\u001b[1;32m    128\u001b[0m         mean, logvar, trend_index, trend_slope, seasonality_fpw, seasonality_amp, seasonality_phase \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_params(cb)\n\u001b[1;32m    129\u001b[0m         rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_rec(mean, logvar)\n\u001b[0;32m--> 130\u001b[0m         rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_trend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrend_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrend_slope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m         rec \u001b[38;5;241m=\u001b[39m rec\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m#         print(\"shape of generated rec after unsqueeze\", rec.shape)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[356], line 110\u001b[0m, in \u001b[0;36mGen_Decoder.add_trend\u001b[0;34m(self, rec, trend_index, trend_slope)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_L)\n\u001b[1;32m    109\u001b[0m             \u001b[38;5;28mprint\u001b[39m(indexes)\n\u001b[0;32m--> 110\u001b[0m             mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_L\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mindexes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m#             print(\"mask\", mask.T.shape)\u001b[39;00m\n\u001b[1;32m    112\u001b[0m             trends[channel:] \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m*\u001b[39m slopes\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,2).to(x)\n",
    "# v.quantizer._embedding.weight.detach()[4] = a\n",
    "v.cpu()\n",
    "compare(train_data, v, VQ=False)\n",
    "v.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "f2e22afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(ax_heatmap, codebook):\n",
    "    ax_heatmap.clear()\n",
    "    heatmap = ax_heatmap.imshow(codebook)\n",
    "    ax_heatmap.set_title('Codebook Heatmap')\n",
    "    ax_heatmap\n",
    "\n",
    "\n",
    "    return heatmap\n",
    "def create_heatmap(codebook):\n",
    "    fig, ax_heatmap = plt.subplots(figsize=(12, 6), dpi=100)\n",
    "    heatmap = plot_heatmap(ax_heatmap, codebook.T)\n",
    "#     heatmap_canvas = FigureCanvasTkAgg(fig, master=heatmap_frame)\n",
    "#     heatmap_canvas.draw()\n",
    "#     heatmap_canvas.get_tk_widget().pack(fill=\"both\", expand=True)\n",
    "    cbar = fig.colorbar(heatmap)\n",
    "    ax_heatmap.set_xlabel('Num of Embeddings')\n",
    "    ax_heatmap.set_ylabel('Latent Dimensions')\n",
    "    \n",
    "    return ax_heatmap\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "9185de3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[320.44104  ,  -4.2680726]]], dtype=float32)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.figure import Figure\n",
    "codebook = v.z.cpu().detach().numpy()\n",
    "heatmap = create_heatmap(codebook)\n",
    "codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a5cd46cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4320])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c7ccc8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([408.2006])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "583dbbc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951229424500714"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(0.5 * -0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eea6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
